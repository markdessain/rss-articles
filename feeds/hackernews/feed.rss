<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Hacker News</title>
    <link>https://news.ycombinator.com/</link>
    <description>Links for the intellectually curious, ranked by readers.</description>
    <image>
      <url></url>
      <title></title>
      <link></link>
    </image>
    <item>
      <title>The CDC Needs to Stop Confusing the Public</title>
      <link>https://www.nytimes.com/2021/08/04/opinion/cdc-covid-guidelines.html</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_nytimes_com_2021_08_04_opinion_cdc-covid-guidelines_html/image.jpg" /> 
<div id="readability-page-1" class="page"><article id="story"><header><p>Guest Essay</p><p><time datetime="2021-08-04T05:00:10-04:00">Aug. 4, 2021, <span>5:00 a.m. ET</span></time></p><div data-testid="photoviewer-wrapper"><div data-testid="photoviewer-children"><figure aria-label="media" role="group"><div><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (-webkit-min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2021/08/04/opinion/04tufekci/04tufekci-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (-webkit-min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2021/08/04/opinion/04tufekci/04tufekci-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (-webkit-min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2021/08/04/opinion/04tufekci/04tufekci-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/></picture></div><figcaption><span><span>Credit...</span><span><span>Jim West/Report Digital-Rea, via Re​Dux</span></span></span></figcaption></figure></div></div><div><div><p><a href="https://www.nytimes.com/column/zeynep-tufekci"></a></p><div><p><span itemprop="name"><a href="https://www.nytimes.com/column/zeynep-tufekci">Zeynep Tufekci</a></span></p><p>Dr. Tufekci is a contributing Opinion writer who has extensively examined the Covid-19 pandemic.</p></div></div></div></header><section name="articleBody"><div><div><p>“Be first, be right, be credible,” are among the most important principles for health authorities to follow in a crisis, the Centers for Disease Control and Prevention shared in a <a href="https://emergency.cdc.gov/cerc/ppt/CERC_Introduction.pdf" title="" rel="noopener noreferrer" target="_blank">pamphlet</a> on crisis communication in 2018.</p><p>To meet those goals, the report advises, avoid sending mixed messages from multiple experts, releasing information too late, taking paternalistic attitudes, failing to counter rumors and myths in real time and engaging in public power struggles and causing confusion.</p><p>Last week, as agency officials announced new mask guidelines and set the nation on edge, I had to wonder if they had swapped their “do list” and their “avoid list.”</p><p>The C.D.C. faces three major problems.</p><p>The first is reality: a sustained campaign of misinformation against vaccines and other public health measures, originating mostly with right-wing commentators and politicians, and a new media environment that has upended traditional information flows.</p></div></div><div><div><p>Second, the C.D.C. is still mired in the fog of pandemic, with too little data, collected too slowly, leaving it chasing epidemic waves and trying to make sense of information from other countries. Epidemics spread exponentially, so delayed responses make problems much worse. If the response to a crisis comes after many people are already aware of it brewing, it leaves them confused and fearful if they look to the C.D.C. for guidance, and vulnerable to misinformation if they do not.</p><p>Third, the agency is simply not doing a good job at what the pamphlet advises: being first, right and credible, and avoiding mixed messaging, delays and confusion.</p><p>It’s hard not to have sympathy for its predicament. The previous administration undermined the C.D.C., and anti-vaxxers’ deliberate misinformation assault has not made the agency’s job any easier. The digital public sphere operates fast and furious, and that’s difficult for traditional institutions to keep up with or to counter.</p><p>All this makes it even more important that the C.D.C. properly handle what’s under its control.</p><p>The response to the Delta variant has been too slow. Data from other countries made it <a href="https://www.nytimes.com/2021/05/28/opinion/covid-vaccine-variants.html" title="">clear</a> months ago that it posed a great threat. Unfortunately, the United States already doesn’t collect the kind of systematic data needed on many important indicators. Making things worse, in early May, the C.D.C. stopped tracking breakthrough infections among the vaccinated unless they were hospitalized or worse, even though the reason for continued surveillance is to see and understand changes in an outbreak as early as possible.</p><p>June passed with little change in the government’s response, despite <a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/991343/Variants_of_Concern_VOC_Technical_Briefing_14.pdf" title="" rel="noopener noreferrer" target="_blank">multiple</a> <a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/993879/Variants_of_Concern_VOC_Technical_Briefing_15.pdf" title="" rel="noopener noreferrer" target="_blank">technical</a> papers from <a href="https://www.gov.uk/government/publications/investigation-of-novel-sars-cov-2-variant-variant-of-concern-20201201#history" title="" rel="noopener noreferrer" target="_blank">Public Health England</a> <a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/991135/3_June_2021_Risk_assessment_for_SARS-CoV-2_variant_DELTA.pdf" title="" rel="noopener noreferrer" target="_blank">showing</a> that the Delta variant was much more transmissible and possibly more severe, and that it was able to cause more breakthrough infections among the vaccinated. Detailed <a href="https://twitter.com/Ayjchan/status/1409680964834369540" title="" rel="noopener noreferrer" target="_blank">contact tracing</a> from Singapore also showed that some of the vaccinated were transmitting.</p></div></div><div><div><p>It was clear something had changed. Severe outbreaks were occurring wherever Delta swept through, leaving millions dead in countries with few vaccinated people, and increasing caseloads and hospitalizations even in countries with substantial vaccination levels.</p><p>Throughout June and July, I felt the same out-of-body experience I had in February 2020, when Covid-19 devastated Wuhan and Milan, while Americans acted as if it would somehow miss them.</p><p>Even with the natural immunity from previous outbreaks, with less than half the country fully vaccinated at the beginning of this summer, tens of millions of Americans were vulnerable — and the unvaccinated include large swathes of disadvantaged groups, like minorities and working poor, not just ideological anti-vaxxers.</p><p>Questions also swirled around how to better protect the immunocompromised and the elderly who can remain more vulnerable even when vaccinated, and how to open schools as safely as possible for children under 12 who cannot be vaccinated.</p><p>Yet the government waited to react.</p><p>Meanwhile, infections in the United States ticked up as the Delta variant swept through places with fewer vaccinated people. Hospitals were filling up again. The <a href="https://twitter.com/KristiWhitePhD/status/1416560790396719108" title="" rel="noopener noreferrer" target="_blank">piling up of anecdotes</a> made it obvious that more vaccinated people were getting infected since the variant hit.</p><p>How many? How sick? How infectious? What to do? The answers came late, in a trickle and in a manner that was more confusing than illuminating.</p><p>On July 21, the White House’s chief medical adviser, Anthony Fauci,<a href="https://www.cnbc.com/2021/07/21/delta-covid-variant-fauci-says-vaccinated-people-might-want-to-consider-wearing-masks-indoors-.html" title="" rel="noopener noreferrer" target="_blank"> told CNBC</a> that Delta was “clearly different” than previous variants, with an extraordinary capacity for transmitting from person to person, and that fully vaccinated people might want to consider wearing masks indoors. However, just one day later, the C.D.C.’s director, Rochelle Walensky, asserted again that wearing masks for the vaccinated was an “<a href="https://abcnews.go.com/Politics/cdc-director-stands-firm-mask-guidance-calls-individual/story?id=78990692" title="" rel="noopener noreferrer" target="_blank">individual choice</a>,” saying that the vaccinated enjoyed “exceptional levels of protection.” Then on July 25, Dr. Fauci confirmed that <a href="https://www.cnn.com/videos/politics/2021/07/25/sotu-fauci-on-revisiting-mask-mandates.cnn" title="" rel="noopener noreferrer" target="_blank">bringing back mask mandates</a> was “under active consideration.”</p></div></div><div><div><p>Just two days later, on July 27, Dr. Walensky addressed the issue again, but now with a very different message: Delta was behaving very differently, she said, and the C.D.C. was now recommending even the fully vaccinated wear masks indoors in public places wherever transmission rates were “substantial.”</p><p>All this was fairly confusing for the public especially since it was already many weeks after the agency should have reacted. A pandemic requires a forceful, quick, clear and unified response from public health authorities.</p><p>In announcing changes in mask recommendations Dr. Walensky notably said that vaccinated people who became infected had viral loads similar to those of unvaccinated people who got sick, and could “forward transmit with the same capacity as an unvaccinated person.”</p><p>That vaccinated people with breakthrough infections could sometimes transmit the virus wasn’t particularly surprising given the <a href="https://twitter.com/michaelzlin/status/1409700004705435649" title="" rel="noopener noreferrer" target="_blank">data</a> and anecdotes that had accumulated. However, that they had “the same capacity” to transmit it as an unvaccinated person certainly was, including <a href="https://twitter.com/j_g_allen/status/1421184927660380161" title="" rel="noopener noreferrer" target="_blank">to</a> <a href="https://twitter.com/profshanecrotty/status/1421131753117212672" title="" rel="noopener noreferrer" target="_blank">many experts</a>. It was the kind of claim that was clearly going to alarm tens of millions of vaccinated people, and needed to be delivered with maximum clarity and context.</p><p>Instead we got a stark lesson in how not to communicate.</p><p>First, the data that the C.D.C. said it based its decision on wasn’t released right away — leaving both experts and ordinary people to try to piece together what was being said.</p><p>The Associated Press, on <a href="https://apnews.com/article/health-coronavirus-pandemic-79959d313428d98ab8aa905bbe287ba0" title="" rel="noopener noreferrer" target="_blank">the same day</a> as Dr. Walensky’s news conference, quoted her as saying the level of virus in infected vaccinated people was “indistinguishable” from the level of virus in the noses and throats of unvaccinated people. The news report also noted that the data the C.D.C. had based its new decision on was unpublished, and had “emerged over the last couple of days from over 100 samples from several states and one other country.”</p><p>Which ones and where?</p><p>The updated C.D.C. guidelines <a href="https://www.cdc.gov/coronavirus/2019-ncov/science/science-briefs/fully-vaccinated-people.html" title="" rel="noopener noreferrer" target="_blank">pointed to a single reference on this question</a>, which was a preprint looking at health care workers in India who became infected after receiving <a href="https://www.researchsquare.com/article/rs-637724/v1" title="" rel="noopener noreferrer" target="_blank">vaccines not approved for use in the United States</a>. In many studies, viral load is ascertained through examining a measure from PCR testing called <a href="https://www.wvdl.wisc.edu/wp-content/uploads/2013/01/WVDL.Info_.PCR_Ct_Values1.pdf" title="" rel="noopener noreferrer" target="_blank">the cycle threshold or CT</a>. <a href="https://science.sciencemag.org/content/373/6552/eabh0635" title="" rel="noopener noreferrer" target="_blank">While useful</a>, because the cycle threshold is a proxy and <a href="https://twitter.com/jameshay218/status/1316202284510978048" title="" rel="noopener noreferrer" target="_blank">a snapshot</a>, it needs to be interpreted with caution. Still, the study from India wasn’t even between those who were vaccinated and those who were not, but <a href="https://assets.researchsquare.com/files/rs-637724/v1_covered.pdf?c=1624377344" title="" rel="noopener noreferrer" target="_blank">among vaccinated workers infected with different variants</a>. That the more transmissible Delta had a higher viral load than other variants <a href="https://www.nature.com/articles/d41586-021-01986-w" title="" rel="noopener noreferrer" target="_blank">had already been reported</a>.</p></div></div><div><div><p>Two days later, based on slides leaked from the C.D.C. and a federal health official who spoke on condition of anonymity, <a href="https://www.washingtonpost.com/health/2021/07/29/cdc-mask-guidance/" title="" rel="noopener noreferrer" target="_blank">The Washington Post reported</a> that the American outbreak that the agency used to base its new guidelines on was in Provincetown, Mass.</p><p>Finally, on July 30, the C.D.C. released <a href="https://www.cdc.gov/mmwr/volumes/70/wr/mm7031e2.htm" title="" rel="noopener noreferrer" target="_blank">its epidemiological study</a> of the Provincetown outbreak.</p><p>In a seashore town with about 3,000 residents, the vast majority of whom were vaccinated, and 60,000 summer visitors, the C.D.C. noted more than 450 infections between July 3 and 17. Health officials later traced <a href="https://www.nytimes.com/2021/07/31/us/covid-outbreak-provincetown-cape-cod.html" title="">more than 960 cases</a> to gatherings in Provincetown. <a href="https://inguyun.medium.com/the-provincetown-outbreak-is-actually-good-news-if-you-are-vaccinated-93a1edd763b6" title="" rel="noopener noreferrer" target="_blank">Attendees said</a> it rained a lot during those two weeks, driving more people to crowded, poorly ventilated bars and restaurants, probably worsening the spread.</p><p>The data showed what had been documented elsewhere: Delta was sometimes able to infect the vaccinated, although there were only seven hospitalizations and no deaths. Most common symptoms were cough, headache, sore throat.</p><p>The cycle threshold numbers among vaccinated and unvaccinated cases were indeed similar, suggesting similar viral loads — at least when the test was taken. However, since there was no follow-up to measure actual transmission events it was unclear how much the vaccinated contributed to the spread. Or did the unvaccinated infect most people in the outbreak, including the vaccinated? It is also hard to draw broader conclusions from such nonsystematic and <a href="https://twitter.com/CarlosdelRio7/status/1421221004328714240" title="" rel="noopener noreferrer" target="_blank">limited</a> data. People who get tested are likely to be more sick, for example, so they probably have higher viral loads to begin with, and thus are probably not fully representative of those who are vaccinated but infected. All this means that the cycle threshold value may be useful, but it’s just one piece of the puzzle without contextual data — suggestive but not conclusive.</p><p>On July 31, a more systematic study from Singapore showed that viral load from Delta could get high but that it <a href="https://twitter.com/apsmunro/status/1421484233810456576" title="" rel="noopener noreferrer" target="_blank">quickly peaked and then crashed in breakthrough cases</a> among the vaccinated — as their immune system responded <a href="https://twitter.com/EricTopol/status/1421526589766979586" title="" rel="noopener noreferrer" target="_blank">to quickly clear the virus</a>. The potential infectious period lingered much longer in the unvaccinated.</p><p>The Provincetown study was certainly useful. It provided one more example of how well the vaccines worked in preventing severe disease or worse, but also of the need to take Delta seriously: to expand vaccine mandates, speed up formal approval of vaccines, work hard at increasing vaccinations and urge the use of masks for everyone, especially in crowded, poorly ventilated indoor spaces in areas where infections are high and vaccinations are low.</p><p>However, by itself, the study should not have been presented as the primary cause for the alarm it set off, and the public certainly should not have been left waiting many days for the data itself while details leaked out in dribs and drabs, often through anonymous sources.</p></div></div><div><div><p>Some in the administration <a href="https://twitter.com/benwakana46/status/1421182153224818694" title="" rel="noopener noreferrer" target="_blank">lashed out at the media</a> coverage of it all. Administration officials, <a href="https://www.cnn.com/2021/07/30/media/variant-media-coverage-white-house/index.html" title="" rel="noopener noreferrer" target="_blank">again anonymously</a>, said they were worried that this might contribute to vaccine hesitancy by making it sound as if vaccines don’t make a difference.</p><p>I’ll be first to say that the media can and should do a better job. But the administration’s job is to make sure the message they send is clear, timely, loud and unified, not just to get angry at factors beyond their control.</p><p>How else could this have played out? Ideally, with better data and earlier response. The C.D.C. should start tracking more breakthrough cases, and do much more systematic data collection including cluster and contact-tracing while the pandemic continues to rage. Yes, such infrastructure cannot be built overnight, but we have to start from where we are.</p><p>The C.D.C. also needs to better take into account the sociological effects of its guidance. Recently, Dr. Walensky <a href="https://www.mcclatchydc.com/news/coronavirus/article253156748.html" title="" rel="noopener noreferrer" target="_blank">attributed the current rise in infections</a> to the unvaccinated, saying: “Unvaccinated people took off their masks as well. And that’s what led us to where we are today.” However, as many <a href="https://www.nytimes.com/2021/05/14/opinion/coronavirus-masks-vaccines.html" title="">pointed out</a> at the time, those who are not eager to get vaccinated were not going to be eager to keep on their masks. And a grocery store or a club cannot be expected to enforce masking selectively, so the practical effect of that guidance change was to undermine masking in general. Getting mad at the public for not following public health advice might be understandable at the individual level, but the agency should focus on how to broaden trust and facilitate better behaviors for everyone.</p><p>The nation should have waited a bit more before lifting indoor mask guidelines, tying changes to concrete benchmarks like vaccination and infection rates, especially given the vulnerability of the immunocompromised and children who are ineligible for vaccination.</p><p>Most important, the C.D.C. can follow the principles it espouses — organize and coordinate the release of information, back up recommendations with solid research, and move as quickly as possible to respond to crises. The C.D.C. should have news conferences weekly, or even a few times a week, with a consistent spokesperson and a team of experts to answer technical questions. If officials feel the media has been misleading, then they should quickly hold a news conference and explain why.</p><p>The Epidemic Intelligence Service unit of the C.D.C. has <a href="https://www.newyorker.com/magazine/2020/05/04/seattles-leaders-let-scientists-take-the-lead-new-yorks-did-not" title="" rel="noopener noreferrer" target="_blank">a core principle</a> that needs to remain at the forefront of everything the administration does: A pandemic is a communications emergency as much as it is a medical crisis. Effective communication is much more than choosing the right words. It needs a wholesale approach starting with clarity of purpose, a realistic assessment of where things are including factors outside the agency’s control, collection and presentation of detailed data when possible and an open acknowledgment of uncertainty and underlying reasoning when precautionary steps are being advised. The agency must have a laser focus on what it can do with what it has, despite the challenges, rather than looking for justifications for what doesn’t work well — even if those exist.</p></div></div><div><p>So remember, C.D.C.: Be first, be right and be credible. The conditions may not be ideal, but that’s the job.</p></div></section></article></div>]]></content:encoded>
      <guid>https://www.nytimes.com/2021/08/04/opinion/cdc-covid-guidelines.html</guid>
      <pubDate>Wed, 04 Aug 2021 13:31:46 +0000</pubDate>
      <source>https://www.nytimes.com/2021/08/04/opinion/cdc-covid-guidelines.html</source>
    </item>
    <item>
      <title>Bye CUPS: Printing with Netcat</title>
      <link>https://retrohacker.substack.com/p/bye-cups-printing-with-netcat</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><p>I recently migrated my daily laptop to FreeBSD. I have a networked HP LaserJet. After 10+ years of CUPS on Linux, I had been dreading setting up this printer on yet another machine. But the day came. I had to print quite a few documents so I decided to bite the bullet and setup printing on FreeBSD.</p><p>Off to the FreeBSD Handbook I went. Conveniently, they have a chapter on <a href="https://docs.freebsd.org/en/books/handbook/printing">Printing</a>. Given my past experience with CUPS, I figured this was going to be a treacherous journey so I read the entire document before getting started. Section 4 stood out:</p><blockquote><p>Direct printing to network printers depends on the abilities of the printer, but most accept print jobs on port 9100, and <a href="https://www.freebsd.org/cgi/man.cgi?query=nc&amp;sektion=1&amp;format=html">nc(1)</a> can be used with them.</p><pre><code><code>nc netlaser 9100 &lt; sample.txt</code></code></pre></blockquote><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6d338e2-5bd5-4f0d-9d7d-51ade4f33989_300x167.gif"></a></figure></div><p>I had to read this several times. WTF. Use netcat? Surely I was missing some wizardry in the FreeBSD kernel that configured a network printer as some local network target aliased to netlaser, which handled all the printing magic for me. But how? Years of arbitrarily picking from a list of similarly named print drivers in CUPS prevented my brain from accepting what was written. I was confused. After trying to decipher the other sections in the document, I decided I would go for it.</p><pre><code>nc 192.168.1.226 9100 &lt; file.pdf</code></pre><p><em>It. Just. Prints.</em></p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb2310d-6f76-4064-9ccb-855d13968900_458x284.gif"></a></figure></div><p>After 10+ years of Linux distributions pushing CUPS on me. Over a decade of this complex stack of drivers and daemons that I never quite trusted but “couldn’t live without.” FreeBSD comes along and is like “yeah, just use netcat.” And they’re right. Just use netcat.</p><p>Bye CUPS.</p></div></div>]]></content:encoded>
      <guid>https://retrohacker.substack.com/p/bye-cups-printing-with-netcat</guid>
      <pubDate>Tue, 03 Aug 2021 22:03:00 +0000</pubDate>
      <source>https://retrohacker.substack.com/p/bye-cups-printing-with-netcat</source>
    </item>
    <item>
      <title>Varnish: Notes from the Architect</title>
      <link>http://varnish-cache.org/docs/trunk/phk/notes.html</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
      <div>
        <div>
          <div role="main">
            
  <section id="notes-from-the-architect">
<span id="phk-notes"></span>
<p>Once you start working with the Varnish source code, you will notice
that Varnish is not your average run of the mill application.</p>
<p>That is not a coincidence.</p>
<p>I have spent many years working on the FreeBSD kernel, and only
rarely did I venture into userland programming, but when I had
occation to do so, I invariably found that people programmed like
it was still 1975.</p>
<p>So when I was approached about the Varnish project I wasn’t really
interested until I realized that this would be a good opportunity
to try to put some of all my knowledge of how hardware and kernels
work to good use, and now that we have reached alpha stage, I can
say I have really enjoyed it.</p>
<section id="so-what-s-wrong-with-1975-programming">
<h2>So what’s wrong with 1975 programming ?<a href="#so-what-s-wrong-with-1975-programming" title="Permalink to this headline">¶</a></h2>
<p>The really short answer is that computers do not have two kinds of
storage any more.</p>
<p>It used to be that you had the primary store, and it was anything
from acoustic delaylines filled with mercury via small magnetic
dougnuts via transistor flip-flops to dynamic RAM.</p>
<p>And then there were the secondary store, paper tape, magnetic tape,
disk drives the size of houses, then the size of washing machines
and these days so small that girls get disappointed if think they
got hold of something else than the MP3 player you had in your
pocket.</p>
<p>And people program this way.</p>
<p>They have variables in “memory” and move data to and from “disk”.</p>
<p>Take Squid for instance, a 1975 program if I ever saw one: You tell
it how much RAM it can use and how much disk it can use. It will
then spend inordinate amounts of time keeping track of what HTTP
objects are in RAM and which are on disk and it will move them forth
and back depending on traffic patterns.</p>
<p>Well, today computers really only have one kind of storage, and it
is usually some sort of disk, the operating system and the virtual
memory management hardware has converted the RAM to a cache for the
disk storage.</p>
<p>So what happens with squids elaborate memory management is that it
gets into fights with the kernels elaborate memory management, and
like any civil war, that never gets anything done.</p>
<p>What happens is this: Squid creates a HTTP object in “RAM” and it
gets used some times rapidly after creation. Then after some time
it get no more hits and the kernel notices this. Then somebody tries
to get memory from the kernel for something and the kernel decides
to push those unused pages of memory out to swap space and use the
(cache-RAM) more sensibly for some data which is actually used by
a program. This however, is done without squid knowing about it.
Squid still thinks that these http objects are in RAM, and they
will be, the very second it tries to access them, but until then,
the RAM is used for something productive.</p>
<p>This is what Virtual Memory is all about.</p>
<p>If squid did nothing else, things would be fine, but this is where
the 1975 programming kicks in.</p>
<p>After some time, squid will also notice that these objects are
unused, and it decides to move them to disk so the RAM can be used
for more busy data. So squid goes out, creates a file and then it
writes the http objects to the file.</p>
<p>Here we switch to the high-speed camera: Squid calls write(2), the
address i gives is a “virtual address” and the kernel has it marked
as “not at home”.</p>
<p>So the CPU hardwares paging unit will raise a trap, a sort of
interrupt to the operating system telling it “fix the memory please”.</p>
<p>The kernel tries to find a free page, if there are none, it will
take a little used page from somewhere, likely another little used
squid object, write it to the paging poll space on the disk (the
“swap area”) when that write completes, it will read from another
place in the paging pool the data it “paged out” into the now unused
RAM page, fix up the paging tables, and retry the instruction which
failed.</p>
<p>Squid knows nothing about this, for squid it was just a single
normal memory acces.</p>
<p>So now squid has the object in a page in RAM and written to the
disk two places: one copy in the operating systems paging space and
one copy in the filesystem.</p>
<p>Squid now uses this RAM for something else but after some time, the
HTTP object gets a hit, so squid needs it back.</p>
<p>First squid needs some RAM, so it may decide to push another HTTP
object out to disk (repeat above), then it reads the filesystem
file back into RAM, and then it sends the data on the network
connections socket.</p>
<p>Did any of that sound like wasted work to you ?</p>
<p>Here is how Varnish does it:</p>
<p>Varnish allocate some virtual memory, it tells the operating system
to back this memory with space from a disk file. When it needs to
send the object to a client, it simply refers to that piece of
virtual memory and leaves the rest to the kernel.</p>
<p>If/when the kernel decides it needs to use RAM for something else,
the page will get written to the backing file and the RAM page
reused elsewhere.</p>
<p>When Varnish next time refers to the virtual memory, the operating
system will find a RAM page, possibly freeing one, and read the
contents in from the backing file.</p>
<p>And that’s it. Varnish doesn’t really try to control what is cached
in RAM and what is not, the kernel has code and hardware support
to do a good job at that, and it does a good job.</p>
<p>Varnish also only has a single file on the disk whereas squid puts
one object in its own separate file. The HTTP objects are not needed
as filesystem objects, so there is no point in wasting time in the
filesystem name space (directories, filenames and all that) for
each object, all we need to have in Varnish is a pointer into virtual
memory and a length, the kernel does the rest.</p>
<p>Virtual memory was meant to make it easier to program when data was
larger than the physical memory, but people have still not caught
on.</p>
</section>
<section id="more-caches">
<h2>More caches<a href="#more-caches" title="Permalink to this headline">¶</a></h2>
<p>But there are more caches around, the silicon mafia has more or
less stalled at 4GHz CPU clock and to get even that far they have
had to put level 1, 2 and sometimes 3 caches between the CPU and
the RAM (which is the level 4 cache), there are also things like
write buffers, pipeline and page-mode fetches involved, all to make
it a tad less slow to pick up something from memory.</p>
<p>And since they have hit the 4GHz limit, but decreasing silicon
feature sizes give them more and more transistors to work with,
multi-cpu designs have become the fancy of the world, despite the
fact that they suck as a programming model.</p>
<p>Multi-CPU systems is nothing new, but writing programs that use
more than one CPU at a time has always been tricky and it still is.</p>
<p>Writing programs that perform well on multi-CPU systems is even trickier.</p>
<p>Imagine I have two statistics counters:</p>
<blockquote>
<p>unsigned    n_foo;
unsigned    n_bar;</p></blockquote>
<p>So one CPU is chugging along and has to execute n_foo++</p>
<p>To do that, it read n_foo and then write n_foo back. It may or may
not involve a load into a CPU register, but that is not important.</p>
<p>To read a memory location means to check if we have it in the CPUs
level 1 cache. It is unlikely to be unless it is very frequently
used. Next check the level two cache, and let us assume that is a
miss as well.</p>
<p>If this is a single CPU system, the game ends here, we pick it out
of RAM and move on.</p>
<p>On a Multi-CPU system, and it doesn’t matter if the CPUs share a
socket or have their own, we first have to check if any of the other
CPUs have a modified copy of n_foo stored in their caches, so a
special bus-transaction goes out to find this out, if if some cpu
comes back and says “yeah, I have it” that cpu gets to write it to
RAM. On good hardware designs, our CPU will listen in on the bus
during that write operation, on bad designs it will have to do a
memory read afterwards.</p>
<p>Now the CPU can increment the value of n_foo, and write it back.
But it is unlikely to go directly back to memory, we might need it
again quickly, so the modified value gets stored in our own L1 cache
and then at some point, it will end up in RAM.</p>
<p>Now imagine that another CPU wants to n_bar+++ at the same time,
can it do that ? No. Caches operate not on bytes but on some
“linesize” of bytes, typically from 8 to 128 bytes in each line.
So since the first cpu was busy dealing with n_foo, the second CPU
will be trying to grab the same cache-line, so it will have to wait,
even through it is a different variable.</p>
<p>Starting to get the idea ?</p>
<p>Yes, it’s ugly.</p>
</section>
<section id="how-do-we-cope">
<h2>How do we cope ?<a href="#how-do-we-cope" title="Permalink to this headline">¶</a></h2>
<p>Avoid memory operations if at all possible.</p>
<p>Here are some ways Varnish tries to do that:</p>
<p>When we need to handle a HTTP request or response, we have an array
of pointers and a workspace. We do not call malloc(3) for each
header. We call it once for the entire workspace and then we pick
space for the headers from there. The nice thing about this is that
we usually free the entire header in one go and we can do that
simply by resetting a pointer to the start of the workspace.</p>
<p>When we need to copy a HTTP header from one request to another (or
from a response to another) we don’t copy the string, we just copy
the pointer to it. Provided we do not change or free the source
headers, this is perfectly safe, a good example is copying from the
client request to the request we will send to the backend.</p>
<p>When the new header has a longer lifetime than the source, then we
have to copy it. For instance when we store headers in a cached
object. But in that case we build the new header in a workspace,
and once we know how big it will be, we do a single malloc(3) to
get the space and then we put the entire header in that space.</p>
<p>We also try to reuse memory which is likely to be in the caches.</p>
<p>The worker threads are used in “most recently busy” fashion, when
a workerthread becomes free it goes to the front of the queue where
it is most likely to get the next request, so that all the memory
it already has cached, stack space, variables etc, can be reused
while in the cache, instead of having the expensive fetches from
RAM.</p>
<p>We also give each worker thread a private set of variables it is
likely to need, all allocated on the stack of the thread. That way
we are certain that they occupy a page in RAM which none of the
other CPUs will ever think about touching as long as this thread
runs on its own CPU. That way they will not fight about the cachelines.</p>
<p>If all this sounds foreign to you, let me just assure you that it
works: we spend less than 18 system calls on serving a cache hit,
and even many of those are calls tog get timestamps for statistics.</p>
<p>These techniques are also nothing new, we have used them in the
kernel for more than a decade, now it’s your turn to learn them :-)</p>
<p>So Welcome to Varnish, a 2006 architecture program.</p>
<p><em>phk</em></p>
</section>
</section>


            
          </div>
        </div>
      </div>
      
      
    </div></div>]]></content:encoded>
      <guid>http://varnish-cache.org/docs/trunk/phk/notes.html</guid>
      <pubDate>Mon, 02 Aug 2021 11:06:50 +0000</pubDate>
      <source>http://varnish-cache.org/docs/trunk/phk/notes.html</source>
    </item>
    <item>
      <title>A Long, Painful History of Time (1999)</title>
      <link>http://naggum.no/lugm-time.html</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page">




<p><a href="http://naggum.no/"><b>Erik Naggum</b></a></p>

<blockquote><p><b>ABSTRACT</b> The programming language Common Lisp offers a few functions to support the concept of time
as humans experience it, including <tt>GET-UNIVERSAL-TIME</tt>, <tt>ENCODE-UNIVERSAL-TIME</tt>,
<tt>DECODE-UNIVERSAL-TIME</tt>, and <tt>GET-DECODED-TIME</tt>. These functions assume the existence of a timezone and a
daylight saving time regime, such that they can support the usual expression of time in the environment in which a small
number of real-life applications run. The majority of applications, however, need more support to be able to read and
write dates and times, calculate with time, schedule events at specific clock times daily, and work with several time
zones and daylight saving time regimes. This paper discusses some of the problems inherent in processing time suitable
to humans and describes a solution employed by the author in a number of applications, the <tt>LOCAL-TIME</tt>
concept.</p></blockquote>

<h2>0 Introduction</h2>

<p>The measurement of time has a very long history, dating back to the first records of human civilization.  Yet,
the archeological evidence suggests that the concept of time evolved no further than ordinary human needs, and any
notion of time remained confined to a fairly short time frame, such as a lifetime past and future.  Expressions of
measurements of time were brief and imprecise, rife with the numerous and nefarious assumptions humans bring into their
communication, consistent with our tendency to suppress information believed to be redundant.</p>

<p>For instance, everyone knows which century they are in or that some two-digit year refers to.  Until computers came
along, the assumptions held by people were either recoverable from the context or shared by contemporary communicators.
After computers came to store information for us, we still held onto the context as if the computers were as able to
recover it as we are.  Quite obviously, they aren&#39;t, and in about three months, we will see whether other humans were
indeed able to recover the context left unstated by other humans when they wrote down their dates with two digits and
assumed it would never be a problem.  The infamous Y2K problem is one of the few opportunities mankind will get to tally
the costs of lack of precision in our common forms of communication.  The lesson learned will not be that our notations
of time need to be precise and include their context, unless the general public stops refusing to be educated in the
face of dire experience.  That so much attention has been granted this silly problem is fortunate for those of us who
argue against legacy notations of time.  However, the inability of most people to deal with issues of such extraordinary
importance when they look <q>most harmless</q> means that those who do understand them must be inordinately careful in
preparing their information such that loss of real information can be minimized.</p>

<p>The basic problem with time is that we need to express both time and place whenever we want to place some event in
time and space, yet we tend to assume spatial coordinates even more than we assume temporal coordinates, and in the case
of time in ordinary communication, it is simply left out entirely.  Despite the existence of time zones and strange
daylight saving time regimes around the world, most people are blithely unaware of their own time zone and certainly of
how it relates to standard references.  Most people are equally unaware that by choosing a notation that is close to the
spoken or written expression of dates, they make it meaningless to people who may not share the culture, but can still
read the language.  It is unlikely that people will change enough to put these issues to rest, so responsible computer
people need to address the issues and resist the otherwise overpowering urge to abbreviate and drop context.</p>

<p>This paper is almost all about how we got ourselves into trouble by neglecting to think about time frames longer than
a human lifetime, how we got all confused by the difference between time as an orderly concept in science and a mess in
the rest of human existence, and how we have missed every opportunity to fix the problems.  This paper proposes a fix to
the most glaring problems in a programming language that should not have been left without a means to express time for
so long.</p>

<h2>1 Scientific Time</h2>

<p>How long does it take the earth to face the Sun at the same angle?  This simple question has a definite and fairly
simple scientific answer, and from this answer, we can work out a long list of answers about what time is and how we
want to deal with astronomical events.  The SI units (Système International d&#39;Unités), probably better known as
<q>metric units</q>, define the second as the fundamental unit of time, and this, too, has a very good scientific
definition.  Time progresses continuously and is only chopped up into units for human convenience.  Agreement on a
single reference point within a scientific community has always been easy, and it is useful to count basic units, like
days in the (Modified) Julian Day system, or seconds since some arbitrary epoch in computers.</p>

<p>Scientific time also lends itself to ease of computation; after all, that is what we do with it.  For instance, we
have a world-wide standard for time, called the Coordinated Universal Time, or UTC. (The C used to be subscripted,
UT<sub>C</sub>, just like the digits in UT<sub>0</sub> and UT<sub>1</sub> which are universal time concepts with
slightly different reference points, but <q>UTC</q> has become the preferred form.)  Scientific time naturally has
origin 0, as usual with scientific measures, even though the rest of human time notations tend to have origin 1, the
problems of which will be treated below.</p>

<p>Most computer-related references to time deal with periods of time, which lend themselves naturally to use scientific
time, and therefore, it makes sense to most programmers to treat the period of time from some epoch until some other
time to be the best way to express said other time. This is the path taken by Common Lisp in its <tt>UNIVERSAL-TIME</tt>
concept, with time 0 equal to 1900-01-01 00:00:00 UTC, and the Unix time concept, with time 0 equal to 1970-01-01
00:00:00 UTC. This approach works well as long as the rules for converting between relative and absolute time are
stable.  As it turns out, they are not.</p>

<p>Not all languages and operating systems use this sensible an approach. Some have used local time as the point of
reference, some use decoded local time as the reference, and some use hardware clocks that try to maintain time suitable
for direct human consumption.  There is no need to make this issue more complex than it already is, so they will not be
granted any importance.</p>

<h2>2 Political Time</h2>

<p>How long does it take for the clock to show the same value?  The answer to this question is only weakly related to
the time the planet takes to make a complete rotation.  Normally, we would say the political rotation takes 24 hours,
just like the scientific, but one day out of the year, it takes only 23 hours, and another day out of the year, it takes
25 hours, thanks to the wonders of daylight saving time.  Which days these are is a decision made by politicians.  It
used to be made by the military to conserve fuel, but was taken over by labor unions as a means to get more daylight in
the workers&#39; spare time, and most countries have gone through an amazing list of strange decision-making in this area
during this century.  Short of coming to their senses and abolishing the whole thing, we might expect that the rules for
daylight saving time will remain the same for some time to come, but there is no guarantee. (We can only be glad there
is no daylight loan time, or we would face decades of too much daylight, only to be faced with a few years of total
darkness to make up for it.)</p>

<p>Political time is closely related to territory, power, and collective human irrationality.  There is no way you can
know from your location alone which time zone applies at some particular point on the face of the earth: you have to ask
the people who live there what they have decided. This is very different from scientific time, which could tell you with
great ease and precision what the mean sidereal time at some location should be.  In some locations, this is as much as
three hours off from what the local population has decided, or has had decided for them.  The Sun is in zenith at noon
at very few places on earth, instead being eclipsed or delayed by political decisions where the randomness never
ends.</p>

<p>Yet, it is this political time that most people want their computers to produce when they ask for the date or the
time of day, so software will have to comply with the randomness and produce results consistent with political
decisions.  The amount of human input into this process is very high, but that is the price we have to pay for our
willingness to let politicians dictate the time.  However, once the human input has been provided, it is annoying to
find that most programming languages and supporting systems do not work with more than one timezone at a time, and
consequently do not retain timezone information with time data.</p>

<p>The languages we use tend to shape the ideas we can talk about. So, too, the way we write dates and times influence
our concepts of time, as they were themselves influenced by the way somebody thought about time a long time ago.
Calendars and decisions like which year is the first, when the year starts, and how to deal with astronomical
irregularities were made so long ago that the rationale for them has not survived in any form, but we can still look at
what we have and try to understand. In solving the problem of dealing with time in computers, a solid knowledge of the
legacy we are attending to is required.</p>

<h2>3 Notations for Time</h2>

<p>The way we write down time coordinates appears to have varied little over the years in only one respect: we tend to
write them differently depending on the smallest perceived unit of time that needs to be communicated. For instance, it
seems sufficiently redundant to include <i>AD</i> or <i>BC</i> in the dates of birth of contemporary people that they
are always omitted. Should some being with age &gt;2000 years come to visit us, it is also unlikely that writing its
date of birth correctly would be a pressing concern. However, we tend to include these markers for the sign of the year
when the possibility of ambiguity reaches a certain level as determined by the reader.  This process is itself fraught
with ambiguity and inconsistency, but when computers need to deal with dates this far back, it does not seem worthwhile
to calculate them in terms of standard reference points, so we can ignore the problem for now, but may need to deal with
it if a system of representation is sufficiently useful to be extended to the ancient past.</p>

<p>Not only do we omit information that is deemed redundant, it is not uncommon for people to omit information out of
sheer laziness.  A particularly flagrant example of the omission of information relative to the current time is the
output from the Unix <tt>ls</tt> program which lists various information about files.  The customary date and time
format in this program is either month-day-hour-minute or month-day-year.  The cutoff for tolerable precision is six
months ago, which most implementations approximate with 180 days.  This reduction in precision appears to have been
motivated by horizontal space requirements, a necessary move after wasting a lot of space on irrelevant information, but
for some reason, precision in time always suffers when people are short of space.</p>

<p>The infamous Y2K problem, for instance, is said to have started when people wanted to save two columns on punched
cards, but there is strong evidence of other, much better alternatives at the time, so the decision to lose the century
was not predicated on the need for space, but rather on the culturally acceptable loss of information from time
coordinates.  The details of this mess are sufficiently involved to fill a separate paper, so the conclusion that time
loses precision first when in need or perceived need of space should be considered supported by the evidence.</p>

<h3>3.1 Natural-Language Notations</h3>

<p>People tend to prefer words to numbers, and go out of their way to name things.  Such names are frequently symbolic
because they are inherently arbitrary, which implies that we can learn much from studying what people call numbers.
(French has a number which means <q>arbitrarily many</q>: 36, used just like English <q>umpteen</q>, but it is
fascinating that a number has meaning like that.  Other numbers with particular meaning include 69, 666, and 4711.  The
number 606 has been used to refer to arsphenamine, because it was the 606th compound tested by Paul Ehrlich to treat
syphilis.) In the present context, the names of the Roman months have been adopted by all Western languages, while the
names of days of the week have more recent and diverse names, probably because weeks are a fairly recent concept.</p>

<p>Using names for numeric entities complicates processing a natural language specification of time tremendously, yet
this is what people seem more comfortable with.  In some cultures, months have only names, while in others, they are
nearly always written as numbers.  The way the names of months and the days of the week are abbreviated varies from
language to language, as well, so software that wants to be international needs to maintain a large repository of names
and notations to cater to the vanity of human users.  However, the names are not the worst we have to deal with in
natural language notations.</p>

<p>Because dates and times are frequently spoken and because the written forms are often modeled after the spoken, we
run into the problem of ordering the elements of time and the omission of perceived redundancy becomes a much more
serious problem, because each language and each culture have handled these problems so differently.  The orders in use
for dates are</p>

<ul>
<li>year-month-day</li>
<li>day-month-year</li>
<li>month-day-year</li>
<li>day-month</li>
<li>month-day</li>
<li>year-month</li>
<li>month-year</li>
</ul>

<p>
As long as the year is zero or greater than 31 or the day greater than 12, it is usually possible to disambiguate these
orders, but we are about to experience renewed problems in 2001, when the year will probably be still be written with
two digits by some people regardless of the experience of mankind as a whole at <tt>2000-01-01 00:00:00</tt>. We live in
interesting times, indeed.</p>

<p>Time is fortunately specified with a uniform hour-minute-second order, but the assumption of either <tt>AM</tt> or
<tt>PM</tt> even in cultures where there is no custom for their specification provides us with an ambiguity that
computers are ill equipped to deal with. This and other historic randomness will be treated in full below.</p>

<p>Most of the time people refer to is in their immediate vicinity, and any system intended to capture human-friendly
time specifications will need to understand relative times, such as <q>yesterday</q>, <q>this time tomorrow</q>, <q>two
hours ago</q>, <q>in fifteen minutes</q>.  All of these forms vary considerably from culture to culture and from
language to language, making the process of reading these forms as input non-trivial.  The common forms of expression
for periods of time is also fuzzy in human communication, with units that fail to convert to intervals of fixed length,
but instead are even more context-sensitive than simple points in time.</p>

<h3>3.2 Language-Neutral Notations</h3>

<p>Various attempts have been made to overcome the problems of human-to-human forms of communication between human and
machine and in machine-to-machine communication.  Machine-to-machine communication generally falls into one of three
categories:</p>
<ol>
<li>Naïve binary</li>
<li>Formatted or encoded binary</li>
<li>Character sequences (text)</li>
</ol>

<p>Binary formats in general suffer from a huge number of problems that there is little value in discussing here, but it
is worth noting that a binary format that is as robust as a textual format is frequently just as verbose as a textual
format, so in the interest of robustness and legibility, this discussion will restrict itself to textual formats</p>

<p>Obviously, a language-neutral notation will have to consist of standardized elements and possibly codes. Fortunately,
a standard like this already exists: ISO 8601.  Since all the work with a good language-neutral notation has already
been done, it would be counter-productive in the extreme to reinvent one.  However, ISO 8601 is fairly expensive from
the appropriate sources and also chock full of weird options, like most compromise standards, so in the interest of
solving some problems with its use, only the extended format of this standard will be employed in this paper.</p>

<p>A language-neutral notation will need to satisfy most, if not all, of the needs satisfied by natural language
notations, but some latitude is necessary when dealing with relative times -- after all, the purpose of the
language-neutral notation is to remove ambiguity and make assumptions more if not completely explicit.  ISO 8601 is
sufficient to cover these needs:</p>

<ul>
<li>absolute positions in time</li>
<li>duration</li>
<li>period with absolute start and end</li>
<li>period with absolute start or end and duration</li>
</ul>

<p>The needs not covered are mostly related to user convenience with respect to the present and absolute positions in
time in its immediate vicinity. E.g., the omission of the date when referring to yesterday, tomorrow, the most recent
occurrence of a time of day, and the forthcoming occurrence of a time of day.  To make this more convenient, the
notation employed in the <tt>LOCAL-TIME</tt> concept described below has special syntax for these relative times.</p>

<p>The full, extended format of ISO 8601 is as follows:</p>

<blockquote><p><tt>1999-10-11T11:10:30,5-07:00</tt></p></blockquote>

<p>The elements are, in order:</p>

<ol>
<li>the year with four digits</li>
<li>a hyphen (omitted in the basic format)</li>
<li>the month with two digits</li>
<li>a hyphen (omitted in the basic format)</li>
<li>the day of month with two digits</li>
<li>the letter T to separate date and time</li>
<li>the hour in the 24-hour system with two digits</li>
<li>a colon (omitted in the basic format)</li>
<li>the minute with two digits</li>
<li>a colon (omitted in the basic format)</li>
<li>the second with two digits</li>
<li>a comma</li>
<li>the fraction of the second with unlimited precision</li>
<li>a plus sign or hyphen (minus) to indicate sign of time zone</li>
<li>the hours of the time zone with two digits</li>
<li>a colon (omitted in the basic format)</li>
<li>the minutes of the time zone with two digits</li>
</ol>

<p>The rules for omission of elements are quite simple.  Elements from the time of day may be omitted from the right and
take their immediately preceding delimiter with them.  Elements from the date may be omitted from the left, but leave
the immediately following delimiter behind. When the year is omitted, it is replaced by a hyphen.  Elements of the date
may also be omitted from the left, provided no other elements follow, in which case they take their immediately
preceding delimiter with them.  The letter T is omitted if the whole of the time of day or the whole of the date are
omitted.  If an element is omitted from the left, it is assumed to be the current value.  (In other words, omitting the
century is really dangerous, so I have even omitted the possibility of doing so.)  If an element is omitted from the
right, it is assumed to cover the whole range of values and thus be indeterminate.</p>

<p>Every element in the time specification needs to be within the normal bounds.  There is no special consideration for
leap seconds, although some might want to express them using this standard.</p>

<p>A duration of time has a separate notation entirely, as follows:</p>

<blockquote><p><tt>P1Y2M3DT4H5M6S&gt;</tt></p></blockquote>

<p>The elements are, in order:</p>

<ol>
<li>the letter P to indicate a duration</li>
<li>the number of years</li>
<li>the letter Y to indicate years</li>
<li>the number of months</li>
<li>the letter M to indicate months</li>
<li>the number of days</li>
<li>the letter D to indicate days</li>
<li>the letter T to separate dates from times</li>
<li>the number of hours</li>
<li>the letter H to indicate hours</li>
<li>the number of minutes</li>
<li>the letter M to indicate minutes</li>
<li>the number of seconds</li>
<li>the letter S to indicate seconds</li>
</ol>

<p>or for the second form, usually used alone</p>

<ol>
<li>the letter P to indicate a duration</li>
<li>the number of weeks</li>
<li>the letter W to indicate weeks</li>
</ol>

<p>Any element (number) may be omitted from this specification and if so takes its following delimited with it.  Unlike
the absolute time format, there is no requirement on the number of digits, and thus no requirement for leading
zeros.</p>

<p>A period of time is indicated by two time specifications, at least one of which has to be absolute, separated by a
single solidus (slash), and has the general forms as follows:</p>

<blockquote><p>start<tt>/</tt>end</p></blockquote>

<p>the end form may have elements of the date omitted from the left with the assumption that the default is the
corresponding value of the element from the start form.  Omissions in the start form follow the normal rules.</p>

<p>The standard also has specifications for weeks of the year and days of the week, but these are used so rarely and are
aesthetically displeasing so are gracefully elided from the presentation.</p>

<p>When discussing the read/write syntax of the <tt>LOCAL-TIME</tt> concept below, the above formats will be employed
with very minor modifications and extensions.</p>

<h2>4 Geography</h2>

<p>It is amusing that when people specify a time, they tend to forget that they looked at their watches or asked other
time-keeping devices at a particular geographic location.  The value they use for <q>current time</q> is colored by this
location so much that the absence of a location at which we have the current time, renders it completely useless -- it
could be specified in any one of the about 30 (semantically different) timezones employed around the planet.  This is
particularly amusing with statements you find on the web:</p>

<blockquote><p><tt>This page was updated 7/10/99 2:00 AM.</tt></p></blockquote>

<p>This piece of information is amazingly useless, yet obviously not so to the person who knows where the machine is
located and who wrote it in the first place.  Only by monitoring for changes to this statement does it have any value at
all.  Specifications of time often has this purpose, but the belief that they carry information, too, is quite
prevalent. The only thing we know about this time specification is that it was made in the past, which may remove most
of the ambiguity, but not quite all -- it could be <tt>1999-07-10.</tt></p>

<p>The geographical origin of a time specification is in practice necessary to understand it.  Even with the standard
notation described above, people will want to know the location of the time.  Unfortunately, there is no widely adopted
standard for geographical locations.  Those equipped with <tt>GPS</tt> units may use ICBM or grid coordinates, but this
is almost as devoid of meaning as raw IP addresses on the Internet.  Above all, geography is even more rife with names
and naming rules that suffer from translation than any other information that cries for a precise standard.</p>

<p>Time zones therefore double as indicators of geographical location, much to the chagrin of anyone who is not from the
same location, because they use names and abbreviations of names with local meaning.  Of course.  Also, the indication
of the daylight saving time in the timezone is rather amusing in the probably unintentional complexity they
introduce. For instance, the Middle or Central European Time can be abbreviated MET or CET, but the <q>summer time</q>
as it is called here is one of MEST, CEST, MET DST, or CET DST.  Add to this that the <q>S for summer</q> in the former
two choices is often translated, and then we have the French.</p>

<p>The only good thing about geography is that most names can be translated into geographical coordinates, and a mapping
from coordinates to time zone and daylight saving time rules is fairly easy to collect, but moderately difficult to
maintain.  This work has been done, however, and is distributed with most Unix systems these days, most notably the free
ones, for some value of <q>free</q>.  In order for a complete time representation to work fully with its environment,
access to this information is necessary. The work on the <tt>LOCAL-TIME </tt>concept includes an interface to the
various databases available under most Unix systems.</p>

<h2>5 Perspective</h2>

<p>An important part of the Y2K problem has been that the information about the perspective on the time stored was
lost. Trivialities like the fact that people were born in the past, bills were paid in the past and fall due in the
future, deliveries will be made in the future, etc, and most of the time, meaningful specifications of time have hard
boundaries that they cannot cross.  Few people have problems with credit cards that expire <tt>02/02</tt>, say.  This
was very obviously not <tt>1902-02</tt>.  The perspective we bring to time specifications usually last beyond the
particular time specified.</p>

<p>When dealing with a particular time, it is therefore necessary to know, or to be told, whether it refers to the past
or the future, and whether the vantage point is different from the present.  If, for instance, a delivery is due
<tt>10/15/99</tt>, and it fails to be delivered that day, only a computer would assume that it was now due
<tt>2099-10-15</tt>. Unfortunately, there is no common practice in this area at all, and most people are satisfied with
a tacit assumption.  That is in large part what caused the Y2K problem to become so enormously expensive to fix.  Had
the assumed, but now missing information been available, the kinds of upgrades required would have been different, and
most likely much less expensive.</p>

<p>There is more to the perspective than just past and future, however. Most computer applications that are concerned
with time are so with only one particular time: the present.  We all expect a log file to be generated along with the
events, and that it would be disastrous if the computer somehow recorded a different time than the time at which an
event occurred, or came back to us and revised its testimony because it suddenly remembered it better.  Modern society
is disproportionately dependent on a common and coordinated concept of the present time, and we have increasingly let
computers take care of this perspective for us.  Telephones and computers, both voice and electronic radio broadcasts,
watches, wall clocks, the trusty old time clocks in factories where the workers depended on its accuracy, they all
portray this common concept of a coordinated understanding of which time it is.  And they all disagree slightly.  A
reportedly Swiss saying goes: <q>A man with one clock knows the time.  A man with two clocks does not.</q></p>

<p>Among the many unsolved problems facing society is an infrastructure for time-keeping that goes beyond individual,
uncoordinated providers, and a time-keeping technology that actually works accurately and is so widely available that
the differences in opinion over what time it is can be resolved authoritatively.  The technology is actually here and
the infrastructure is almost available to everyone, but it is not used by the multitude of purported sources of the
current time.  On the Internet, NTP&gt; (the Network TIme Protocol) keeps fully connected systems in sync, and most
telecommunications and energy providers have amazingly accurate clocks, but mere mortals are still left with alarming
inaccuracies.  This fact alone has a tendency to reduce the interest in accurate representation of time, for the obvious
reason that the more accurate the notation and representation, the less trustworthy the value expressed.</p>

<h2>6 Calculations with Time</h2>

<p>The notation for duration and periods bounded by one absolute position in time and one duration described above have
intuitive meaning, but when pressed for actual meaning, suffer somewhat from the distressing effects of political time.
For instance, a period of one year that starts <tt>1999-03-01</tt> would end on <tt>2000-02-29</tt> or
<tt>2000-03-01</tt> with equal probability of being correct.  More common problems occur with the varying lengths of
months, but those are also more widely understood and the heuristics are in place to deal with them.</p><p>Less obvious is the problem of adding one day to a particular time of day.  This was the original problem that
spurred the development of the <tt>LOCAL-TIME</tt> concept and its implementation. In brief, the problem is to determine
which two days of the year the day is not 24 hours long.  One good solution is to assume the day is 24 hours long and
see if the new time has a different timezone than the original time.  If so, add the difference between the timezones to
the internal time.  This, however, is not the trivial task it sounds like it should be.</p>

<p>The first complication is that none of the usual time functions can report the absolute time that some timezone
identifier will cause a change in the value of timezone as applicable to the time of day.  Resolving this complications
means that we do not have to test for a straddled timezone boundary the hard way with every calculation, but could just
compare with the edge of the current timezone.  Most software currently does this the hard way, including the Unix
<tt>cron</tt> scheduler.  However, if we accept the limitation that we can work with only one timezone at a time, this
becomes much less of a problem, so Unix and C people tend to ignore this problem.</p>

<p>The second complication is that there really is no way around working with an internal time representation in any
calculation -- attempts to adjust elements of a decoded time generally fail, not only because programmers are forgetful,
but also because the boundary conditions are hard to enumerate.</p>

<p>Most often, however, calculations fall into two mutually exclusive categories:</p>

<ol>
<li>calculations with the time of day possibly including days</li>
<li>calculations with the date with no concept of a time of day</li>
</ol>

<p>When time is represented internally in terms of seconds since an epoch, only the former is easy -- the latter is
irrevocably linked with all the timezone problems.  The latter may in particular be calculated without reference to
timezones at all, and indeed should be conducted in <tt>UTC</tt>. As far as the author knows, there are no tools or
packages available in modern programming languages or environments that provide significant support for calculations
with dates apart from calculation with times of day -- these are usually deferred to the application-level, and appear
not to have been solved as far as the application programmer is concerned.</p>

<h2>7 Historic Randomness</h2>

<p>The Roman tradition of using Ante Meridiem and Post Meridiem to refer to the two halves have survived into English,
despite the departure from the custom of changing the day of the month at noon.  The Meridiem therefore has a very
different role in modern usage than in ancient usage. This legacy notation also carries a number system that is fairly
unusual. As seen from members of the 24-hour world, the order 12,1,2,...11,12,1,2,...,11 as mapped onto 0,1,2...,23 is
not only confusing, it is nearly impossible to make people believe that 13 hours have elapsed from 11 AM to 12 AM.  For
instance, several Scandinavian restaurants are open only 1 hour a day to tourists from the world of the 12-hour clock,
but open 13 hours a day to natives of the world of the 24-hour clock.</p>

<p>The Roman tradition of starting the year in the month of March has also been lost.  Most agrarian societies were far
more interested in the onset of spring than in the winter solstice, even though various deities were naturally
celebrated when the sun returned Most calendars were designed by people who made no particular effort to be general or
accurate outside their own lifetime or needs, but Julius Cæsar decided to move the Roman calendar back two months, and
thus it came to be known as the Julian calendar.  This means that month number 7, 8, 9, and 10 suddenly came in as
number 9, 10, 11, and 12, but kept their names: September, October, November, December.  This is of interest mostly to
those who remember their Latin but far more important was the decision to retain the leap day in February.  In the old
calendar, the leap day was added at the end of the year, as makes perfect sense, when the month was already short, but
now it is squeezed into the middle of the first quarter, complicating all sorts of calculations, and affecting how much
people work.  In the old days, the leap day was used as an extra day for the various fertility festivities.  You would
just <i>have</i> to be a cæsar to find this unappealing.</p>

<p>The Gregorian calendar improved on the quadrennial leap years in the Julian calendar by making only every fourth
centennial a leap year, but the decision was unexpectedly wise for a calendar decision.  It still is not accurate, so in
a few thousand years, they may have to insert an extra leap day the way we introduce leap seconds now, but the
simplicity of the scheme is quite amazing: a 400-year cycle not only starts <tt>2000-03-01</tt> (as it did
<tt>1600-03-01</tt>), it contains an even number of weeks: 20,871.  This means that we can make do with a single
400-year calculation for all time within the Gregorian calendar with respect to days of week, leap days, etc.  Pope
Gregory XIII may well have given a similar paper to this one to another unsuspecting audience that probably also failed
to appreciate the elegance of his solution., and 400 more years will pass before it is truly appreciated.</p>

<p>Other than the unexpected elegance of the Gregorian calendar, the world is now quite fortunate to have reached
consensus on its calendars. Other calendars are still used, but we now have a global reference calendar with complete
convertibility.  This is great news for computers. It is almost as great news as the complete intercurrency
convertibility that the monetary markets achieved only as late as 1992.  Before that time, you could wind up with a
different amount of money depending on which currencies you traded obscure currencies like the ruble through. The same
applied to calendars: not infrequently, you could wind up on different dates according as you converted between calendar
systems, similar to the problem of adding a year to February 29 any year and then subtracting a year.</p>

<h2>8. The <tt>LOCAL-TIME</tt> Concept</h2>

<p>The groundwork should now have been laid for the introduction of the several counter-intuitive decisions made in the
design of the LOCAL-TIME concept and its implementation.</p>

<h3>8.1 Time Elements as Fixnums</h3>

<p>Unix time has the <q>advantage</q> that it is representable as a 32-bit machine integer.  It has the equal
disadvantage of not working if the time is not representable as a 32-bit machine integer, and thus can only represent
times in the interval <tt>1901-12-13T20:45:52/2038-01-19T03:14:07</tt>. If we choose an unsigned machine integer, the
interval is <tt>1970-01-01T00:00:00/2106-02-07T06:28:16</tt>. The Common Lisp <tt>UNIVERSAL-TIME</tt> concept has the
disadvantage that it turned into a bignum on most 32-bit machines on <tt>1934-01-10T13:37:04</tt> and runs out of 32
bits two years earlier than Unix time, on <tt>2036-02-07T06:28:16</tt>. I find these restrictions to be uncomfortable,
regardless of whether there are any 32-bit computers left in 2036 to share my pain.</p>

<p>Bignum operations are generally far more expensive than fixnum operations, and they have to be, regardless of how
heavily the Common Lisp implementation has optimized them.  It therefore became a pronounced need to work with fixnums
in time-intensive applications.  The decision fell on splitting between days and seconds, which should require no
particular explanation, other than to point out that calculation with days regardless of the time of day is now fully
supported and very efficient.</p>

<p>Because we are very close to the beginning of the next 400-year leap-year cycle, thanks to Pope Gregory, day 0 is
defined to be <tt>2000-03-01</tt>, which much less arbitrary than other systems, but not obviously so. Each 400-year
cycle contains 146,097 days, so an arbitrary decision was made to limit the day to a maximal negative value of -146,097,
or <tt>1600-03-01</tt>. This can be changed at the peril of accurately representing days that do not belong to the
calendar used at the time.  No attempt has been made to accurately describe dates not belonging to the Gregorian
calendar, as that is an issue resolvable only with reference to the borders between countries and sometimes counties at
the many different times throughout history that monarchs, church leaders, or other power figures decided to change to
the Gregorian calendar.  Catering to such needs is also only necessary with dates prior to the conversion of the Russian
calendar to Gregorian, a decision made by Lenin as late as 1918, or any other conversion, such as 1582 in most of
Europe, 1752 in the United States, and even more embarrassingly late in Norway.</p>

<p>Not mention above is the need for millisecond resolution.  Most events on modern computers fall within the same
second, so it is now necessary to separate them by increasing the granularity of the clock representation. This part is
obviously optional in most time processing functions.</p>

<p>The LOCAL-TIME concept therefore represents time as three disjoint fixnums:</p>

<ol>
<li>the number of days since (or until, when negative) <tt>2000-03-01</tt></li>
<li>the number of seconds since the start of the day in Coordinated UniversalTime</li>
<li>the number of milliseconds since the start of the second.</li>
</ol>

<p>All numbers have origin 0.  Only the number of days may be negative.</p>

<p>The choice of epoch needs some more explanation.  Conversion to this system only requires subtracting two from the
month and making January and February part of the previous year.</p>

<p>The moderate size of the fixnums allows us another enormous advantage over customary ways to represent time.  Since
the leap year is now always at the end of the year, it has no bearing on the decoding of the year, month, day, and
day-of-week of the date.  By choosing this odd-looking epoch, the entire problem with computing leap years and days
evaporates.  This also means that a single, moderately large table of decoded date elements may be pre-computed for 400
years, providing a tremendous speed-up over the division-based calculations used by other systems. </p>

<h3>8.2 Timezone Representation</h3>

<p>David Olsen of Digital Equipment Corporation has laid down a tremendous amount of work in collecting the timezones of
the world and their daylight saving time boundaries.  Contrary to the Unix System V approach from New Jersey (insert
appropriate booing for best effect), which codifies a daylight saving time regime only for the current year, and apply
it to all years, David Olsen&#39;s approach is to maintain tables of all the timezone changes.  A particular timezone thus
has a fairly long table of periods of applicability of the specific number of seconds of to add to get local time.  Each
interval is represented by the start and end times of the specific value, the specific value, a daylight saving time
flag, and the customary abbreviation of the timezone.  On most Unix systems, this is available in compiled files in
<tt>/usr/share/zoneinfo/</tt> under names based on the continent and capital of the region in most cases, or more
general names in other cases.  While not perfect, this is probably a scheme good as any -- it is fairly easy to figure
out which to use.  Usually, a table is also provided with geographic coordinates mapped to the timezone file.</p>

<p>For the timezone information, the <tt>LOCAL-TIME</tt> concept implements a package, <tt>TZ</tt>, or <tt>TIMEZONE</tt>
in full, which contains symbols named after the files, whose values are lazy-loaded timezone objects.  Because the
source files for the zoneinfo files are generally not as available as the portably coded binary information, the
information are loaded into memory from the compiled files, thus maintaining maximum compatibility with the other
timezone functions on the system.</p>

<p>In the <tt>LOCAL-TIME</tt> instances, the timezone is represented as a symbol to aid in the ability to save literal
time objects in compiled Lisp files.  The package TZ can easily be autoloaded in systems that support such facilities,
in order to reduce the load-order complexity.</p>

<p>In order to increase efficiency substantially once again, each timezone object holds the last few references to
timezone periods in it, in order to limit the search time.  Empirical studies of long-running systems have showed that
more than 98% of the lookups on a given timezone were for time in the same period, with more than 80% of the remaining
lookups at the neighboring periods, so caching these values made ample sense.</p>

<h3>8.3 Efficiency Considerations in Table Storage</h3>

<p>In order to store 146,072 entries for the days of a 400-year cycle with the decoded year, month, day, and day-of-week
and 86401 entries for the seconds of a day with the decoded hour, minute and second efficiently, various optimizations
were employed.  The naïve approach, to uses lists, consumes approximately 6519K on a 32-bit machine.  Due to their
overhead, vectors did worse.  Since the decoded elements are small, well-behaved unsigned integers, encoding them in bit
fields within a fixnum turns out to save a lot of memory:</p>

<blockquote><pre>+----------+----+-----+---+   +-----+------+------+
|   yyyy   | mm | day |dow|   |hour | min  | sec  |
+----------+----+-----+---+   +-----+------+------+
     10       4    5    3         5     6      6
</pre></blockquote>

<p>This simple optimization meant 7 times more compact storage of the exact same data, with significantly improved
access times, to boot (depending on processor and memory speeds as well as considerations for caching strategies, a
factor of 1.5 to 3 has been measured in production). </p><p>Still, 909K of storage to keep tables of precomputed dates and
times may seem a steep price to pay for the improved performance.  Unsurprisingly, more empirical evidence confirmed
that most dates decoded were in the same century.  Worst case over the next few years, we will access two centuries
frequently, but it is still a waste to store four full centuries.  A reduction to 100 years per table also meant the
number of years were representable in 7 bits, meaning that an specialized vector of type <tt>(UNSIGNED-BYTE 16)</tt>
could represent them all.  The day of week would be lost in this optimization, but a specialized vector of type
<tt>(UNSIGNED-BYTE 4)</tt> of the full length (146097) could hold them if a single division to get the day of week was
too expensive.  It turns out that the day of week is much less used than the other decoded elements, so the specialized
vector was dropped and an option included with the call to the decoder to skip the day of week.</p>

<p>Similarly, by representing only 12 hours in a specialized vector of type <tt>(UNSIGNED-BYTE 16)</tt>, the hour would
need only 4 bits and the lookup could do the 12-hour shift in code.  This reduces the table memory needs to only 156K,
and it is still faster than access to the full list representation.  This compaction yields almost a factor 42
improvement over the naïve approach</p>

<p>For completeness, the bit field layout is now simplified as follows.</p>

<blockquote><pre>+-------+----+-----+     +----+------+------+
| 0-100 |1-12| 1-31|     |0-11| 0-59 | 0-59 |
+-------+----+-----+     +----+------+------+
    7      4    5           4     6      6
</pre></blockquote>

<p>Decoding the day now means finding the 400-year cycle for the day of week, the century within it for the table
lookup, and adding together the values of the centuries and the year from the table, which may be 100 to represent
January and February of the following century.  All of this can be done with very inexpensive fixnum operations for
about 2,939,600 years, after which the day will incur a bignum subtraction to bring it into fixnum space for the next
2,939,600&gt; years.  (This optimization has not actually been implemented.)</p>

<h2>9 Reading and Printing Time</h2>

<p>Common Lisp is renowned for the ability to print and read back almost all of its data types.  The motivation for the
<tt>LOCAL-TIME</tt> concept included the ability to save human-readable timestamps in files, as well as the ability to
store literal time objects efficiently in compiled Lisp files.  The former has been accomplished through the use of the
reader macros.  Ignoring all other possible uses of the <tt>@</tt> character, it was chosen to be the reader macro for
the full representation of a <tt>LOCAL-TIME</tt> object.  Considering the prevalence of software that works with the
<tt>UNIVERSAL-TIME</tt> concept, especially in light of the lack of alternatives until now, <tt>#@</tt> was chosen to be
the reader macro for the <tt>UNIVERSAL-TIME</tt> representation of a time object.  This latter notation obviously loses
the original time zone information and any milliseconds.</p>

<h3>9.1 Timestring Syntax</h3>

<p>The Lisp reader is instructed to parse a timestring following the reader macro characters.  Other functions may call
<tt>PARSE-TIMESTRING</tt> directly.  Such a timestring follows ISO 8601 closely, but allows for a few enhancements and
an additional option: the ability to choose between comma and period for the fractional second delimiter.</p>

<p>Supported formats of the timestring syntax include</p>

<ol>
<li>absolute time with all elements, the default printed format</li>
<li>absolute time with some elements omitted, as per <tt>ISO 8601</tt></li>
<li>absolute time with date omitted, defaulting to the current</li>
<li>absolute time with time omitted, defaulting to <tt>00:00:00Z</tt>.</li>
<li>the most recent occurrence of a time of day, with a leading <tt>&lt;</tt>.</li>
<li>the forthcoming occurrence of a time of day, with a leading <tt>&gt;</tt>.</li>
<li>the time of day specified, but yesterday, with a leading <tt>-</tt>.</li>
<li>the time of day specified, but tomorrow, with a leading <tt>+</tt>.</li>
<li>the current time of day, with a single <tt>=</tt>.</li>
</ol>

<p>Work in progress includes adding and subtracting a duration from the specified time, such as the present, explaining
the use of the <tt>=</tt>, which is also needed to represent periods with one anchor at the present.  The duration
syntax is, however, rife with assumptions that are fairly hard to express concisely and to use without causing
unexpected and unwanted results.</p>

<p>The standard syntax from <tt>ISO 8601</tt> is fairly rich with options.  These are mostly unsupported due to the
ambiguity they introduce.  The goal with the timestring syntax is that positions and periods of time shall be so easy to
read and write in an information-preserving syntax that there will be no need to cater to the information-losing formats
preferred by some only because of their attempt at similarity to their spoken forms.</p>

<h3>9.2 Formatting Timestrings</h3>

<p>Considering that the primary problem with time formats is randomness in the order of the elements, the timestring
formatter for <tt>LOCAL-TIME</tt> objects allows no options in that regard, but allows elements to be omitted as per the
standard.  The loss of 12-hour clocks will annoy a few people for a time, but there is nothing quite like shaking a bad
habit for good.  Of course, the persistent programmer will write his own formatter, anyway, so the default should be
made most sensible for representing time in programs and in lisp-oriented input files.

</p><p>At present, the interface to the timestring formatter is well suited for a call from <tt>FORMAT</tt> control strings
with the <tt>~//</tt> construct, and takes arguments a follows:</p>

<ol>
<li><tt>stream</tt> -- the stream to receive the formatter timestring</li>
<li><tt>local-time</tt> -- the <tt>LOCAL-TIME</tt> instance</li>
<li><tt>universal</tt> -- if true, ignore the timezone and use UTC&gt;. 
This is the colon modifier.</li>
<li><tt>timezone</tt> -- if true, print a timezone specification at the end. 
This is the atsign modifier.</li>
<li><tt>date-elements</tt> -- the number of elements of the date to write,
counted from the right.  This is a number from 0 to 4 (the default
if omitted or <tt>NIL</tt>).</li>
<li><tt>time-elements</tt> -- the number of elements of the time to write,
counted from the left.  This is a number from 0 to 4 (the default
if omitted or <tt>NIL</tt>).</li>
<li><tt>date-separator</tt> -- the character to print between elements of the
date. If omitted or <tt>NIL</tt>, defaults to the hyphen.</li>
<li><tt>time-separator</tt> -- the character to print between elements of the
time.  If omitted or <tt>NIL</tt>, defaults to the colon.  This argument also applies to the timezone when it is
printed, and when it has a minute component.</li>
<li><tt>internal-separator</tt> -- the character to print between the date
and the time elements.  May also be specified as the number 0, to
omit it entirely, which is the default if either the date or the time elements
are entirely omitted, or the letter <tt>T</tt> otherwise.</li>
</ol>

<h3>9.3 Exported <tt>LOCAL-TIME</tt> Symbols</h3>

<ul>
<li><tt>LOCAL-TIME</tt></li>
<li><tt>MAKE-LOCAL-TIME</tt></li>
<li><tt>LOCAL-TIME-DAY</tt></li>
<li><tt>LOCAL-TIME-SEC</tt></li>
<li><tt>LOCAL-TIME-MSEC</tt></li>
<li><tt>LOCAL-TIME-ZONE</tt></li>
<li><tt>LOCAL-TIME&lt;</tt></li>
<li><tt>LOCAL-TIME&lt;=</tt></li>
<li><tt>LOCAL-TIME&gt;</tt></li>
<li><tt>LOCAL-TIME&gt;=</tt></li>
<li><tt>LOCAL-TIME=</tt></li>
<li><tt>LOCAL-TIME/=</tt></li>
<li><tt>LOCAL-TIME-ADJUST</tt></li>
<li><tt>LOCAL-TIME-DESIGNATOR</tt></li>
<li><tt>GET-LOCAL-TIME</tt></li>
<li><tt>ENCODE-LOCAL-TIME</tt></li>
<li><tt>DECODE-LOCAL-TIME</tt></li>
<li><tt>PARSE-TIMESTRING</tt></li>
<li><tt>FORMAT-TIMESTRING</tt></li>
<li><tt>UNIVERSAL-TIME</tt></li>
<li><tt>INTERNAL-TIME</tt></li>
<li><tt>UNIX-TIME</tt></li>
<li><tt>TIMEZONE</tt></li>
<li><tt>LOCAL-TIMEZONE</tt></li>
<li><tt>DEFINE-TIMEZONE</tt></li>
<li><tt>*DEFAULT-TIMEZONE*</tt></li>
</ul>

<h2>10 Conclusions</h2>

<ol>
<li>The absence of a standard notation for time in Common Lisp required all this work.</li>
<li>The presence of International Standards for the representation of time made it all a lot easier.</li>
<li>Time basically has the most messed-up legacy you can imagine.</li>
<li>Pope Gregory XIII made it a little easier on us all.</li>
<li>Adoption of this proposal in Common Lisp systems and applications would
make time a lot easier for almost everyone involved, except users who cling
to the habits that caused the <tt>Y2K</tt> problems.</li>
<li>This package is far from complete.</li>
</ol>

<h2>11 Credits and Acknowledgments</h2>

<p>This work has been funded by the author and by NHST, publishers of Norway&#39;s financial daily, and TDN, their
electronic news agency, and has been a work in progress since late 1997.  My colleagues and managers have been extremely
supportive in bringing this fundamental work to fruition.  In particular, Linn Iré;n Humlekjæ;r and Erik Haugan suffered
numerous weird proposals and false starts but encouraged the conceptual framework and improved on the execution with
their ideas and by lending me an ear.  My management line, consisting of Ole-Martin Halden, Bjørn Hole, and Hasse
Farstad, have encouraged the quality of the implementation and were willing listeners to the many problems and odd ideas
that preceded the realization that this had to be done. </p><p>The great guys at Franz Inc have helped with internal details
in Allegro CL and have of course made a wonderful Common Lisp environment to begin with.  Thanks in particular to
Samantha Cichon and Anna McCurdy for taking care of all the details and making my stays so carefree, and to Liliana
Avila for putting up with my total lack of respect for deadlines.</p> <p>Many thanks to Pernille Nylehn for reading and
commenting on drafts, nudging me towards finishing this work, and for taking care of my cat Xyzzy so I could write this
in peace and deliver it at LUGM &#39;99 without worrying about the little furball&#39;s constant craving for attention, but also
without both their warmth and comfort when computers simply refuse to behave rationally.</p>





</div>]]></content:encoded>
      <guid>http://naggum.no/lugm-time.html</guid>
      <pubDate>Tue, 03 Aug 2021 12:14:34 +0000</pubDate>
      <source>http://naggum.no/lugm-time.html</source>
    </item>
    <item>
      <title>Launch YC: 3D Web; Training; Child privacy; Pregnancy; Life science; Desk rental</title>
      <link>https://news.ycombinator.com/item?id=28049500</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>Here&#39;s the third &#34;Meet the Batch&#34; thread - previous one was <a href="https://news.ycombinator.com/item?id=27996057" rel="nofollow">https://news.ycombinator.com/item?id=27996057</a>. This time I&#39;ve tweaked the title slightly in the hope of doing better re <a href="https://news.ycombinator.com/item?id=27996536" rel="nofollow">https://news.ycombinator.com/item?id=27996536</a>.<p>Here are 6 startups for you to read about and engage with where interested. The initial order is random.</p><p>Lernit (YC S21) - Corporate training program for Latin America - <a href="https://news.ycombinator.com/item?id=28049505" rel="nofollow">https://news.ycombinator.com/item?id=28049505</a></p><p>StandardCode (YC S21) - APIs to easily comply with child privacy laws - <a href="https://news.ycombinator.com/item?id=28049504" rel="nofollow">https://news.ycombinator.com/item?id=28049504</a></p><p>Scispot (YC S21) - Workflow automation for life science - <a href="https://news.ycombinator.com/item?id=28049501" rel="nofollow">https://news.ycombinator.com/item?id=28049501</a></p><p>Muse (YC S21) - Allow anyone to build 3D websites - <a href="https://news.ycombinator.com/item?id=28049502" rel="nofollow">https://news.ycombinator.com/item?id=28049502</a></p><p>Ruth Health (YC S21) - Digital, at-home post-pregnancy care - <a href="https://news.ycombinator.com/item?id=28049503" rel="nofollow">https://news.ycombinator.com/item?id=28049503</a></p><p>Deskimo (YC S21) - Book workspace by the minute in Singapore and Hong Kong - <a href="https://news.ycombinator.com/item?id=28049507" rel="nofollow">https://news.ycombinator.com/item?id=28049507</a></p></div></div>]]></content:encoded>
      <guid>https://news.ycombinator.com/item?id=28049500</guid>
      <pubDate>Tue, 03 Aug 2021 14:19:59 +0000</pubDate>
      <source>https://news.ycombinator.com/item?id=28049500</source>
    </item>
    <item>
      <title>Barbie launches six new dolls celebrating female scientists</title>
      <link>https://www.abc.net.au/news/2021-08-04/barbie-doll-covid-19-vaccine-developer/100349932</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><div><p>An Australian doctor is among six professionals being honoured for their work by having Barbie dolls modelled on them. </p><section role="contentinfo" aria-label="key points" data-component="KeyPoints"><h2 data-component="Heading">Key points:</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span>Kirby White founded Gowns for Doctors which developed reusable gowns for use during the pandemic </li><li data-component="ListItem"><span></span>She is one of six women being honoured with a Barbie made in her likeness</li><li data-component="ListItem"><span></span>The other six include a co-developer of the AstraZeneca vaccine, a frontline doctor and a biomedical researcher</li></ul></section><p>Kirby White is an Australian doctor based in Victoria who pioneered Gowns for Doctors, an initiative behind surgical gowns that can be washed and reused by frontline workers during the pandemic.</p><p>Dr White was inspired to create the gowns after her clinic experienced a shortage just three weeks into the COVID-19 pandemic last year. </p><p>The initiative has produced more than 5,000 gowns since a GoFundMe campaign raised more than $40,000 to develop them.</p><p>Dr White has already won a Local Hero Award in the Australian of the Year Awards. </p><figure role="group" data-print="inline-media" aria-labelledby="100350140" data-component="Figure" data-uri="coremedia://imageproxy/100350140"><div data-component="IntersectionObserver"><p></p></div><figcaption id="100350140" data-component="Figure__figcaption"> <!-- -->More than 5,000 reusable gowns have been created through Kirby White&#39;s initiative.<cite data-component="Figure__cite">(<span data-component="Byline"><span data-component="Text"><p>Supplied: Salty Dingo</p></span></span>)</cite></figcaption></figure><p>Now, toymaker Mattel is recognising her and other female scientists with a line of Barbie &#34;role model&#34; dolls.</p><p>Dr White will have a Barbie doll that has a likeness of her, with blonde hair, scrubs and even a hospital gown similar to the one she pioneered. </p><figure role="group" data-print="inline-media" aria-labelledby="100350170" data-component="Figure" data-uri="coremedia://imageproxy/100350170"><div data-component="IntersectionObserver"><p></p></div><figcaption id="100350170" data-component="Figure__figcaption"> <!-- -->The Barbie modelled on Victorian doctor Kirby White. </figcaption></figure><p>British woman Sarah Gilbert, a 59-year-old professor at Oxford University who co-developed the AstraZeneca vaccine, is another scientist to be honoured with a doll.</p><p>The others are emergency room nurse Amy O&#39;Sullivan, who treated the first COVID-19 patient at the Wycoff Hospital in Brooklyn, New York, Audrey Cruz, a frontline doctor in Las Vegas who fought discrimination, Chika Stacy Oriuwa, a Canadian psychiatry resident at the University of Toronto who battled systemic racism in healthcare, and Brazilian biomedical researcher Jaqueline Goes de Jesus, who led sequencing of the genome of a COVID-19 variant in Brazil.</p><p>&#34;It&#39;s a very strange concept having a Barbie doll created in my likeness,&#34; Professor Gilbert said in an interview for Mattel.</p><p>Professor Gilbert chose non-profit organisation WISE (Women in Science &amp; Engineering), which is dedicated to inspiring girls to consider a career in STEM, to receive a financial donation from the toymaker. </p><p><strong>Reuters </strong></p></div></div></div>]]></content:encoded>
      <guid>https://www.abc.net.au/news/2021-08-04/barbie-doll-covid-19-vaccine-developer/100349932</guid>
      <pubDate>Wed, 04 Aug 2021 13:35:47 +0000</pubDate>
      <source>https://www.abc.net.au/news/2021-08-04/barbie-doll-covid-19-vaccine-developer/100349932</source>
    </item>
    <item>
      <title>Ancient Roman ship laden with wine jars discovered off Sicily</title>
      <link>https://www.theguardian.com/world/2021/jul/28/ancient-roman-ship-discovered-off-coast-of-sicily</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div tabindex="0" id="maincontent"><div><p>An ancient Roman vessel dating back to the second century BC has been discovered in the Mediterranean Sea off the coast of Palermo.</p><p>The ship lies 92 metres (302ft) deep in the ocean, near Isola delle Femmine,<em> </em>and from the first images taken by a submarine robot it was carrying a copious cargo of wine <em>amphorae</em>.</p><p>“The Mediterranean continually gives us precious elements for the reconstruction of our history linked to maritime trade, the types of boats, the transport carried out,’’ said the superintendent of the sea of the Sicilian region, Valeria Li Vigni, who launched the expedition. “Now we will know more about life on board and the relationships between coastal populations.’’</p><p>The discovery was described by the Sicilian authorities as one of the most important archaeological finds of recent years.</p><p>A few weeks ago, Sicilian archeologists discovered another wreck: an ancient Roman ship about 70 metres deep near the island of Ustica. That ship also carried a huge load of <em>amphorae</em>, containing wine dating back to the second century BC.</p><p>The findings will shed light on Rome’s trade activity in the Mediterranean, where the Romans traded spices, wine, olives and other products in north Africa, Spain, France and the Middle East.</p><p>There are numerous wrecks of Roman ships throughout the Mediterranean, such as the almost intact Roman ship from the second century BC found in 2013 off the coast of Genoa. The vessel, which contained roughly 50 valuable <em>amphorae</em>, was spotted by police divers, roughly one mile from the shore of Alassio, 50 metres underwater.</p><p>In that case, police were tipped off about the whereabouts of the boat during a year-long investigation into purloined artefacts sold on the black market in northern <a href="https://www.theguardian.com/world/italy" data-component="auto-linked-tag" data-link-name="in body link">Italy</a>.</p><p>Every year, hundreds of ancient Roman <em>amphorae</em>, taken illegally, are found by the Italian police in the homes of art dealers.</p><p>In June, <a href="https://www.theguardian.com/world/2021/jun/21/italy-recovers-nearly-800-illegally-gathered-archaeological-finds" data-link-name="in body link">Italian authorities recovered hundreds of illegally gathered archeological finds</a> from a Belgian collector, dating as far back as the sixth century BC and worth €11m (£9.4m).</p><p>The nearly 800 pieces “of exceptional rarity and inestimable value”, including <em>stelae</em>, <em>amphorae</em> and other items, came from clandestine excavations in Puglia, in Italy’s south-eastern tip, according to the <em>carabinieri</em> in charge of cultural heritage. The collector is awaiting trial.</p></div></div></div>]]></content:encoded>
      <guid>https://www.theguardian.com/world/2021/jul/28/ancient-roman-ship-discovered-off-coast-of-sicily</guid>
      <pubDate>Sun, 01 Aug 2021 23:50:09 +0000</pubDate>
      <source>https://www.theguardian.com/world/2021/jul/28/ancient-roman-ship-discovered-off-coast-of-sicily</source>
    </item>
    <item>
      <title>A Bestiary of Functions for Systems Designers</title>
      <link>https://brunodias.dev/2021/03/19/functions-for-system-designers.html</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Whether your game surfaces its numbers to the player or not, odds are it has underlying systems that rely on them, and you use functions to determine how those numbers affect each other. In other words, a mathematical function is usually at the core of the answer to a bunch of frequent game design questions.</p>

<!--more-->

<ul>
  <li>If I increase my strength, how much does my damage grow?</li>
  <li>How fast does damage fall off when shooting a gun outside its normal range?</li>
  <li>How much more expensive is each subsequent upgrade?</li>
  <li>How aggressively does the AI respond to this stimulus?</li>
  <li>How much XP to the next level?</li>
  <li>How fast does the difficulty ramp up in Infinite Mode?</li>
  <li>How much harder do enemies hit in each New Game+?</li>
  <li>How does the player character’s speed change when the sprint button is held?</li>
  <li>How do quest rewards ramp up over the course of the game?</li>
  <li>How much time do I expect players to spend on each level?</li>
</ul>

<p>And so on. Sometimes those questions are about the explicit math undergirding game rules; sometimes they’re more about analysis and intent, about the spreadsheet you’re building to later turn into a loot or progression table that the game will express in some other way.</p>

<p>This isn’t meant as a comprehensive primer on those functions let alone some sort of taxonomy. It’s more a collection of interesting specimens, alongside histories, use cases, and observations on their behavior.</p>

<div>
    <p>
    </p>
    
<h4 id="a-note-about-mathematics">A note about mathematics</h4>

<p>There are some mentions here of calculus and some potentially scary notation like \(\frac{d}{dx}\), but hopefully the main gist of everything should be understandable as long as you’re familiar with the idea of functions and the most common mathematical notation. If any part of this is very unclear or you think I’ve made a math error (which is very likely!), a good place to ask me questions is actually <a href="https://forcemeat-ghost.tumblr.com">on tumblr</a>.</p>

<h4 id="conventions-in-this-article">Conventions in this article</h4>
<p>The word <em>parameter</em> has different meaning to programmers and mathematicians. Usually, in programming, we think of any input of a function as a parameter; if I define a function like <code>(x) -&gt; pow(x, a)</code>, we think of <em>x</em> as the parameter and <em>a</em> as some constant defined elsewhere in the code.</p>

<p>In mathematics it’s basically the other way around. Parameters are known or constant values for a given “version” of a function, and the inputs of the function are <em>arguments.</em> So in math, if I define a function like \(f(x) = x^a\), then <em>a</em> is the parameter.</p>

<p>So instead of talking about parameters, I’ll be talking about <em>arguments</em> (the variables that go into a function) and <em>coefficients</em> (the constant values that adjust a function) instead.</p>

<p>For variables, I’ll follow the usual mathematical convention that arguments are usually \(x, y,\) or \(z\). Anything else is a coefficient; \(a, b, c\) are usually additive or multiplicative, \(s\) is usually an exponent, \(k\) is usually a counting number.</p>


</div>

<h2 id="the-functions">The Functions</h2>

<h3 id="the-linear-function">The Linear Function</h3>

<p></p>

<p>Let’s first establish a baseline by talking about the simplest function, \(f(x) = bx + a\). This is useful to point out the basics of parametrization – by adjusting \(b\), we can change the function’s slope to make it steeper or shallower; by adjusting \(a\) we can shift the output up or down by some amount.</p>

<p>The main reason to use a linear curve is simplicity and clarity. Players can grasp a linear system pretty intuitively, and those functions will hold to several properties that players will expect. For example, if you double \(x\), then \(f(x)\) will also double. If you increase \(x\) by some value \(y\), then \(f(x)\) will increase by the same amount no matter what \(x\) was. This kind of clarity can be invaluable to players who are trying to optimize something.</p>

<p>This simplicity, of course, precludes more complex behaviors. In a lot of game systems, players won’t be cognizant of the underlying numbers and will be learning how the system works by “feel” – in which case, using an easily graspable linear function might not even be any more clear to the player at all. And of course sometimes we <em>don’t</em> want game systems to be immediately clear.</p>

<div>
    <p>
    </p>
    
<p>For the rest of these functions, I’ll usually be skipping over various multiplicative and additive coefficients you can throw into these functions to shift or scale their outputs, just so we don’t get lost in symbolic soup.</p>

</div>

<h3 id="quadratic-cubic-and-other-power-functions">Quadratic, Cubic, and other Power Functions</h3>

<p>There are several “families” of power functions but at first we should talk about \(f(x) = x^b\), where \(b\) is some small counting number greater than 1, like 2, 3, or 4.</p>

<p>Power functions “feel” very different in the range \(0 \leq x \leq 1 \) than they do when \(x \gt 1\). Inside the unit square, they “bulge” down and away from the linear function, meeting at the 0 and 1 points. Outside that square, \(f_{\mathrm{linear}}(x) \lt f_{\mathrm{quadratic}}(x)\), and the slope increases to infinity.</p>

<p></p>

<p>If you expect a normalized input where \(0 \leq x \leq 1\), power functions are often used to add “juice” to animations or character movement, as easing functions, and so on.</p>

<p>The “outer” range where \(x \gt 1\) is more interesting in designing systems. Power functions give you accelerating growth, which feels powerful for players and, indeed, is inherently dangerous.</p>

<p>Say the player character’s damage and enemy hit points both scale exponentially with their level; say \(f_\mathrm{damage}(x_\mathrm{level}) = x_\mathrm{level}^2\). In practice, the game stays at the same balance if the player is of the same level as the enemies, but the numbers grow explosively. As we know, player happy when number big; this is the kind of thing that works well for games with visible damage numbers.</p>

<p>However, as the player gains levels, each subsequent level is a little less significant. You can understand why by looking at a graph of \(\frac{f(x)}{f(x) + 1}\):</p>

<p></p>

<p>A level 1 player fighting a level 2 enemy might as well be doing 25% damage, whereas the difference between level 10 and level 11 is nowhere near as dramatic. This isn’t nearly as noticeable if x is always a bigger integer, of course, but it’s still there.</p>

<div>
    <p>
    </p>
    
<p>In calculus terms, \(\ddx f(x)\) increases as \(x\) increases, but not as fast as \(f(x)\) increases. So while each subsequent level has a bigger <em>absolute</em> effect, in <em>relative</em> terms the percent increase in damage from one level to the next is slowing down.</p>

</div>

<p>Everything is relative, so the “feel” of a growth curve will depend on what it’s being compared against. To illustrate this with power curves, consider an RPG where:</p>

<ul>
  <li>As the player increases in level and enters higher-level content, quest rewards (in money) scale quadratically;</li>
  <li>The prices of consumables stay constant;</li>
  <li>The prices of equipment grows linearly;</li>
  <li>The cost of resurrecting a dead party member grows quadratically;</li>
  <li>The cost of retraining your character grows <em>cubically.</em></li>
</ul>

<p>You can see how all those things interact:</p>
<ul>
  <li>Consumables quickly become unlimited, at least as far as money is concerned. After a few levels you can buy as many potions as you need.</li>
  <li>Equipment becomes cheaper relative to quest rewards, though higher-level equipment is still meaningfully more expensive than low-level equipment. If overleveled equipment is available to buy, it becomes more and more affordable.</li>
  <li>If you lose a party member, the level of effort to pay for their resurrection is always about the same.</li>
  <li>Retraining your character becomes ever more onerous as the game goes on.</li>
</ul>

<p>These kinds of relationships, of course, emerge from using any function against any other function. It’s very useful to look at the ratios between different functions at different stages of the game, and using those relationships is very important in achieving whatever player experience goals you have.</p>

<h3 id="fractional-and-negative-powers">Fractional and negative powers</h3>

<p>Reconsidering \(f(x) = x^s\), we obviously can look at exponents other than 2, 3, and 4. It’s very common to use a value of \(s\) that’s a fractional number to fine-tune the curve and make it exactly as dramatic as intended.</p>

<p>But there are two other useful cases. The first is when \(s\) is some fractional number between 0 and 1. These are <em>diminishing returns</em> functions, where the rate of change decreases as \(x\) increases, but \(f(x)\) keeps increasing forever.</p>

<p></p>

<p>This kind of function has many, many applications. First of all, it front-loads the effect of \(x\); you get the most benefit from having a little of it, instead of stacking a lot of it. A good use for this might be something like damage reduction. Say you can equip a certain amount \(x\) of cold-resistant items, and all together they negate \(10x^{0.5}\) cold damage. Your first cold-resistant item negates 10 damage. But to double that protection and negate 20 damage, you have to <em>quadruple</em> the number of cold-resistant items you have equipped. Putting on another warm pair of mittens, thermal vest, or ushanka will always increase the amount of damage reduction the player gets, but having <em>some</em> is vastly more important and noticeable than having <em>a lot.</em></p>

<p>Finally, we come to negative powers. If you don’t remember how negative powers work: raising something to a negative power is the same thing as taking the inverse of a positive power; that is:</p>

<p>\[x^{-s} = \frac{1}{x^s}\]</p>

<p>The latter might be easier to reason about, and is probably computationally faster, if that matters for your application.</p>

<p></p>

<p>Note that for a function like \(f(x) = x^{-s}\), the value for \(x = 0\) is undefined, and it approaches infinity. This tends to not be useful, so you will usually see this in the form \(f(x) = (x + 1)^{-s}\), which normalizes it so that \(f(0) = 1\).</p>

<p>You can think of this as an easing-out function; it slows down as \(x\) increases, though it never quite plateaus. Effects are very front-loaded; in the graph above, the first point of \(x\) will <em>halve</em> \(y\), but subsequent points are nowhere near as efficient. But the time we’re at 4, effects are pretty marginal.</p>

<p>The clearest use case for this is as a multiplier for something else which reduces it – giving the player a discount based on their Barter skill, reducing incoming damage based on their armor, and so on. Because of its slowing-down nature, it’s useful for negative effects that can be stacked – for example, reducing the player’s speed as they accumulate stacks of a debuff. Subsequent stacks will always have an effect, but the majority of the impact comes from the first one.</p>

<div>
    <p>
    </p>
    
<p>For this article, I’m not really thinking about performance considerations, but they might matter in your game; if a function has to run hundreds of times a second, that might constrain what operations you can really use, depending on context.</p>

<p>As a rule, addition, subtraction, and multiplication are safe to use; division and raising things to fractional or negative powers are computationally expensive.</p>

</div>

<h3 id="exponential-growth">Exponential Growth</h3>

<p></p>

<p>The exponential function, \(f(x) = b^x\), in which \(b \gt 1\). Shown above, specifically, is \(e^x - 1\).</p>

<p>Like the power functions, the exponential function grows to infinity. <em>Unlike</em> with the power function, the rate of growth keeps accelerating in pace with the growth of the value itself, so the relative multiplicative value of each increase stays the same.</p>

<div>
    <p>
    </p>
    
<p>Or, again in calculus terms, \(\ddx f(x)\) is just some multiple of of \(f(x)\), such that \(\frac{\ddx f(x)}{f(x)}\) stays constant.</p>

</div>

<p>Exponential growth blows up even more than quadratic or cubic growth; if those functions created big, player-pleasing numbers, exponential functions are liable to create even bigger ones. It may make sense to use an exponential function when:</p>

<ul>
  <li>You want the numbers to truly explode. Maybe you’re exposing them to the player and your game is <em>about</em> seeing that giant pile of zeroes. Maybe your game deals with a huge range of scales. Maybe the input has a small range but you want that small range to translate into <em>dramatic</em> changes in output.
    <ul>
      <li>Example: An idle game like <em>Cookie Clicker,</em> where the numbers exploding is the point.</li>
      <li>Example: A comical physics-driven golf game where the joke is that the 9-iron shoots the ball \(2^9\) times further than the 0-iron.</li>
    </ul>
  </li>
  <li>You want multiplicative consistency; that is, the absolute increase when you increase the input will grow, but the ratio between level 10 and level 11 is the same as the ratio between level 1 and level 2.</li>
  <li>You’re not worried about game balance, either because the input of the function is strictly bounded, or because you just don’t care.
    <ul>
      <li>Example: Player hit point growth is exponential, but the base is small and the game has a level cap, so we know what the min and max values are.</li>
      <li>Example: Again, an idle game where the joke <em>is</em> that the numbers explode and are unbounded, until the game itself breaks.</li>
    </ul>
  </li>
</ul>

<p>Keep in mind that while exponential curves usually look like very sharp “hockey stick” curves, you can have a shallow exponential curve! Just use a base that’s barely above 1. Say you use exponential scaling for the player’s hit points, based on their level, and your game is capped at 20 levels. You can define a function like \(f(x) = 8 \times 1.25^x\). This leads to numbers that feel relatively sane – the player starts with 10 hit points at level 1 and will have nearly 700 at level 20 – but still increase rapidly in absolute terms, while keeping the relative increase.</p>

<p>Exponential functions can also be surprisingly intuitive. The \(8 \times 1.25^x\) example above is essentially “8 hit points at level 0, and a 25% increase each level.” This can be actively easier for players to understand, in some contexts, than a quadratic function.</p>

<p>An useful perceptual trick when you are exposing numbers to the player – directly or indirectly through something like a bar that grows in length – is to make each <em>absolute</em> increase bigger than the last one, giving the player a sensation of rising power, while maintaining their <em>relative</em> marginal value. The exponential function does that.</p>

<p>A close relative to the exponential growth curve is the exponential decay curve. This is given by a function like:</p>

<p>\[f(x) = \frac{1}{b^x}\]</p>

<p>It feels very similar to the negative power curve above, but has a couple of useful features. First, it’s inherently normalized; \(f(0) = 1\), and it always approaches 0 as \(x\) approaches infinity. Like with the “small” exponential curve, it can also be inherently graspable if you use a nice round base.</p>

<p>For example, if \(f(x) = \frac{1}{2^x}\), the output halves at each unit step of \(x\).</p>

<h3 id="the-triangle-number-function">The Triangle Number Function</h3>

<p>Triangle numbers are very interesting. Imagine you’re arranging marbles into an equilateral triangle, with \(n\) marbles to a side. How many marbles do you need in total to complete the triangle?</p>

<p>The answer is always the triangle number for n, \(T_n\). Originally, triangle numbers are defined as an integer sequence:</p>

<p>\[ T_n = \sum^n_{k=1} k\]</p>

<p>That is, the triangle number for \(n\) is just the sum of all integers between 1 and \(n\). But this sum generalizes into a function:</p>

<p>\[ f(x) = \frac{x(x+1)}{2}\]</p>

<p>When you graph this function, it looks something like this:</p>

<p></p>

<p>The triangle function looks a lot like a power function, though it’s a subtly distinct curve. It has a nice feature: \(f(x) - f(x-1) = x\). That is, the difference between one step and the next is the same as the value of the function at that next step.</p>

<p>This makes triangle numbers a very useful curve to apply to costs and milestones, like experience thresholds to level up or upgrading something with successive levels.</p>

<p>The triangle numbers are ingrained in <em>Fallen London;</em> to increase an attribute in FL, you need as many “change points” as the next level. So to go from, say, level 4 to 5, you need 5 change points; to get from 0 to 5, you will need \(T_5 = 15\) change points in total.</p>

<p>This intuitive property was also used in 3rd Edition <em>Dungeons &amp; Dragons</em>, where the XP needed to get to a level \(x\) is \(1000 \times T_{x - 1}\). Therefore, the difference between your current level and the next is always 1000xp times your current level.</p>

<p>If a system exposes the numbers to players, this can be more graspable than a power function while still having many of the same properties.</p>

<h3 id="the-sigmoid-curve">The Sigmoid Curve</h3>

<p>A “sigmoid” is any s-shaped curve. A sigmoid starts at some lower bound, rises steadily, and then smoothly plateaus at some upper bound.</p>

<p>There are many functions with a roughly sigmoid shape, but I’ll talk here about two: The logistic function, \(f_l\), and the algebraic sigmoid \(f_s\).</p>

<p></p>

<p>Above: \(f_l\), in red, and \(f_s\), in blue. Both functions have had their coefficients adjusted so they would mostly line up.</p>

<p>First, the algebraic sigmoid, which has a simpler expression:</p>

<p>\[f_s(x) = \frac{x^\lambda}{\sigma^\lambda + x^\lambda}\]</p>

<p>As far as I know, this sigmoid was first described in the context of game design by <a href="https://www.jfurness.uk/sigmoid-functions-in-game-design/">James Furness</a>. It’s a normalized curve; it always ranges from \(0\) when \(x = 0\) on to \(1\) at some point when \(x\) is large enough.</p>

<p>The two coefficients, \(\sigma\) and \(\lambda\), respectively determine how big \(x\) has to be before the curve reaches its lower bound, and how steeply the curve rises to that point.</p>

<p>There are a lot of nice things about this curve, including that it’s inherently normalized. It also has a useful asymmetry, in that the it starts climbing quickly but plateaus relatively slowly.</p>

<p>The logistic function is widely used in statistical modeling. The traditional parametrized definition is:</p>

<p>\[ f_l(x) = \frac{L}{1 + e^{-k(x-x_0)}}\]</p>

<p>There are a lot of coefficients here, but the nice thing about the logistic function is that all of them have very well-defined effects.</p>

<ul>
  <li>\(L\) is the upper bound, the number the function converges to for high values of \(x\).</li>
  <li>\(k\) is a coefficient that determines how steeply the curve rises; you can make this a negative number to invert the slope of the function and get a logistic decay curve.</li>
  <li>\(e\) is Euler’s number, of course, but for our purposes it’s just an arbitrary base. You could use any number greater than \(1\), though adjusting this value is completely equivalent to adjusting \(k\).</li>
  <li>\(x_0\) is a simple additive coefficient that shifts the curve left or right as desired. The inflection point of the curve is wherever \(x_0\) is set – that is, the midpoint in the rise is exactly at that point.</li>
</ul>

<p>Besides the difference in how easy they are to compute, the main difference is in their skew. The algebraic sigmoid skews early, front-loading the effect of \(x\); the logistic function is perfectly symmetrical.</p>

<p></p>

<p>Above: A graph of the derivatives of the two sigmoid functions. They are both bell curves, but the algebraic sigmoid’s “bump” is noticeably skewed to the left.</p>

<p>What are sigmoids good for? Well:</p>

<ul>
  <li>First, they are normalization functions; they will take an unbounded input and spit out a bounded output. Depending on the kinds of values you expect for the input, that output might not contain a lot of information about the original input, of course.</li>
  <li>Because of their bounded output, sigmoids are great for implementing caps and limits on the growth of something, or for smoothing between two values.</li>
</ul>

<p>Some examples of this:</p>
<ul>
  <li>Damage faloff: It’s very simple to define a logistic curve between, say, 0.5 and 1, which multiplies the damage done by a weapon based on the distance to the target. The logistic function is useful here because different weapons could have different ranges and more or less sharp faloff curves by adjusting \(k\) and \(x_0\).</li>
  <li>Speed bonus: On a game with scoring and leaderboards, a logistic curve could be used to scale a speed bonus based on a par time for the level. Players who complete the level very fast would get the maximum bonus; players who complete the level very slowly would get nothing. In this case, \(x\) is the completion time, and \(k\) is negative, inverting the curve’s slope.</li>
  <li>NPC disposition: Your game is a social simulation and it works by stacking NPC disposition modifiers on top of each other, like <em>Crusader Kings 3</em>. Using a sigmoid can be useful for normalizing an unbounded number into a bounded value, for example as an input to a facial animation system.</li>
</ul>

<h2 id="in-conclusion">In conclusion…</h2>

<p>Again, this is a <em>bestiary;</em> a limited selection of interesting specimens. There are more use cases and examples than the ones here, but I hope those work as a useful starting point for folks.</p>

<p>If you have a question, want to point out an error, or have a suggestion of something I didn’t include, the best place to ask questions is through <a href="https://forcemeat-ghost.tumblr.com">tumblr</a>.</p>

<p>I’m not the first to apply any of those ideas to game design, and a lot of this is widely known at least in some subset of the industry. Much of my thinking here (and desire to document it) has emerged from conversations with colleagues, in particular Emily Short, who originally pointed out the logistic curve to me.</p>

<p>This article is also inspired by some other pieces on the subject:</p>

<ul>
  <li><a href="https://www.jfurness.uk/sigmoid-functions-in-game-design/">James Furness’ article on sigmoids</a></li>
  <li><a href="https://medium.com/@pedro.camara/sigmoid-curves-are-game-designers-friends-8b1f5b53d2fc">Pedro Gardel Câmara’s article on sigmoids</a></li>
  <li><a href="https://www.youtube.com/watch?v=mr5xkf6zSzk">Squirrel Eiserloh’s talk on functions for easing</a></li>
</ul>

  </div>

  
</article>

      </div>
    </div></div>]]></content:encoded>
      <guid>https://brunodias.dev/2021/03/19/functions-for-system-designers.html</guid>
      <pubDate>Tue, 03 Aug 2021 06:03:50 +0000</pubDate>
      <source>https://brunodias.dev/2021/03/19/functions-for-system-designers.html</source>
    </item>
    <item>
      <title>How to Build Resilient JavaScript UIs</title>
      <link>https://www.smashingmagazine.com/2021/08/build-resilient-javascript-ui/</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

        <div id="article__content">

          <div>

            

              
              


<ul>

  
  <li>
    12 min read
     </li>




  <li>
    
    
      
        <a href="https://www.smashingmagazine.com/category/ui">UI</a>,
      
    
      
        <a href="https://www.smashingmagazine.com/category/ux">UX</a>,
      
    
      
        <a href="https://www.smashingmagazine.com/category/workflow">Workflow</a>,
      
    
      
        <a href="https://www.smashingmagazine.com/category/javascript">JavaScript</a>
      
    
  </li>





  


</ul>


            


            

							
							
								
							
								
									
								
							
								
									
								
							
								
							
								
									
								
							
								
							
							

							
              

							

							
            

            <p id="article__start">
              <section aria-label="quick summary">
                <span>Quick summary ↬ </span>
                
                  Embracing the fragility of the web empowers us to build UIs capable of adapting to the functionality they can offer, whilst still providing value to users. This article explores how graceful degradation, defensive coding, observability, and a healthy attitude towards failures better equips us before, during, and after an error occurs.
                
              </section>
            </p>

            

            
							



						

            

<p>Things on the web can break — the odds are stacked against us. Lots can go wrong: a network request fails, a third-party library breaks, a JavaScript feature is unsupported (assuming JavaScript is even available), a CDN goes down, a user behaves unexpectedly (they double-click a submit button), the list goes on.</p>

<p>Fortunately, we as engineers can avoid, or at least mitigate the impact of breakages in the web apps we build. This however requires a conscious effort and mindset shift towards thinking about unhappy scenarios just as much as happy ones.</p>

<p><strong>The User Experience (UX) doesn’t need to be all or nothing — just what is usable.</strong> This premise, known as graceful degradation allows a system to continue working when parts of it are dysfunctional — much like an electric bike becomes a regular bike when its battery dies. If something fails only the functionality dependent on that should be impacted.</p>

<p>UIs should adapt to the functionality they can offer, whilst providing as much value to end-users as possible.</p>

<h2 id="why-be-resilient">Why Be Resilient</h2>

<p>Resilience is <a href="https://www.w3.org/TR/html-design-principles/#degrade-gracefully">intrinsic to the web</a>.</p>

<p>Browsers ignore invalid HTML tags and unsupported CSS properties. This liberal attitude is known as Postel’s Law, which is conveyed superbly by Jeremy Keith in <a href="https://resilientwebdesign.com/chapter4/">Resilient Web Design</a>:</p>

<blockquote>“Even if there are errors in the HTML or CSS, the browser will still attempt to process the information, skipping over any pieces that it can’t parse.”</blockquote>

<p>JavaScript is less forgiving. Resilience is extrinsic. We instruct JavaScript what to do if something unexpected happens. If an API request fails the onus falls on us to catch the error, and subsequently decide what to do. And that decision directly impacts users.</p>

<p>Resilience builds trust with users. A buggy experience reflects poorly on the brand. According to <a href="https://hbr.org/2000/09/knowing-a-winning-business-idea-when-you-see-one">Kim and Mauborgne, convenience (availability, ease of consumption)</a> is one of six characteristics associated with a successful brand, which makes graceful degradation synonymous with brand perception.</p>

<p>A robust and reliable UX is a signal of quality and trustworthiness, both of which feed into the brand. A user unable to perform a task because something is broken will naturally face disappointment they could associate with your brand.</p>

<p>Often system failures are chalked up as “corner cases” — things that rarely happen, however, the web has many corners. Different browsers running on different platforms and hardware, respecting our user preferences and browsing modes (Safari Reader/ assistive technologies), being served to geo-locations with varying latency and intermittency increase the likeness of something not working as intended.</p>

<div data-audience="non-subscriber" data-remove="true"><p><span>More after jump! Continue reading below ↓</span></p>
</div>

<h2 id="error-equality">Error Equality</h2>

<p>Much like content on a webpage has hierarchy, failures — things going wrong — also follow a pecking order. Not all errors are equal, some are more important than others.</p>

<p>We can categorize errors by their impact. How does XYZ not working prevent a user from achieving their goal? The answer generally mirrors the content hierarchy.</p>

<p>For example, a dashboard overview of your bank account contains data of varying importance. The total value of your balance is more important than a notification prompting you to check in-app messages. <a href="https://en.wikipedia.org/wiki/MoSCoW_method">MoSCoWs method of prioritization</a> categorizes the former as a must-have, and the latter a nice to have.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7486947-6755-43ad-9426-5b3773aa7b14/8-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="525" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7486947-6755-43ad-9426-5b3773aa7b14/8-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7486947-6755-43ad-9426-5b3773aa7b14/8-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7486947-6755-43ad-9426-5b3773aa7b14/8-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7486947-6755-43ad-9426-5b3773aa7b14/8-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7486947-6755-43ad-9426-5b3773aa7b14/8-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7486947-6755-43ad-9426-5b3773aa7b14/8-resilience-is-a-feature.png" sizes="100vw" alt="Wireframe of a banking website. Black text on a white background. The left side displays the account balance of £500. The top right contains a notification (bell) icon and count of 3. Below the icon is a popup displaying the 3 unread items."/>
    
    </a>
  

  
    <figcaption>
      An example of primary versus secondary information. The account balance (£500) is primary information integral to the user experience, whereas unread notifications are a non-essential enhancement (secondary information). (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7486947-6755-43ad-9426-5b3773aa7b14/8-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>If primary information is unavailable (i.e: network request fails) we should be transparent and let users know, usually via an error message. If secondary information is unavailable we can still provide the core (must have) experience whilst gracefully hiding the degraded component.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/09ab88dd-1e87-4c40-995f-9cbf380e4fe3/3-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="536" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/09ab88dd-1e87-4c40-995f-9cbf380e4fe3/3-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/09ab88dd-1e87-4c40-995f-9cbf380e4fe3/3-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/09ab88dd-1e87-4c40-995f-9cbf380e4fe3/3-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/09ab88dd-1e87-4c40-995f-9cbf380e4fe3/3-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/09ab88dd-1e87-4c40-995f-9cbf380e4fe3/3-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/09ab88dd-1e87-4c40-995f-9cbf380e4fe3/3-resilience-is-a-feature.png" sizes="100vw" alt="Wireframe of a banking website. A red icon with error message reads: Sorry, unable to load your bank balance. The top right contains a notification (bell) icon."/>
    
    </a>
  

  
    <figcaption>
      When the account balance is unavailable we show an error message. When unread notifications are unavailable we simply remove the count and popup from the UI, whilst preserving the semantic link <code>a href=&#39;/notifications&#39;</code> to the notification center. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/09ab88dd-1e87-4c40-995f-9cbf380e4fe3/3-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>Knowing when to show an error message or not can be represented using a simple decision tree:</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f3b257e8-88c8-4c84-b567-b30641195f2d/14-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="231" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f3b257e8-88c8-4c84-b567-b30641195f2d/14-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f3b257e8-88c8-4c84-b567-b30641195f2d/14-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f3b257e8-88c8-4c84-b567-b30641195f2d/14-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f3b257e8-88c8-4c84-b567-b30641195f2d/14-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f3b257e8-88c8-4c84-b567-b30641195f2d/14-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f3b257e8-88c8-4c84-b567-b30641195f2d/14-resilience-is-a-feature.png" sizes="100vw" alt="Decision tree with 2 leaf nodes that read (from left to right): Primary error? No: Hide degraded component, Yes: Show error message."/>
    
    </a>
  

  
    <figcaption>
      Primary errors should surface to the UI, whereas secondary errors can be gracefully hidden. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f3b257e8-88c8-4c84-b567-b30641195f2d/14-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>Categorization removes the 1-1 relationship between failures and error messages in the UI. Otherwise, we risk bombarding users and cluttering the UI with too many error messages. Guided by content hierarchy we can cherry-pick what failures are surfaced to the UI, and what happen unbeknownst to end-users.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/86ca5913-57f4-44e7-a77f-20a119a1de39/2-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="388" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/86ca5913-57f4-44e7-a77f-20a119a1de39/2-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/86ca5913-57f4-44e7-a77f-20a119a1de39/2-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/86ca5913-57f4-44e7-a77f-20a119a1de39/2-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/86ca5913-57f4-44e7-a77f-20a119a1de39/2-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/86ca5913-57f4-44e7-a77f-20a119a1de39/2-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/86ca5913-57f4-44e7-a77f-20a119a1de39/2-resilience-is-a-feature.png" sizes="100vw" alt="Two wireframes of different error states. The left one titled: Error message per failure, displays 3 red error notifications (1 for each failure). The right one titled: Single error message with action, shows a single error notification with a blue button below."/>
    
    </a>
  

  
    <figcaption>
      Just because 3 errors occurred (left) doesn’t automatically mean 3 error messages should be shown. An action, such as a retry button, or a link to the previous page helps guide users what to do next. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/86ca5913-57f4-44e7-a77f-20a119a1de39/2-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<h2 id="prevention-is-better-than-cure">Prevention is Better than Cure</h2>

<p>Medicine has an adage that prevention is better than cure.</p>

<p>Applied to the context of building resilient UIs, preventing an error from happening in the first place is more desirable than needing to recover from one. <strong>The best type of error is one that doesn’t happen.</strong></p>

<p>It’s safe to assume never to make assumptions, especially when consuming remote data, interacting with third-party libraries, or using newer language features. Outages or unplanned API changes alongside what browsers users choose or must use are outside of our control. Whilst we cannot stop breakages outside our control from occurring, we can protect ourselves against their (side) effects.</p>

<p>Taking a more defensive approach when writing code helps reduce programmer errors arising from making assumptions. Pessimism over optimism favours resilience. The code example below is too optimistic:</p>

<pre><code>const debitCards = useDebitCards();

return (
  &lt;ul&gt;
    {debitCards.map(card =&gt; {
      &lt;li&gt;{card.lastFourDigits}&lt;/li&gt;
    })}
  &lt;/ul&gt;
);
</code></pre>

<p>It assumes that debit cards exist, the endpoint returns an Array, the array contains objects, and each object has a property named <code>lastFourDigits</code>. The current implementation forces end-users to test our assumptions. It would be safer, and more user friendly if these assumptions were embedded in the code:</p>

<pre><code>const debitCards = useDebitCards();

if (Array.isArray(debitCards) &amp;&amp; debitCards.length) {
  return (
    &lt;ul&gt;
      {debitCards.map(card =&gt; {
        if (card.lastFourDigits) {
          return &lt;li&gt;{card.lastFourDigits}&lt;/li&gt;
        }
      })}
    &lt;/ul&gt;
  );
}

return &#34;Something else&#34;;
</code></pre>

<p>Using a third-party method without first checking the method is available is equally optimistic:</p>

<pre><code>stripe.handleCardPayment(/* ... */);
</code></pre>

<p>The code snippet above assumes that the <code>stripe</code> object exists, it has a property named <code>handleCardPayment</code>, and that said property is a function. It would be safer, and therefore more defensive if these assumptions were verified by us beforehand:</p>

<pre><code>if (
  typeof stripe === &#39;object&#39; &amp;&amp; 
  typeof stripe.handleCardPayment === &#39;function&#39;
) {
  stripe.handleCardPayment(/* ... */);
}
</code></pre>

<p>Both examples check something is available before using it. Those familiar with feature detection may recognize this pattern:</p>

<pre><code>if (navigator.clipboard) {
  /* ... */
}
</code></pre>

<p>Simply asking the browser whether it supports the Clipboard API before attempting to cut, copy or paste is a simple yet effective example of resilience. The UI can adapt ahead of time by hiding clipboard functionality from unsupported browsers, or from users yet to grant permission.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/788ea5b0-a67a-43e8-a2f3-80e1bbb0de6e/5-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="340" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/788ea5b0-a67a-43e8-a2f3-80e1bbb0de6e/5-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/788ea5b0-a67a-43e8-a2f3-80e1bbb0de6e/5-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/788ea5b0-a67a-43e8-a2f3-80e1bbb0de6e/5-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/788ea5b0-a67a-43e8-a2f3-80e1bbb0de6e/5-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/788ea5b0-a67a-43e8-a2f3-80e1bbb0de6e/5-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/788ea5b0-a67a-43e8-a2f3-80e1bbb0de6e/5-resilience-is-a-feature.png" sizes="100vw" alt="Two black and white wireframes. The left one titled: Clipboard unavailable, displays 2 rows of numbers. The right one titled: Clipboard available, shows the same 2 numbers alongside a clipboard icon."/>
    
    </a>
  

  
    <figcaption>
      Only offer users functionality when we know they can use it. The copy to clipboard buttons (right) are conditionally shown based on whether the Clipboard API is available. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/788ea5b0-a67a-43e8-a2f3-80e1bbb0de6e/5-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>User browsing habits are another area living outside our control. Whilst we cannot dictate how our application is used, we can instill guardrails that prevent what we perceive as “misuse”. Some people double-click buttons — a behavior mostly redundant on the web, however not a punishable offense.</p>

<p>Double-clicking a button that submits a form should not submit the form twice, especially for <a href="https://developer.mozilla.org/en-US/docs/Glossary/Idempotent#technical_knowledge">non-idempotent HTTP methods</a>. During form submission, prevent subsequent submissions to mitigate any fallout from multiple requests being made.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/396cd330-df81-4582-a8e2-24c189329dd9/12-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="375" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/396cd330-df81-4582-a8e2-24c189329dd9/12-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/396cd330-df81-4582-a8e2-24c189329dd9/12-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/396cd330-df81-4582-a8e2-24c189329dd9/12-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/396cd330-df81-4582-a8e2-24c189329dd9/12-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/396cd330-df81-4582-a8e2-24c189329dd9/12-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/396cd330-df81-4582-a8e2-24c189329dd9/12-resilience-is-a-feature.png" sizes="100vw" alt="Two black and white wireframes. The left one titled: Double-click = 2 requests, displays a form and button (labelled submit) above a console showing 2 XHR requests to the orders endpoint. The left one titled: Double-click = 1 request, displays a form and button (labelled submitting) above a console showing 1 XHR request to the orders endpoint."/>
    
    </a>
  

  
    <figcaption>
      Users should not be punished for their browsing habits or mishaps. Preventing multiple form submissions because of intentional or accidental double-clicks is easier than cancelling duplicate transactions at a later date. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/396cd330-df81-4582-a8e2-24c189329dd9/12-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>Preventing form resubmission in JavaScript alongside using <code>aria-disabled=&#34;true&#34;</code> is more usable and accessible than the <code>disabled</code> HTML attribute. Sandrina Pereira explains <a href="https://css-tricks.com/making-disabled-buttons-more-inclusive/">Making Disabled Buttons More Inclusive</a> in great detail.</p>



<h2 id="responding-to-errors">Responding to Errors</h2>

<p>Not all errors are preventable via defensive programming. This means responding to an operational error (those occurring within correctly written programs) falls on us.</p>

<p>Responding to an error can be modelled using a decision tree. We can either recover, fallback or acknowledge the error:</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4804ef49-a1b1-463f-a579-f290871ca53d/6-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="385" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4804ef49-a1b1-463f-a579-f290871ca53d/6-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4804ef49-a1b1-463f-a579-f290871ca53d/6-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4804ef49-a1b1-463f-a579-f290871ca53d/6-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4804ef49-a1b1-463f-a579-f290871ca53d/6-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4804ef49-a1b1-463f-a579-f290871ca53d/6-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4804ef49-a1b1-463f-a579-f290871ca53d/6-resilience-is-a-feature.png" sizes="100vw" alt="Decision tree with 3 leaf nodes that read (from left to right): Recover from error? No: Fallback from error?, Yes: Resume as usual. The decision node: Fallback from error? has 2 paths: No: Acknowledge error, Yes: Show fallback."/>
    
    </a>
  

  
    <figcaption>
      Decision tree representing how we can respond to runtime errors. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4804ef49-a1b1-463f-a579-f290871ca53d/6-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>When facing an error, the first question should be, “can we recover?” For example, does retrying a network request that failed for the first time succeed on subsequent attempts? Intermittent micro-services, unstable internet connections, or eventual consistency are all reasons to try again. Data fetching libraries such as <a href="https://swr.vercel.app/">SWR</a> offer this functionality for free.</p>

<p>Risk appetite and surrounding context influence what HTTP methods you are comfortable retrying. At Nutmeg we retry failed reads (GET requests), but not writes (POST/ PUT/ PATCH/ DELETE). Multiple attempts to retrieve data (portfolio performance) is safer than mutating it (resubmitting a form).</p>

<p>The second question should be: If we cannot recover, can we provide a fallback? For example, if an online card payment fails can we offer an alternative means of payment such as via PayPal or Open Banking.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f115d83a-b305-421d-91b6-f8799c1c5a08/1-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="533" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f115d83a-b305-421d-91b6-f8799c1c5a08/1-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f115d83a-b305-421d-91b6-f8799c1c5a08/1-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f115d83a-b305-421d-91b6-f8799c1c5a08/1-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f115d83a-b305-421d-91b6-f8799c1c5a08/1-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f115d83a-b305-421d-91b6-f8799c1c5a08/1-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f115d83a-b305-421d-91b6-f8799c1c5a08/1-resilience-is-a-feature.png" sizes="100vw" alt="Wireframe of a red error notification above a form. The error message reads: Card payment failed. Please try again, or use a different payment method. The text: different payment method is underlined denoting it&#39;s a link."/>
    
    </a>
  

  
    <figcaption>
      When something goes wrong offering an alternative helps users help themselves, and avoids dead ends. This is especially important for time sensitive transactions such as buying stock, or contributing to an ISA before the tax year ends. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f115d83a-b305-421d-91b6-f8799c1c5a08/1-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>Fallbacks don’t always need to be so elaborate, they can be subtle. Copy containing text dependant on remote data can fallback to less specific text when the request fails:</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8a9290b3-a920-4fe4-ba95-32740fb3d100/4-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="399" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8a9290b3-a920-4fe4-ba95-32740fb3d100/4-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8a9290b3-a920-4fe4-ba95-32740fb3d100/4-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8a9290b3-a920-4fe4-ba95-32740fb3d100/4-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8a9290b3-a920-4fe4-ba95-32740fb3d100/4-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8a9290b3-a920-4fe4-ba95-32740fb3d100/4-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8a9290b3-a920-4fe4-ba95-32740fb3d100/4-resilience-is-a-feature.png" sizes="100vw" alt="Two black and white wireframes. The left one titled: Remote data unavailable, displays a paragraph that reads: Make the most of your remaining ISA allowance for the current tax year. The right wireframe titled: Remote data available, shows a paragraph that reads: Make the most of your £16500 ISA allowance for April 2021-2022"/>
    
    </a>
  

  
    <figcaption>
      UIs can adapt to what data is available and still provide value. The vaguer sentence (left) still reminds users that ISA allowances lapse each year. The more enriched sentence (right) is an enhancement for when the network request succeeds. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8a9290b3-a920-4fe4-ba95-32740fb3d100/4-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>The third and final question should be: If we cannot recover, or fallback how important is this failure (which relates to “Error Equality”). The UI should acknowledge primary errors by informing users something went wrong, whilst providing actionable prompts such as contacting customer support or linking to relevant support articles.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/66669728-b30a-42f3-ac19-16241d06959b/7-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="430" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/66669728-b30a-42f3-ac19-16241d06959b/7-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/66669728-b30a-42f3-ac19-16241d06959b/7-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/66669728-b30a-42f3-ac19-16241d06959b/7-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/66669728-b30a-42f3-ac19-16241d06959b/7-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/66669728-b30a-42f3-ac19-16241d06959b/7-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/66669728-b30a-42f3-ac19-16241d06959b/7-resilience-is-a-feature.png" sizes="100vw" alt="Two wireframes, each containing a red error notification. The left one titled: Unhelpful error message, displays the text: Something went wrong. The right one titled: Helpful error message shows a paragraph that reads: Sorry, unable to load your bank balance. Please try again, or. Below the paragraph is a list of the following items, phone us on 01234567890 8am to 8pm Mon to Fri, email us on support at email dot com and search ‘bank balance’ in our knowledge base"/>
    
    </a>
  

  
    <figcaption>
      Avoid unhelpful error messages. The helpful error message (right) prompts the user to contact CS, including how (phone/ email) and what hours they operate to manage expectations. It’s not uncommon to provide errors with a unique identifier that users can reference when making contact. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/66669728-b30a-42f3-ac19-16241d06959b/7-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<h2 id="observability">Observability</h2>

<p>UIs adapting to something going wrong is not the end. There is another side to the same coin.</p>

<p>Engineers need visibility on the root cause behind a degraded experience. Even errors not surfaced to end-users (secondary errors) must propagate to engineers. Real-time error monitoring services such as <a href="https://sentry.io/">Sentry</a> or <a href="https://rollbar.com/">Rollbar</a> are invaluable tools for modern-day web development.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/49acff6d-a19d-4d40-a317-6c55fffb59b5/11-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="458" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/49acff6d-a19d-4d40-a317-6c55fffb59b5/11-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/49acff6d-a19d-4d40-a317-6c55fffb59b5/11-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/49acff6d-a19d-4d40-a317-6c55fffb59b5/11-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/49acff6d-a19d-4d40-a317-6c55fffb59b5/11-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/49acff6d-a19d-4d40-a317-6c55fffb59b5/11-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/49acff6d-a19d-4d40-a317-6c55fffb59b5/11-resilience-is-a-feature.png" sizes="100vw" alt=" A screenshot taken from Sentry’s online sandbox of a TypeError. An error message reads: Cannot read property func of undefined. Below the error is a stack trace of where the exception was thrown"/>
    
    </a>
  

  
    <figcaption>
      A screenshot of an error captured in Sentry. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/49acff6d-a19d-4d40-a317-6c55fffb59b5/11-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>Most error monitoring providers capture all unhandled exceptions automatically. Setup requires minimal engineering effort that quickly pays dividends for an improved healthy production environment and MTTA (mean time to acknowledge).</p>



<p>The real power comes when explicitly logging errors ourselves. Whilst this involves more upfront effort it allows us to enrich logged errors with more meaning and context — both of which aid troubleshooting. Where possible aim for error messages that are understandable to non-technical members of the team.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/637a48c0-d69b-40de-8e96-1a715233ce01/13-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="170" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/637a48c0-d69b-40de-8e96-1a715233ce01/13-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/637a48c0-d69b-40de-8e96-1a715233ce01/13-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/637a48c0-d69b-40de-8e96-1a715233ce01/13-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/637a48c0-d69b-40de-8e96-1a715233ce01/13-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/637a48c0-d69b-40de-8e96-1a715233ce01/13-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/637a48c0-d69b-40de-8e96-1a715233ce01/13-resilience-is-a-feature.png" sizes="100vw" alt="Grey text on white background showing a function logging an error. The 1st function argument reads: Payment Bank transfer – Unable to connect with ${bank}. The 2nd argument is the error. Below the function are 3 labels: Domain, Context, and Problem."/>
    
    </a>
  

  
    <figcaption>
      Naming conventions help standardise explicit error messages, which make them easier to find/ read. The diagram above uses the format: [Domain] Context — Problem. You needn’t be an engineer to understand a bank transfer failed, and that the payments teams should investigate (if they aren’t already doing so). (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/637a48c0-d69b-40de-8e96-1a715233ce01/13-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>Extending the earlier Stripe example with an else branch is the perfect contender for explicit error logging:</p>

<pre><code>if (
  typeof stripe === &#34;object&#34; &amp;&amp;
  typeof stripe.handleCardPayment === &#34;function&#34;
) {
  stripe.handleCardPayment(/* ... */);
} else {
  logger.capture(
    &#34;[Payment] Card charge — Unable to fulfill card payment because stripe.handleCardPayment was unavailable&#34;
  );
}
</code></pre>

<p><strong>Note</strong>: <em>This defensive style needn’t be bound to form submission (at the time of error), it can happen when a component first mounts (before the error) giving us and the UI more time to adapt.</em></p>

<p>Observability helps pinpoint weaknesses in code and areas that can be hardened. Once a weakness surfaces look at if/ how it can be hardened to prevent the same thing from happening again. Look at trends and risk areas such as third-party integrations to identify what could be wrapped in an operational feature flag (otherwise known as kill switches).</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/6e961101-47fc-4939-be4e-fb59d2990c13/10-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="401" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/6e961101-47fc-4939-be4e-fb59d2990c13/10-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/6e961101-47fc-4939-be4e-fb59d2990c13/10-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/6e961101-47fc-4939-be4e-fb59d2990c13/10-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/6e961101-47fc-4939-be4e-fb59d2990c13/10-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/6e961101-47fc-4939-be4e-fb59d2990c13/10-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/6e961101-47fc-4939-be4e-fb59d2990c13/10-resilience-is-a-feature.png" sizes="100vw" alt="Two black and white wireframes. The left one titled: Kill switch off, displays 3 form fields above a blue button. The right one titled: Kill switch on, shows the text: Download PDF next to a download icon."/>
    
    </a>
  

  
    <figcaption>
      Not all fallbacks need to be digital. This is especially true for processes that already involve manual steps, such as transferring an ISA from one bank to another. When everything is operational (left) users submit an online form that populates a PDF they print and sign. When the third-party suffers an outage or is down for maintenance (right) a kill switch allows users to download a blank PDF form they can fill in (by hand), print and sign. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/6e961101-47fc-4939-be4e-fb59d2990c13/10-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<p>Users forewarned about something not working will be less frustrated than those without warning. Knowing about road works ahead of time helps manage expectations, allowing drivers to plan alternative routes. When dealing with an outage (hopefully discovered by monitoring and not reported by users) be transparent.</p>














<figure>
  
    <a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5d900508-6f8f-4fc7-878c-c4e3f2242936/9-resilience-is-a-feature.png">
    
    <img loading="lazy" decoding="async" importance="low" width="800" height="340" srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5d900508-6f8f-4fc7-878c-c4e3f2242936/9-resilience-is-a-feature.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5d900508-6f8f-4fc7-878c-c4e3f2242936/9-resilience-is-a-feature.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5d900508-6f8f-4fc7-878c-c4e3f2242936/9-resilience-is-a-feature.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5d900508-6f8f-4fc7-878c-c4e3f2242936/9-resilience-is-a-feature.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5d900508-6f8f-4fc7-878c-c4e3f2242936/9-resilience-is-a-feature.png 2000w" src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5d900508-6f8f-4fc7-878c-c4e3f2242936/9-resilience-is-a-feature.png" sizes="100vw" alt="Wireframe of a blue banner atop of a page. The banner reads: We’re currently experiencing problems with online payments and are working on resolving the issue"/>
    
    </a>
  

  
    <figcaption>
      Avoid offloading observability to end users. Finding and acknowledging issues before customers do leads to a better user experience. The information banner above is clear, concise, and reassures users that the issue is known about, and a fix is incoming. (<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5d900508-6f8f-4fc7-878c-c4e3f2242936/9-resilience-is-a-feature.png">Large preview</a>)
    </figcaption>
  
</figure>

<h2 id="retrospectives">Retrospectives</h2>

<p>It’s very tempting to gloss over errors.</p>

<p>However, they provide valuable learning opportunities for us and our current or future colleagues. Removing the stigma from the inevitability that things go wrong is crucial. In <a href="https://www.goodreads.com/book/show/24611735-black-box-thinking">Black box thinking</a> this is described as:</p>

<blockquote>“In highly complex organizations, success can happen only when we confront our mistakes, learn from our own version of a black box, and create a climate where it’s safe to fail.”</blockquote>

<p>Being analytical helps prevent or mitigate the same error from happening again. Much like black boxes in the aviation industry record incidents, we should document errors. At the very least documentation from prior incidents helps reduce the MTTR (mean time to repair) should the same error occur again.</p>

<p>Documentation often in the form of RCA (root cause analysis) reports should be honest, discoverable, and include: what the issue was, its impact, the technical details, how it was fixed, and actions that should follow the incident.</p>

<h2 id="closing-thoughts">Closing Thoughts</h2>

<p>Accepting the fragility of the web is a necessary step towards building resilient systems. A more reliable user experience is synonymous with happy customers. Being equipped for the worst (proactive) is better than putting out fires (reactive) from a business, customer, and developer standpoint (less bugs!).</p>

<p>Things to remember:</p>

<ul>
<li>UIs should adapt to the functionality they can offer, whilst still providing value to users;</li>
<li>Always think what can wrong (never make assumptions);</li>
<li>Categorize errors based on their impact (not all errors are equal);</li>
<li>Preventing errors is better than responding to them (code defensively);</li>
<li>When facing an error, ask whether a recovery or fallback is available;</li>
<li>User facing error messages should provide actionable prompts;</li>
<li>Engineers must have visibility on errors (use error monitoring services);</li>
<li>Error messages for engineers/ colleagues should be meaningful and provide context;</li>
<li>Learn from errors to help our future selves and others.</li>
</ul>

<p>
  <span>(vf, il)</span>
</p>



						
							
      			

          </div>

        </div>

      </div></div>]]></content:encoded>
      <guid>https://www.smashingmagazine.com/2021/08/build-resilient-javascript-ui/</guid>
      <pubDate>Wed, 04 Aug 2021 12:09:45 +0000</pubDate>
      <source>https://www.smashingmagazine.com/2021/08/build-resilient-javascript-ui/</source>
    </item>
    <item>
      <title>AdObserver Blocked by Facebook</title>
      <link>https://twitter.com/LauraEdelson2/status/1422736706554433538</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div dir="auto"><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p></div></form></div></div>]]></content:encoded>
      <guid>https://twitter.com/LauraEdelson2/status/1422736706554433538</guid>
      <pubDate>Wed, 04 Aug 2021 11:50:29 +0000</pubDate>
      <source>https://twitter.com/LauraEdelson2/status/1422736706554433538</source>
    </item>
    <item>
      <title>Lisp in an “impossible” language, the most complex malbolge program to date</title>
      <link>https://github.com/kspalaiologos/malbolge-lisp</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
          <article itemprop="text">
<p>Made by Palaiologos, 2020 - 2021. Released to the public domain.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/kspalaiologos/malbolge-lisp/raw/master/session.gif"></a></p>
<h2><a id="user-content-what-is-malbolgelisp" aria-hidden="true" href="#what-is-malbolgelisp"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is MalbolgeLisp?</h2>
<p><strong>MalbolgeLisp</strong> is a LISP interpreter written in Malbolge. It&#39;s as of 2020 and 2021, <a href="https://en.wikipedia.org/wiki/Malbolge#Programming_in_Malbolge" rel="nofollow">the most advanced, usable Malbolge program ever created</a>. It supports everything LISPs generally tend to support (like <code>cond</code>, <code>let</code>, <code>lambda</code>, etc...). The v1.1 release greatly improved the performance and reduced the code size, while adding a few features.</p>
<h2><a id="user-content-what-is-malbolge-why-is-it-difficult" aria-hidden="true" href="#what-is-malbolge-why-is-it-difficult"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is Malbolge? Why is it difficult?</h2>
<p><strong>Malbolge</strong> is a public domain esoteric programming language. It was specifically designed to be almost impossible to use, via a counter-intuitive &#39;crazy operation&#39;, trinary arithmetic, and self-modifying code. It builds on the difficulty of earlier, challenging esoteric languages like Brainfuck, but takes this aspect to the extreme. Despite this design, it is possible to write useful Malbolge programs (as this project proves).</p>
<p>What Malbolge instructions do depends on their position in the source code. After being ran, they are encrypted (so to make a loop, one has to decrypt it after each iteration - sounds hard already?). This is how so-called instruction cycles have been discovered - it has been observed that some instructions on certain locations form looping cycles, which is the basis of Malbolge programming.</p>
<p>The most complex programs made in Malbolge, to date, include an adder, a &#34;99 bottles of beer&#34; program, and a &#34;Hello, world!&#34; program (originally generated by a Lisp program utilizing a genetic algorithm).</p>
<p>MalbolgeLisp uses a special variant of Malbolge called <strong>Malbolge Unshackled</strong>. It&#39;s considerably harder to program for multiple reasons:</p>
<ol>
<li>The rotation width is chosen randomly by the interpreter</li>
<li>Malbolge Unshackled lets the width of rotation be variable, which grows with the values in the D register, and since the initial rotation width is unknown, you have to probe it (because otherwise <code>*</code> returns unpredictable results)</li>
<li>Malbolge Unshackled&#39;s print instruction requires unicode codepoints</li>
<li>if the rotation width is unknown then you can&#39;t load values larger than 3^4-1, except values starting with a <code>1</code> trit</li>
<li>to overcome this you need a loop that probes the rotation width which is probably beyond most people&#39;s comprehension</li>
<li>the specification says that the value <code>0t21</code> should be used to print a newline, but this value is <em>theoretically</em> impossible to obtain without having read an end of line or end of file from I/O before.</li>
<li>Malbolge Unshackled is actually usable because it&#39;s (as this project proves) Turing complete. The default Malbolge rotation width (10) constraints the addressable memory enough to make something cool with it.</li>
</ol>
<p>A few example Malbolge programs:</p>
<p>A &#34;Hello World&#34; program:</p>
<div data-snippet-clipboard-copy-content="(=&lt;`#9]~6ZY327Uv4-QsqpMn&amp;+Ij&#34;&#39;E%e{Ab~w=_:]Kw%o44Uqp0/Q?xNvL:`H%c#DD2^WV&gt;gY;dts76qKJImZkj
"><pre><code>(=&lt;`#9]~6ZY327Uv4-QsqpMn&amp;+Ij&#34;&#39;E%e{Ab~w=_:]Kw%o44Uqp0/Q?xNvL:`H%c#DD2^WV&gt;gY;dts76qKJImZkj
</code></pre></div>
<p>A cat program that doesn&#39;t terminate on EOF:</p>
<div data-snippet-clipboard-copy-content="(=BA#9&#34;=&lt;;:3y7x54-21q/p-,+*)&#34;!h%B0/.~P&lt;&lt;:(8&amp;66#&#34;!~}|{zyxwvugJk
"><pre><code>(=BA#9&#34;=&lt;;:3y7x54-21q/p-,+*)&#34;!h%B0/.~P&lt;&lt;:(8&amp;66#&#34;!~}|{zyxwvugJk
</code></pre></div>
<h2><a id="user-content-what-is-inside-the-zip-file" aria-hidden="true" href="#what-is-inside-the-zip-file"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is inside the zip file?</h2>
<p>The release bundle includes interpreter binaries for Windows (x64, one is optimized for memory consumption and one for speed), and a few interpreters for Linux (the names should be self explanatory, also x64). If you&#39;re not running a x64 machine, you&#39;ll have to compile <code>fast20.c</code> yourself. From my observations, the best results were yielded by GCC and the code has been tuned to perform well when compiled with it. <code>malbolgelisp-v1.1.mb</code> is the source code for the interpreter.</p>
<h2><a id="user-content-how-to-use-malbolgelisp" aria-hidden="true" href="#how-to-use-malbolgelisp"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How to use MalbolgeLisp?</h2>
<p>Pass the Lisp interpreter source code (<code>malbolgelisp-v1.1.mb</code>) as an argument to <code>fast20</code>.</p>
<h2><a id="user-content-using-the-repl" aria-hidden="true" href="#using-the-repl"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Using the REPL</h2>
<p>After you start the interpreter, you should be greeted with a banner (containing the version, dot commands, etc..). The following dot commands are currently available:</p>
<ul>
<li><code>.F</code> - display all the features recognized by the interpreter</li>
<li><code>.A</code> - display the information about the interpreter&#39;s author</li>
<li><code>.R</code> - reset the memory</li>
<li><code>.M</code> - display the information about memory consumption</li>
<li>Any other command (and sending EOF - ^Z on Windows, ^D on UNIXes
will terminate the interpreter).</li>
</ul>
<p>After entering an expression (for example - <code>(+ 2 2)</code>), the interpreter should print something akin to this:</p>
<div data-snippet-clipboard-copy-content="   % (+ 2 2)
   ..............|.....
   4
   %
"><pre><code>   % (+ 2 2)
   ..............|.....
   4
   %
</code></pre></div>
<p>The dots are used to signal that the code is being parsed (before the pipe character) or evaluated (after the pipe character). The amount of dots doesn&#39;t correspond to any consistent amount of time.</p>
<h2><a id="user-content-the-language" aria-hidden="true" href="#the-language"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The language</h2>
<p>The interpreter supports the following features that will be discussed in this README:</p>
<div data-snippet-clipboard-copy-content="   MALBOLGELISP V1.1 (2020-2021, PALAIOLOGOS)
   DOT COMMANDS:  .F(eatures)  .A(uthor)  .R(eset)  .M(emory)
   % .F
   &#39; + - &lt; &gt; = ! &amp; | * / define defun lambda
   cond print atom cons car cdr if let
   iota size nth
"><pre><code>   MALBOLGELISP V1.1 (2020-2021, PALAIOLOGOS)
   DOT COMMANDS:  .F(eatures)  .A(uthor)  .R(eset)  .M(emory)
   % .F
   &#39; + - &lt; &gt; = ! &amp; | * / define defun lambda
   cond print atom cons car cdr if let
   iota size nth
</code></pre></div>
<p><strong>MalbolgeLisp</strong> is fairly slow. If you want to test programs in a faster, mostly compatible environment to MalbolgeLisp, try <a href="https://github.com/kspalaiologos/x86lisp">x86Lisp</a> - a Lisp interpreter based on this dialect, squashed into a 2.1KB Windows <code>.exe</code> file.</p>
<h2><a id="user-content-arithmetic-on-numbers" aria-hidden="true" href="#arithmetic-on-numbers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Arithmetic on numbers</h2>
<p>A few examples of performing arithmetic in MalbolgeLisp are presented below. Note: Signed integers aren&#39;t supported.</p>
<div data-snippet-clipboard-copy-content="   % (+ 2 -1)
   ..............|.....
   1
   % (* 3 3)
   ..............|.....
   9
   % (% 5 2)
   ..............|.....
   1
   % (&gt; 5 6)
   ..............|.....
   0
   % (&lt; 5 6)
   ..............|.....
   1
   % (= 6 6)
   ..............|......
   1
   % (! (= 6 7))
   ......................|.........
   1
   % (/ 25 5)
   ..............|.....
   5
   % (- 4 3)
   ..............|.....
   1
   % (&amp; (= 2 2) (= 3 3))
   ....................................|...............
   1
"><pre><code>   % (+ 2 -1)
   ..............|.....
   1
   % (* 3 3)
   ..............|.....
   9
   % (% 5 2)
   ..............|.....
   1
   % (&gt; 5 6)
   ..............|.....
   0
   % (&lt; 5 6)
   ..............|.....
   1
   % (= 6 6)
   ..............|......
   1
   % (! (= 6 7))
   ......................|.........
   1
   % (/ 25 5)
   ..............|.....
   5
   % (- 4 3)
   ..............|.....
   1
   % (&amp; (= 2 2) (= 3 3))
   ....................................|...............
   1
</code></pre></div>
<p>There is no <code>&gt;=</code> or <code>&lt;=</code>, it has to be implemented by negating respectively <code>&lt;</code> and <code>&gt;</code>. A single <code>&amp;</code> and <code>|</code> are respectively logical AND and OR.</p>
<h2><a id="user-content-define-defun-and-lambda" aria-hidden="true" href="#define-defun-and-lambda"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>define</code>, <code>defun</code> and <code>lambda</code>.</h2>
<p>In MalbolgeLISP, you can introduce constants using the <code>define</code> function. For example:</p>
<div data-snippet-clipboard-copy-content="   % (define x 5)
   ................|..
   5
   % (+ x 1)
   ................|.....
   6
"><pre><code>   % (define x 5)
   ................|..
   5
   % (+ x 1)
   ................|.....
   6
</code></pre></div>
<p>MalbolgeLISP also supports lexically scoped (a function sees its lexical ancestor&#39;s environment, not its callers) lambdas. For instance:</p>
<div data-snippet-clipboard-copy-content="   % ((lambda (n) (* n n)) 3)
   ........................................|.........
   9
   % ((lambda (m) ((lambda (n) (+ m n)) 2)) 2)
   ........................................................|.............
   4
"><pre><code>   % ((lambda (n) (* n n)) 3)
   ........................................|.........
   9
   % ((lambda (m) ((lambda (n) (+ m n)) 2)) 2)
   ........................................................|.............
   4
</code></pre></div>
<p>How to define functions then? Well, a logical conclusion can be drawn from the examples above:</p>
<div data-snippet-clipboard-copy-content="   % (define succ (lambda (x) (+ x 1)))
   .............................................|..
   (lambda (x) (+ x 1))
   % (succ 2)
   ...........|.........
   3
"><pre><code>   % (define succ (lambda (x) (+ x 1)))
   .............................................|..
   (lambda (x) (+ x 1))
   % (succ 2)
   ...........|.........
   3
</code></pre></div>
<p>Because <code>define lambda</code> is a bit annoying to type each time and doesn&#39;t look good, we can use a nicer syntax for that - <code>defun</code>:</p>
<div data-snippet-clipboard-copy-content="   % (defun succ (x) (+ x 1))
   .....................................|.
   (lambda (x) (+ x 1))
   % (succ 2)
   ...........|.........
   3
"><pre><code>   % (defun succ (x) (+ x 1))
   .....................................|.
   (lambda (x) (+ x 1))
   % (succ 2)
   ...........|.........
   3
</code></pre></div>
<h2><a id="user-content-operations-on-lists-and-atoms" aria-hidden="true" href="#operations-on-lists-and-atoms"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Operations on lists and atoms</h2>
<p>The equals function (<code>=</code>) also works on atoms and lists. Lists have to be quoted to prevent the interpreter from evaluating them:</p>
<div data-snippet-clipboard-copy-content="   % (= test test)
   ..................|......
   1
   % (= &#39;(1 2 3) &#39;(1 2 3))
   ..................................|.........
   1
   %
"><pre><code>   % (= test test)
   ..................|......
   1
   % (= &#39;(1 2 3) &#39;(1 2 3))
   ..................................|.........
   1
   %
</code></pre></div>
<p>The <code>iota</code> function generates natural numbers from <code>0</code> to <code>N-1</code> (inclusive). For instance:</p>
<div data-snippet-clipboard-copy-content="   % (iota 5)
   ...........|....
   (0 1 2 3 4)
"><pre><code>   % (iota 5)
   ...........|....
   (0 1 2 3 4)
</code></pre></div>
<p><code>size</code> yields the size of a list passed as an argument. Using <code>size</code> and <code>iota</code>, we can make a really inefficient identity function for numbers:</p>
<div data-snippet-clipboard-copy-content="   % (defun id (x) (size (iota x)))
   ..........................................|.
   (lambda (x) (size (iota x)))
   % (id 6)
   ...........|...........
   6
   % .M
   233B USED
   % # That&#39;s a lot of memory wasted!
"><pre><code>   % (defun id (x) (size (iota x)))
   ..........................................|.
   (lambda (x) (size (iota x)))
   % (id 6)
   ...........|...........
   6
   % .M
   233B USED
   % # That&#39;s a lot of memory wasted!
</code></pre></div>
<p><code>nth</code> yields the N-th element of a list. It uses the same indexing as <code>iota</code>, as demonstrated below:</p>
<div data-snippet-clipboard-copy-content="   % (nth 3 (iota 5))
   ......................|........
   3
"><pre><code>   % (nth 3 (iota 5))
   ......................|........
   3
</code></pre></div>
<p>Let&#39;s try using atoms now. We can print an atom or a list using <code>print</code>, and we can check if something is an atom using the <code>atom</code> function. Note that <code>print</code> is an identity function with a side effect of printing an expression&#39;s value:</p>
<div data-snippet-clipboard-copy-content="   % (atom null)
   ...........|....
   1
   % (atom &#39;(1 2 3))
   .....................|....
   0
   % (print hello)
   .............|....
   hello

   hello
   % (print &#39;(1 2 3))
   .....................|....
   (1 2 3)

   (1 2 3)
   %
"><pre><code>   % (atom null)
   ...........|....
   1
   % (atom &#39;(1 2 3))
   .....................|....
   0
   % (print hello)
   .............|....
   hello

   hello
   % (print &#39;(1 2 3))
   .....................|....
   (1 2 3)

   (1 2 3)
   %
</code></pre></div>
<p>Let&#39;s look at how <code>car</code> and <code>cdr</code> work. <code>car</code> takes the head (first element) of a list, and <code>cdr</code> takes the tail (everything <em>except</em> the first element) of a list:</p>
<div data-snippet-clipboard-copy-content="   % (define list &#39;(1 2 3))
   ..........................|..
   (1 2 3)
   % (car list)
   .............|....
   1
   % (cdr list)
   .............|....
   (2 3)
"><pre><code>   % (define list &#39;(1 2 3))
   ..........................|..
   (1 2 3)
   % (car list)
   .............|....
   1
   % (cdr list)
   .............|....
   (2 3)
</code></pre></div>
<p>Finally, <code>cons</code> can be used to prepend something to a list. Like so:</p>
<div data-snippet-clipboard-copy-content="   % (cons 3 &#39;(1 2))
   .....................|.....
   (3 1 2)
"><pre><code>   % (cons 3 &#39;(1 2))
   .....................|.....
   (3 1 2)
</code></pre></div>
<h2><a id="user-content-if-cond-and-let" aria-hidden="true" href="#if-cond-and-let"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>if</code>, <code>cond</code> and <code>let</code></h2>
<p>The <code>if</code> function requires exactly 3 parameters - the condition, the expression to evaluate if the condition is true, and the expression to evaluate if the condition is false. A small example:</p>
<div data-snippet-clipboard-copy-content="   % (defun gt10 (x) (if (&gt; x 10) (print yes) (print no)))
   .......................................................................|.
   (lambda (x) (if (&gt; x 10) (print yes) (print no)))
   % (gt10 5)
   ...........|...............
   no

   no
   % (gt10 16)
   ...........|...............
   yes

   yes
"><pre><code>   % (defun gt10 (x) (if (&gt; x 10) (print yes) (print no)))
   .......................................................................|.
   (lambda (x) (if (&gt; x 10) (print yes) (print no)))
   % (gt10 5)
   ...........|...............
   no

   no
   % (gt10 16)
   ...........|...............
   yes

   yes
</code></pre></div>
<p>If you need a lot of <code>if</code>s in one place, you can consider using <code>cond</code>. It takes a list of lists <code>(condition result)</code>, optionally including a <code>(result)</code> list. Using <code>cond</code>, we can make a three-way-comparison function (that returns <code>2 1 0</code> for <code>&gt; = &lt;</code>):</p>
<div data-snippet-clipboard-copy-content="   % (defun &lt;=&gt; (x y) (cond ((&gt; x y) 2) ((&lt; x y) 0) (1)))
   ....................................................................................|.
   (lambda (x y) (cond ((&gt; x y) 2) ((&lt; x y) 0) (1)))
   % (&lt;=&gt; 5 6)
   ..............|.....................
   0
"><pre><code>   % (defun &lt;=&gt; (x y) (cond ((&gt; x y) 2) ((&lt; x y) 0) (1)))
   ....................................................................................|.
   (lambda (x y) (cond ((&gt; x y) 2) ((&lt; x y) 0) (1)))
   % (&lt;=&gt; 5 6)
   ..............|.....................
   0
</code></pre></div>
<p>The final builtin function is <code>let</code>. It allows us to bind names to expressions (so that they&#39;re not re-evaluated at a later time). For example:</p>
<div data-snippet-clipboard-copy-content="   % (let (x 5 y 6) (+ x y))
   .............................................|.............
   11
"><pre><code>   % (let (x 5 y 6) (+ x y))
   .............................................|.............
   11
</code></pre></div>
<h2><a id="user-content-example-programs" aria-hidden="true" href="#example-programs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example programs</h2>
<p>Quicksort (@La Condizione; yes, they know that this is not a real quicksort)</p>
<div data-snippet-clipboard-copy-content="(defun filter (f l) (cond ((= l null) null) ((f (car l)) (cons (car l) (filter f (cdr l)))) (1 (filter f (cdr l)))))
(defun append (a b) (if (= null a) b (cons (car a) (append (cdr a) b) )))
(defun append3 (a b c) (append a (append b c)))
(defun list&gt;= (m list) (filter (lambda (n) (! (&lt; n m))) list))
(defun list&lt; (m list) (filter (lambda (n) (&lt; n m)) list))
(defun qsort (l) (if (= null l) null (append3 (qsort (list&lt; (car l) (cdr l))) (cons (car l) null) (qsort (list&gt;= (car l) (cdr l))))))
(qsort &#39;(6 8 1 0 6 8 2))
"><pre><code>(defun filter (f l) (cond ((= l null) null) ((f (car l)) (cons (car l) (filter f (cdr l)))) (1 (filter f (cdr l)))))
(defun append (a b) (if (= null a) b (cons (car a) (append (cdr a) b) )))
(defun append3 (a b c) (append a (append b c)))
(defun list&gt;= (m list) (filter (lambda (n) (! (&lt; n m))) list))
(defun list&lt; (m list) (filter (lambda (n) (&lt; n m)) list))
(defun qsort (l) (if (= null l) null (append3 (qsort (list&lt; (car l) (cdr l))) (cons (car l) null) (qsort (list&gt;= (car l) (cdr l))))))
(qsort &#39;(6 8 1 0 6 8 2))
</code></pre></div>
<p>&#34;Dumb&#34; sort (@umnikos)</p>
<div data-snippet-clipboard-copy-content="(defun insert (i l) (if (= null l) (cons i l) (if (&gt; i (car l)) (cons (car l) (insert i (cdr l))) (cons i l))))
(defun insertsort (l) (if (= null l) null (insert (car l) (insertsort (cdr l)))))
(insertsort &#39;(6 8 1 0 6 8 2))
"><pre><code>(defun insert (i l) (if (= null l) (cons i l) (if (&gt; i (car l)) (cons (car l) (insert i (cdr l))) (cons i l))))
(defun insertsort (l) (if (= null l) null (insert (car l) (insertsort (cdr l)))))
(insertsort &#39;(6 8 1 0 6 8 2))
</code></pre></div>
<p>And a bunch of programs I wrote myself - a counter, power function, factorial function and the maximum of a list.</p>
<div data-snippet-clipboard-copy-content="(defun counter (x) (cond ((&lt; x 10) (counter (print (+ 1 x)))) (print ok)))
(counter 1)

(defun power (x y) (if (= y 0) 1 (* x (power x (- y 1)))))
(power 3 5)

(defun fac (n) (if (= n 0) 1 (* n (fac (- n 1)))))
(fac 6)

(defun max (l) (if (= null l) 0 (let (m (max (cdr l))) (if (&gt; (car l) m) (car l) m))))
(max &#39;(2 5 1 9 10))
"><pre><code>(defun counter (x) (cond ((&lt; x 10) (counter (print (+ 1 x)))) (print ok)))
(counter 1)

(defun power (x y) (if (= y 0) 1 (* x (power x (- y 1)))))
(power 3 5)

(defun fac (n) (if (= n 0) 1 (* n (fac (- n 1)))))
(fac 6)

(defun max (l) (if (= null l) 0 (let (m (max (cdr l))) (if (&gt; (car l) m) (car l) m))))
(max &#39;(2 5 1 9 10))
</code></pre></div>
<p>Do you want <em>your</em> code featured? Please open a pull request.</p>
</article>
        </div></div>]]></content:encoded>
      <guid>https://github.com/kspalaiologos/malbolge-lisp</guid>
      <pubDate>Tue, 03 Aug 2021 11:44:18 +0000</pubDate>
      <source>https://github.com/kspalaiologos/malbolge-lisp</source>
    </item>
    <item>
      <title>Show HN: Till – Unblock and scale your web scrapers, with minimal code changes</title>
      <link>https://github.com/DataHenHQ/till</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
          <article itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/DataHenHQ/till/blob/main/img/till-logo.svg"></a> <strong>DataHen Till</strong> is a standalone tool that runs alongside your web scraper, and instantly makes your existing web scraper scalable, maintainable and unblockable. It integrates with your existing web scraper without requiring any code changes on your scraper code.</p>
<p>Till was architected to follow best practices that <a href="https://www.datahen.com" rel="nofollow">DataHen</a> has accumulated over the years of scraping at a massive scale.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/DataHenHQ/till/blob/main/img/how-it-works.png"></a></p>
<h2><a id="user-content-problems-with-web-scraping" aria-hidden="true" href="#problems-with-web-scraping"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Problems with Web Scraping</h2>
<p>Web scraping is usually easy to get started, especially on a small scale. However, as you try to scale it up, it gets exponentially difficult. Scraping 10,000 records can easily be done with simple web scraper scripts in any programming language, but as you try to scrape millions of pages, you would need to architect and build features on your web scraping script that allows you to scale, maintain and unblock your scrapers.</p>
<p><strong>DataHen Till</strong> solves the following problems:</p>
<h3><a id="user-content-scaling-your-scraper" aria-hidden="true" href="#scaling-your-scraper"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scaling your scraper</h3>
<p>Scraping to millions or even billions of records requires much more pre-planning. It&#39;s not simply running your existing web scraper script in a bigger CPU/Ram machine.
More thoughts are needed, such as:</p>
<ul>
<li>How to log massive amounts of HTTP requests.</li>
<li>How to troubleshoot HTTP requests, when it fails at scale.</li>
<li>How to minimize bandwidth usage.</li>
<li>How to rotate proxy IPs.</li>
<li>How to handle anti-scrapers.</li>
<li>What happens when a scraper fails.</li>
<li>How to resume scrapers after they are fixed.</li>
<li>etc.</li>
</ul>
<p>Till provides a plug-and-play method of making your web scrapers scalable, and maintainable following best practices at <a href="https://www.datahen.com" rel="nofollow">DataHen</a> that makes web scraping a pleasant experience.</p>
<h3><a id="user-content-blocked-scraper" aria-hidden="true" href="#blocked-scraper"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blocked scraper</h3>
<p>As you try to scale up the number of requests, quite often, the target websites will detect your scraper and try to block your requests using Captcha, or throttling, or denying your request completely.</p>
<p>Till helps you circumvent detected as a web scraper by identifying your scraper as a real web browser. It does this by generating random <code>user-agent</code> headers and randomizing proxy IPs (that you supply) on every HTTP request.</p>
<p>Till also makes it easy for you to troubleshoot on why the target website block your scraper.</p>
<h3><a id="user-content-scraper-maintenance" aria-hidden="true" href="#scraper-maintenance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scraper Maintenance</h3>
<p>Maintaining high-scale scrapers is challenging due to the massive volume of requests and interactions between your scrapers and the target websites. In order for a smooth operation, you need to think through how to maintain your scrapers regularly.</p>
<p>You need to know how to raise and triage errors as they occur on your scrapers, not all errors on web scraping should be treated equally. some are ignorable, and some are urgent. So, you will need to know what will be the details of your &#34;development-deployment-maintenance&#34; process will be.</p>
<p>Till solves this by logging all your HTTP requests and categorizing them whether it was successful (2XX statuses) or failures(non 2XX statuses). Till also provides a Web UI to analyze the request history and make sense of what happened during your scraping process.</p>
<p>Till makes it even easier for scraper maintenance by assigning each request with a unique Global ID (GID) that is derived from the request&#39;s URL, method, body, etc. You can then use this GID to troubleshoot your scrapers on where it went wrong.</p>
<h3><a id="user-content-postmortem-analysis--reproducability" aria-hidden="true" href="#postmortem-analysis--reproducability"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Postmortem analysis &amp; reproducability</h3>
<p>The biggest difficulty facing any web scraper developer is when there are scraping failures. Your scraper fails when fetching or parsing certain URLs, but when you look at the target website and URLs, everything looks fine. How do you troubleshoot what already happened in the scenario?. How do you reproduce that failed scrape so that you can fix the issue?</p>
<p>Till stores all HTTP requests and the responses (including the response body/content) into a local cache. If at anytime your scraper encounters an error, you can then use the request&#39;s GID (Till assigns a Global ID, also called GID, on every request) to find the request and the actual response and content from the cache. In this way, you can analyze what went wrong with that particular request.</p>
<h3><a id="user-content-starting-over-from-scratch-when-it-fails-mid-way" aria-hidden="true" href="#starting-over-from-scratch-when-it-fails-mid-way"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Starting over from scratch when it fails mid-way</h3>
<p>Websites change all the time and without notice. Imagine running your web scraper for a week and then suddenly, somewhere along the way, it fails. It is frustrating that once you&#39;ve fixed the scraper, there is a high chance that you&#39;d need to start over from scratch again. And, on top of this, there are additional consequences, such as time delay, and further charges related to proxy usage, bandwidth, storage, VM costs, etc.</p>
<p>Till solves this by allowing you to replay your scrapers without actually needing to resend the HTTP requests to the target server.
Till does this by assigning each HTTP request its own unique Global ID (GID) that is generated from the request&#39;s URL, method, headers, etc. It then stores all HTTP responses in the Cache based on their GID.</p>
<p>When you restart your scraper, the scraping process can go blazingly fast because Till now serves the cached version of the HTTP responses. All of this without any code changes on your existing web scraper.</p>
<h2><a id="user-content-features" aria-hidden="true" href="#features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Features</h2>
<h3><a id="user-content-user-agent-randomizer" aria-hidden="true" href="#user-agent-randomizer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://till.datahen.com/docs/user-agent-randomizer" rel="nofollow">User-Agent randomizer</a></h3>
<p>Till automatically generates random user-agent on every request. Choose to identify your scraper as a desktop browser, or a mobile browser, or you can even override it with your custom user-agent.</p>
<h3><a id="user-content-proxy-ip-address-rotation" aria-hidden="true" href="#proxy-ip-address-rotation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://till.datahen.com/docs/proxy-ip-address-rotation" rel="nofollow">Proxy IP address rotation</a></h3>
<p>Supply a list of proxy IPs, and Till will randomly use them on every request. Saves you time in needing to set up a separate proxy rotation service.</p>
<h3><a id="user-content-sticky-sessions" aria-hidden="true" href="#sticky-sessions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://till.datahen.com/docs/sticky-sessions" rel="nofollow">Sticky Sessions</a></h3>
<p>Your scraper can selectively reuse the same user-agent, proxy IP, and cookie jar for multiple requests. This allows you to easily group your requests based on certain workflow, and allow you to avoid detection from anti-scraping systems.</p>
<h3><a id="user-content-managing-cookies" aria-hidden="true" href="#managing-cookies"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://till.datahen.com/docs/sticky-sessions#manage-cookies" rel="nofollow">Managing Cookies</a></h3>
<p>No need to build your cookie management logic in your scraper codes. Till can store the cookies for you so that you can easily reuse them on subsequent requests.</p>
<h3><a id="user-content-request-logging" aria-hidden="true" href="#request-logging"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://till.datahen.com/docs/request-log" rel="nofollow">Request Logging</a></h3>
<p>Till will log your requests based on successful request (2XX status code) or failed request (non 2XX status code). This will allow you to easily troubleshoot your scraper later.</p>
<p>The Till UI allows you to make sense of HTTP request history, and troubleshoot what happens during a scraping session.</p>
<h3><a id="user-content-http-caching" aria-hidden="true" href="#http-caching"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://till.datahen.com/docs/http-caching" rel="nofollow">HTTP Caching</a></h3>
<p>Till caches all of your HTTP responses (and their contents), so that as needed, your web scraper will reuse the cache without needing to do another HTTP request to the target server.</p>
<p>You can selectively choose whether to use a particular cached content or not by specifying how fresh you want Till to serve the cache. For example: If Till holds an existing cached content that is 1 week old, but your web scraper only wants 1-day old content, Till will then only serve cached contents that are 1 day old.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/DataHenHQ/till/blob/main/img/http-caching-flowchart.png"></a></p>
<h3><a id="user-content-global-id-gid" aria-hidden="true" href="#global-id-gid"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://till.datahen.com/docs/http-caching#gid" rel="nofollow">Global ID (GID)</a></h3>
<p>Till uses <a href="https://www.datahen.com/platform" rel="nofollow">DataHen Platform</a>&#39;s convention of marking every unique request with a signature (we call this the Global ID or GID for short). Think of it like a Checksum of the actual request.</p>
<p>Anytime your scraper sends a request through Till, it will return a response with the header <code>X-DH-GID</code> that contains the GID. This GID allows you to easily troubleshoot requests when you need to look up specific requests in the log, or contents in the cache.</p>
<h2><a id="user-content-how-datahen-till-works" aria-hidden="true" href="#how-datahen-till-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How DataHen Till works</h2>
<p>Till works as a Man In The Middle (MITM) proxy that listens to incoming HTTP(S) requests and forwards those requests to the target server as needed. While it does so, it enhances each request to avoid being detected by anti-scrapers. It also logs and caches the responses to make your scraper maintainable and scalable.</p>
<p>Connect your scraper to Till via the <code>proxy</code> protocol that is typically common in any programming language.</p>
<p>Your scraper will then continue to run as-is and it will get instantly become more unblockable, scalable, and maintainable.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/DataHenHQ/till/blob/main/img/how-it-works.png"></a></p>
<h2><a id="user-content-installation" aria-hidden="true" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<h3><a id="user-content-step-1-download-till" aria-hidden="true" href="#step-1-download-till"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Step 1: Download Till</h3>
<p>The recommended way to install DataHen Till is by downloading one of the <a href="https://github.com/DataHenHQ/till/releases">standalone binaries</a> according to your OS.</p>
<h3><a id="user-content-step-2-get-your-auth-token" aria-hidden="true" href="#step-2-get-your-auth-token"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Step 2: Get your auth Token</h3>
<p>You need to get your auth token to run Till.</p>
<p>Get your token for FREE by signing up for an account at <a href="https://till.datahen.com/login?type=signup" rel="nofollow">till.datahen.com</a>.</p>
<h3><a id="user-content-step-3-start-till" aria-hidden="true" href="#step-3-start-till"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Step 3: Start Till</h3>
<p>start the Till server with the following command:</p>
<div data-snippet-clipboard-copy-content="$ till serve -t &lt;your token here&gt; 
"><pre>$ till serve -t <span>&lt;</span>your token here<span>&gt;</span> </pre></div>
<p>The above will start a proxy port on <a href="http://localhost:2933" rel="nofollow">http://localhost:2933</a>
and the Till UI on <a href="http://localhost:2980" rel="nofollow">http://localhost:2980</a>.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/DataHenHQ/till/blob/main/img/request-log-ui.png"></a></p>
<h3><a id="user-content-step-4-connect-to-till" aria-hidden="true" href="#step-4-connect-to-till"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Step 4 Connect to Till</h3>
<p>You can connect your scraper to Till without many code changes.</p>
<p>If you want to connect to Till using curl, this is how:</p>
<div data-snippet-clipboard-copy-content="$ curl -k --proxy http://localhost:2933 https://fetchtest.datahen.com/echo/request
"><pre>$ curl -k --proxy http://localhost:2933 https://fetchtest.datahen.com/echo/request</pre></div>
<h2><a id="user-content-certificate-authority-ca-certificates" aria-hidden="true" href="#certificate-authority-ca-certificates"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Certificate Authority (CA) Certificates</h2>
<p>Till decrypts and encrypts HTTPS traffic on the fly between your scraper and the target websites.  In order to do so, your scraper (or browser) must be able to trust the built-in Certificate Authority (CA). This means the CA certificate that Till generates for you, needs to be installed on the computer where the scraper is running.</p>
<p><strong>Note:</strong> If you do not wish to install the CA certificate, you can still have your scraper connect to the Till server by disabling/ignoring security checks in your scraper. Please refer to the programming language/framework/tool that your scraper uses.</p>
<h3><a id="user-content-installing-the-generated-ca-certificates-onto-your-computer" aria-hidden="true" href="#installing-the-generated-ca-certificates-onto-your-computer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installing the generated CA certificates onto your computer</h3>
<p>The first time Till runs as a server, Till generates the CA certificates in the following directory:</p>
<p>Linux or MacOS:</p>

<p>Windows:</p>
<div data-snippet-clipboard-copy-content="C:\Users\&lt;your user&gt;\.config\datahen\till\
"><pre><code>C:\Users\&lt;your user&gt;\.config\datahen\till\
</code></pre></div>
<p>Then, please follow the following instructions to install the CA certificates:</p>
<h4><a id="user-content-macos" aria-hidden="true" href="#macos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MacOS</h4>
<p><a href="https://support.apple.com/en-ca/guide/keychain-access/kyca2431/mac" rel="nofollow">Add certificates to a keychain using Keychain Access on Mac</a></p>
<h4><a id="user-content-ubuntudebian" aria-hidden="true" href="#ubuntudebian"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Ubuntu/Debian</h4>
<p><a href="https://askubuntu.com/questions/73287/how-do-i-install-a-root-certificate/94861#94861" rel="nofollow">How do I install a root certificate</a></p>
<h4><a id="user-content-mozilla-firefox" aria-hidden="true" href="#mozilla-firefox"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Mozilla Firefox</h4>
<p><a href="https://wiki.mozilla.org/MozillaRootCertificate#Mozilla_Firefox" rel="nofollow">how to import the Mozilla Root Certificate into your Firefox web browser</a></p>
<h4><a id="user-content-chrome" aria-hidden="true" href="#chrome"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Chrome</h4>
<p><a href="https://stackoverflow.com/questions/7580508/getting-chrome-to-accept-self-signed-localhost-certificate/15076602#15076602" rel="nofollow">Getting Chrome to accept self-signed localhost certificate</a></p>
<h4><a id="user-content-windows" aria-hidden="true" href="#windows"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Windows</h4>
<p>Use <code>certutil</code> with the following command:</p>
<div data-snippet-clipboard-copy-content="certutil -addstore root &lt;path to your CA cert file&gt;
"><pre><code>certutil -addstore root &lt;path to your CA cert file&gt;
</code></pre></div>
<p>Read more about <a href="https://web.archive.org/web/20160612045445/http://windows.microsoft.com/en-ca/windows/import-export-certificates-private-keys#1TC=windows-7" rel="nofollow">certutil</a></p>
</article>
        </div></div>]]></content:encoded>
      <guid>https://github.com/DataHenHQ/till</guid>
      <pubDate>Wed, 04 Aug 2021 10:16:52 +0000</pubDate>
      <source>https://github.com/DataHenHQ/till</source>
    </item>
    <item>
      <title>Simple Systems Have Less Downtime (2020)</title>
      <link>https://www.gkogan.co/blog/simple-systems/</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>The Maersk Triple-E Class container ship is 1,300 feet long, carries over 18,000 containers across 11,000 miles between Europe and Asia, and… Its entire crew can fit inside a passenger van.</p>

<p></p>

<p>As a former naval architect and a current <a href="https://www.gkogan.co/">marketing consultant to startups</a>, I found that the same principle that lets a 13-person crew navigate the world’s largest container ship to a port halfway around the world without breaking down also applies to startups working towards aggressive growth goals:</p>

<p><strong>Simple systems have less downtime.</strong></p>

<p>Ships contain simple systems that are easy to operate and easy to understand, which makes them easy to fix, which means they have less downtime. An important quality, considering that “downtime” for a ship could mean being stranded thousands of miles from help.</p>

<p>Take the ship’s steering system, for instance. The rudder is pushed left or right by metal rods. Those rods are moved by hydraulic pressure. That pressure is controlled by a hydraulic pump. That pump is controlled by an electronic signal from the wheelhouse. That signal is controlled by the autopilot. It doesn’t require a rocket scientist or a naval architect to find the cause of and solution to any problem:</p>

<ul>
  <li>If the autopilot fails, steer the ship manually from the wheelhouse.</li>
  <li>If the electronic signals fail, go to the rudder control room to control the pump by hand, while talking with the bridge through a <a href="https://en.wikipedia.org/wiki/Sound-powered_telephone">simple sound-powered phone</a>.</li>
  <li>If the hydraulics fail, use the mechanically linked emergency steering wheel.</li>
  <li>If the mechanical linkage fails, hook a chain to both sides of the rudder and pull in the direction you want!</li>
</ul>

<p></p>

<p>Startups, like ships, can’t afford to stall from system downtimes. Extended downtime in sales, marketing, web, customer support, hiring, product, and other systems may cause irreparable damage to the growth rate.</p>

<p>(Although automation is prevalent on modern ships, it only affects the time it takes to do things and the attention required to monitor everything. The propulsion and auxiliary systems are more simple than ever, thanks to modern diesel and electric propulsion systems that replaced pipe-laden steam plants.)</p>

<h2 id="why-simplicity-leads-to-less-downtime">Why Simplicity Leads to Less Downtime</h2>

<h3 id="1-proficiency-takes-less-time">1. Proficiency takes less time.</h3>

<p>If the person responsible for the system leaves, falls overboard, <a href="https://en.wikipedia.org/wiki/Bus_factor">gets hit by a bus</a>, or gets pulled into another project, another person can take over without much learning or training. That means more people are able to step in to troubleshoot and fix issues.</p>

<p>For example, an analytics dashboard built with Tableau is likely to have more qualified people to fix it than one built with a patchwork of custom scripts and APIs. Nobody should have to pull data scientists or product developers away from their work to fix a bar chart.</p>

<hr/>

<p><em>By the way, I write an article like this every month or so, covering lessons learned from growing B2B software startups. Get an email update when the next one is published:</em>
<!-- Begin MailChimp Signup Form --></p>



<!--End mc_embed_signup-->
<hr/>

<h3 id="2-troubleshooting-takes-less-time">2. Troubleshooting takes less time.</h3>

<p>In a system where the behavior of each component and its relationship to other comments is easily understood, ruling out issues and finding the broken component—the root cause—is more intuitive.</p>

<p>For example, if a company has many downloadable whitepapers on their website, and they’re all gated behind a single form—as opposed to a custom form for each one—then they need only troubleshoot one form and one automation workflow if the whitepaper downloads stop working.</p>

<p></p>

<h3 id="3-more-alternative-solutions">3. More alternative solutions.</h3>

<p>When each part of the system has a clear function, alternatives are easier to find.</p>

<p>For example, imagine a Salesforce process that uses a mishmash of automation and third-party tools to score, filter, classify, and assign new sales leads. If that fails then there is no obvious replacement. Everything will be put on hold until the process is fixed or replaced with a similarly complex solution.</p>

<p>Now imagine a sales process in which the sales team is simply notified of each new sales lead along with pertinent details, letting them decide whether or not to follow up with the lead. If the Salesforce notification step fails, it’s easy to come up with a hundred other ways of getting that information to the sales team: Reports, Slack notifications, list exports, manual observation, or using Zapier to send an alert through virtually any medium. The downtime would last a few minutes, at most.</p>

<p></p>

<hr/>

<h2 id="startup-story">Startup Story</h2>

<p>One of my clients was using a legacy enterprise marketing automation platform (Marketo) with 629 automated processes built up over several years. When something broke or needed tweaking, there was only one person among the 150+ employees who could do it. Each issue took several days or even weeks to fix, all the while marketing campaigns stalled. And with each patch, the overall system only grew more complex.</p>

<p></p>

<p>When that person left the company, there was nobody left to operate the system. With every passing week a new issue would come up, faster than we could find and fix them.</p>

<p>To keep the marketing operation from coming to a standstill, I rushed to migrate the company from Marketo to HubSpot, a more simple platform that would be easier to operate and troubleshoot.</p>

<p>The migration took just one week. Along the way, however, another complex system reared its head: Salesforce. There were 10 automated processes in Salesforce with over 100 combined operations, all dependent on various delicately timed automations in Marketo. It took two weeks—twice as long as the migration—to understand and integrate those processes with the new marketing platform.</p>

<p></p>

<p>Overall, these two complex systems (in Marketo and in Salesforce) resulted in six weeks of downtime for the marketing team and three weeks of downtime for the sales team. That’s not counting the many weeks of downtime they experienced throughout the past few years, nor the many weeks of downtime they would experience in the future if we did not overhaul the underlying systems.</p>

<p>In the end, the system I put in place had 97% fewer processes (from 629 to 20) while providing all the same capabilities. A bug that was found a few days later got resolved in four minutes.</p>

<p>This experience made me wonder what principles startups can adopt to avoid the pitfalls of complex systems.</p>

<h2 id="principles-for-simple-systems">Principles for Simple Systems</h2>

<p>Rip-and-replace projects are painful and disruptive, even when the long-term benefits are worth it. Many startups—as with ships—don’t have the luxury of time and resources to perform overhauls once they’re underway.</p>

<p>Here are my three principles to follow when evaluating or implementing new systems:</p>

<ol>
  <li>
    <p><strong>Features don’t justify complexity.</strong> What good is a complicated flight control system if it grounds an entire fleet of aircraft, or an enterprise marketing platform like Marketo if nobody can run a marketing campaign? Choose tools that are simple to operate over those that promise the most features. A frequent recommendation I give to startups is to choose HubSpot for their marketing platform instead of an enterprise platform like Marketo, Eloqua, or Pardot.</p>
  </li>
  <li>
    <p><strong>Complex ideas lead to complex implementations.</strong> If it takes too long to explain or grasp an idea, then its implementation will be complex, and it will take too long to fix when something inevitably breaks. For example, a proposed sales process that requires an hour-long presentation will be a nightmare to maintain, regardless of how clever it seems.</p>
  </li>
  <li>
    <p><strong>Modifications before additions.</strong> When new requirements come up, the tendency is to add layers on top of the existing system—by way of additional steps or integrations. Instead, see if the system’s core can be modified to meet the new requirements. The change may cause (planned) downtime upfront, as with my Marketo-to-HubSpot migration example, but less (unplanned) downtime over the long term.</p>
  </li>
</ol>

<h2 id="smooth-sailing">Smooth Sailing</h2>

<blockquote>
  <p>“… the more simple any thing is, the less liable it is to be disordered, and the easier repaired when disordered.” — Thomas Paine, Common Sense, 1776</p>
</blockquote>

<p>There’s no question things will break along the startup journey, just as surely as they do on a ship crossing the globe. However, if the onboard systems are simple, those issues won’t leave the startup drifting helplessly in the middle of the ocean.</p>

<hr/>

<p><em>Image source: <a href="http://maritime-connector.com/ship/maersk-mc-kinney-moller-9619907/">http://maritime-connector.com/ship/maersk-mc-kinney-moller-9619907/</a></em></p>


    <p>◼</p>

    <p>PS - Liked this article? I write one every month or so, covering lessons learned on B2B startup growth. Don&#39;t miss the next one:</p>

    <!-- Begin MailChimp Signup Form -->
    

    <!--End mc_embed_signup-->
    </div>

  

  
  
  



</article>

<!-- Begin MailChimp popup signup form -->



<!-- End MailChimp popup signup form -->
      </div>
    </div></div>]]></content:encoded>
      <guid>https://www.gkogan.co/blog/simple-systems/</guid>
      <pubDate>Wed, 04 Aug 2021 09:16:24 +0000</pubDate>
      <source>https://www.gkogan.co/blog/simple-systems/</source>
    </item>
    <item>
      <title>Pitfalls of Data Anonymization</title>
      <link>https://palant.info/2020/02/18/insights-from-avast/jumpshot-data-pitfalls-of-data-anonymization/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___palant_info_2020_02_18_insights-from-avast_jumpshot-data-pitfalls-of-data-anonymization_/image.jpg" /> 
<div id="readability-page-1" class="page"><div>
              <p>There has been a surprising development after my <a href="https://palant.info/2020/01/27/avasts-broken-data-anonymization-approach/">previous article on the topic</a>, Avast having <a href="https://www.pcmag.com/news/avast-to-end-browser-data-harvesting-terminates-jumpshot">announced that they will terminate Jumpshot and stop selling users’ data</a>. That’s not the end of the story however, with the Czech Office for Personal Data Protection <a href="https://www.uoou.cz/en/vismo/dokumenty2.asp?id_org=200156&amp;id=1896">starting an investigation into Avast’s practices</a>. I’m very curious to see whether this investigation will confirm Avast’s claims that they were always fully compliant with the <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a> requirements. For my part, I now got a glimpse of what the Jumpshot data actually looks like. And I learned that I massively overestimated Avast’s success when anonymizing this data.</p>
<figure></figure>

<p>In reality, the data sold by Jumpshot contained plenty of user identifiers, names, email addresses, even home addresses. That’s partly due to Avast being incapable or unwilling to remove user-specific data as they <a href="https://patents.google.com/patent/US20160203337A1">planned to</a>. Many issues are generic however and almost impossible to avoid. This once again underlines the central takeaway: anonymizing browser history data is very hard. That’s especially the case if you plan to sell it to advertisers. You <em>can</em> make data completely anonymous, but you will have to dumb it down so much in the process that advertisers won’t have any use for it any more.</p>
<p>Why did I decide to document Avast’s failure in so much detail? My goal is to spread appreciation for the task of data anonymization: it’s very hard to ensure that no conclusions about users’ identity are possible. So maybe whoever is toying with the idea of collecting anonymized data will better think twice whether they really want do go there. And maybe next time we see a vendor collecting data we’ll ask the right questions about how they ensure it’s a “completely anonymous” process.</p>


<h2 id="the-data"><a href="#the-data"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>The data</h2>
<p>The data I saw was an example that Jumpshot provided to potential customers: an excerpt of real data for one week of 2019. Each record included an exact timestamp (milliseconds precision), a persistent user identifier, the platform used (desktop or mobile, which browser), the approximate geographic location (country, city and ZIP code derived from the user’s IP address), a guess for user’s gender and age group.</p>
<p>What it didn’t contain was “every click, on every site.” This data sample didn’t belong to the “All Clicks Feed” which has received much media attention. Instead, it was the “Limited Insights Pro Feed” which is supposed to merely cover user’s shopping behavior: which products they looked at, what they added to the cart and whether they completed the order. All of that limited to shopping sites and grouped by country (Germany, UK and USA) as well as product category such as Shoes or Men’s Clothing.</p>
<p>This doesn’t sound like there would be all too much personal data? But there is, thanks to a “referrer” field being there. This one is supposed to indicate how the user came to the shopping site, e.g. from a Google search page or by clicking an ad on another website. Given the <a href="https://palant.info/2019/10/28/avast-online-security-and-avast-secure-browser-are-spying-on-you/#what-data-is-being-sent">detailed information collected by Avast</a>, determining this referrer website should have been easy – yet Avast somehow failed this task. And so the supposed referrer is typically a completely unrelated random web page that this user visited, and sometimes not even a page but an image or JSON data.</p>
<p>If you extract a list of these referrers (which I did), you see news that people read, their web mail sessions, search queries completely unrelated to shopping, and of course porn. You get a glimpse into what porn sites are most popular, what people watch there and even what they search for. For each user, the “limited insights” actually contain a tiny slice of their entire browsing behavior. Over the course of a week this exposed way too much information on some users however, and Jumpshot customers watching users over longer periods of time could learn a lot about each user even without the “All Clicks Feed.”</p>
<h2 id="what-about-anonymization"><a href="#what-about-anonymization"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>What about anonymization?</h2>
<p>Some parameters and address parts have been censored in the data. For example, you will see an entry like the following:</p>
<pre><code>http://example.com/email/edit-details/[PII_EMAIL_abcdef1234567890]
</code></pre>
<p>A heuristic is at work here and will replace anything looking like an email address with a placeholder. Other heuristics will produce placeholders like <code>[PII_USER_abcdef1234567890]</code> and <code>[PII_NM_abcdef1234567890]</code> – these seem to be more rudimentary, applying based on parameter names. This is particularly obvious in entries like this one:</p>
<pre><code>https://www.ancestry.co.uk/name-origin?surname=[PII_NM_abcdef1234567890]
</code></pre>
<p>Obviously, the <code>surname</code> parameter here is merely a search query. Given that search queries aren’t being censored elsewhere, it doesn’t make much sense to censor them here. But this heuristic isn’t terribly clever and cannot detect whether the parameter refers to the user.</p>
<p>Finally, the <a href="https://palant.info/2020/01/27/avasts-broken-data-anonymization-approach/#the-patented-data-scrubbing-approach">generic algorithm described in the previous article</a> seems to apply, this one will produce placeholders like <code>[PII_UNKWN_abcdef1234567890]</code>.</p>
<h2 id="failures-to-censor-user-specific-parameters"><a href="#failures-to-censor-user-specific-parameters"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Failures to censor user-specific parameters</h2>
<p>It isn’t a big surprise that heuristic approaches will miss some data. The generic algorithm seemed sane from <a href="https://patents.google.com/patent/US20160203337A1">its description in the patent</a> however and should be able to recognize most user-specific data. In reality, this algorithm appears misimplemented, censoring only few of the relevant parameters and without an apparent system. So you will see addresses like the following without any censoring applied:</p>
<pre><code>https://nolp.dhl.de/nextt-online-public/set_identcodes.do?zip=12345&amp;idc=9876543210987654321
</code></pre>
<p>Residents of Germany will immediately recognize this as a DHL package tracking link. The <code>idc</code> parameter is the package identifier whereas the sometimes present <code>zip</code> parameter is the recipient’s ZIP code. And now you’d need to remember that DHL only requires you to know these two pieces of information to access the “detailed view,” the one that will show you the name of whoever received the package. Yes, now we have a name to associate the browsing history with. And even if the <code>zip</code> parameter isn’t in the tracking link – remember, the data contains a guess for it based on the user’s IP address, a fairly accurate one in fact.</p>
<p>Want more examples? Quite a few “referrers” are related to the authentication process of websites. A search for keywords like “oauth”, “openid” or “token” will produce lots of hits, usually without any of the parameters being censored. Worst-case scenario here: somebody with access to Jumpshot data could hijack an already authenticated session and impersonate this user, allowing them to access and modify user’s data. One has to hope that larger websites like Facebook and Google use short enough expiration intervals that such attacks would be impracticable for Jumpshot customers.</p>
<p>JWT tokens are problematic even under ideal conditions however. <a href="https://jwt.io/">JWT</a> is an authentication approach which works without server-side state, all the relevant information is encoded in the token itself. These tokens are easily found by searching for the “.ey” keyword. There are some issued by Facebook, AOL, Microsoft and other big names. And after reversing Base64 encoding you get something like:</p>
<pre><code>{&#34;instanceId&#34;:&#34;abcd1234&#34;,&#34;uid&#34;:12345,&#34;nonce&#34;:&#34;dcba4321&#34;,&#34;sid&#34;:&#34;1234567890&#34;}
</code></pre>
<p>Most values here are implementation-specific and differ from site to site. But usually there is some user identifier, either a numerical one (can likely be converted into a user name somewhere on the website), occasionally an email or even an IP address. It also often contains tokens related to the user’s session and potentially allowing hijacking it: session identifier, nonce, sometimes even OAuth tokens.</p>
<p>Last but not least, there is this:</p>
<pre><code>https://mail.yandex.ru/u1234/?uid=987654321&amp;login=myyandexname#inbox
</code></pre>
<p>This address also wasn’t worth censoring for Avast. Now I never used Yandex Mail but I guess that this user’s email address is <code>myyandexname@yandex.ru</code>. There are quite a few addresses looking like this, most of them contain only the numerical user identifier however. I strongly suspect that some Yandex service or API allows translating these numerical IDs into user names however, thus allowing to deduce user’s email address.</p>
<h2 id="shortcomings-of-the-heuristics"><a href="#shortcomings-of-the-heuristics"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Shortcomings of the heuristics</h2>
<p>Now let’s have a look at the heuristic removing email addresses, the last line of defense. This one will reliably remove any URL-encoded email addresses, so you won’t find anything like <code>me%40example.com</code> in the data. But what about unusual encodings? Heuristics aren’t flexible, so these won’t be detected.</p>
<p>It starts with the obvious case of URL encoding applied twice: <code>me%2540example.com</code>. The Avast data contains plenty of email addresses encoded like this, for example:</p>
<pre><code>https://m.facebook.com/login.php?next=https%3A%2F%2Fm.facebook.com%2Fn%2F
%3Fthread_fbid%3D123456789%26source%3Demail%26cp%3Dme%2540example.com
</code></pre>
<p>Did you notice what happened here? The email address isn’t a parameter to Facebook’s <code>login.php</code>. The only parameter here is <code>next</code>, it’s the address to navigate to after a successful login. And that address just happens to contain the user’s email address as a parameter, for whatever reason. Hence URL encoding applied twice.</p>
<p>Another scenario:</p>
<pre><code>https://www.google.com/url?q=http://example.com/
confirm?email%3dme%2540example.com&amp;source=gmail
</code></pre>
<p>What’s that, a really weird Google query? The <code>source=gmail</code> parameter indicates that it isn’t, it’s rather a link that somebody clicked in Gmail. Apparently, Gmail will will send such links as “queries” to the search engine before the user is redirected to their destination. And the destination address contains the email address here, given how the link originated from an address confirmation email apparently. Links from newsletters will also frequently contain the user’s email address.</p>
<p>And then there is this unexpected scenario:</p>
<pre><code>https://mail.yahoo.com/d/search/name=John%2520Smith&amp;emailAddresses=me%2540example.com
</code></pre>
<p>I have no idea why search in Yahoo Mail will encode parameters twice but it does. And searches of Yahoo Mail users contain plenty of names and email addresses of the people they communicate with.</p>
<p>Note that I only mentioned the most obvious encoding approach here. Some websites encode their parameters using Base64 encoding for some reason, and these also contain email addresses quite frequently.</p>
<h2 id="where-do-these-users-live"><a href="#where-do-these-users-live"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Where do these users live?</h2>
<p>So far we have names, email and IP addresses. That’s interesting of course but where do these users actually live? Jumpshot data provides only a rough approximation for that. Luckily (or unfortunately – for the users), Google Maps is a wildly popular service, and so it is very present in the data. For example:</p>
<pre><code>https://www.google.de/maps/@52.518283,13.3735008,17z
</code></pre>
<p>That’s a set of very precise geographical coordinates, could it be the user’s home? It could be, but it also might be a place where they wanted to go, or just an area that they looked at. The following entry is actually way more telling:</p>
<pre><code>https://www.google.de/maps/dir/Platz+der+Republik+1,+10557+Berlin/
Museum+für+Kommunikation,+Leipziger+Straße,+Berlin/@52.5140286,13.3774848,16z
</code></pre>
<p>By Avast’s standards, a route planned on Google Maps isn’t personally identifiable information – any number of people could have planned the same route. However, if the start of the route is an address and the end a museum, a hotel or a restaurant, it’s a fairly safe bet that the address is actually the user’s home address. Even when it isn’t obvious which end of the route the user lives at, the ZIP code in the Jumpshot data helps one make an educated guess here.</p>
<p>And then you type “Platz der Republik 1, Berlin” into a search engine and in quite a few cases the address will immediately map to a name. So your formerly anonymous user is now called Angela Merkel.</p>
<h2 id="wasn-t-it-all-aggregated"><a href="#wasn-t-it-all-aggregated"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Wasn’t it all aggregated?</h2>
<p>In 2015 Avast’s then-CTO Ondřej Vlček <a href="https://forum.avast.com/?topic=171725.0">promised</a>:</p>
<blockquote>
<p>These aggregated results are the only thing that Avast makes available to Jumpshot customers and end users.</p>
</blockquote>
<p>Aggregation would combine data from multiple users into a single record, an approach that would make conclusions about individual users much harder. Sounds quite privacy-friendly? Unfortunately, Jumpshot’s marketing already cast significant doubt on the claims that aggregation is being used consistently.</p>
<p>What was <a href="https://palant.info/2020/01/27/avasts-broken-data-anonymization-approach/#what-about-aggregation">merely a suspicion</a> in my previous blog post is now a fact. I don’t want to say anything about Jumpshot data in general, I haven’t seen all of it. But the data I saw wasn’t aggregated at all, each record was associated with exactly one user and there was a unique user identifier to tell records from different users apart. Also, I’ve seen marketing material for the “All Clicks Feed” suggesting that this data isn’t aggregated either.</p>
<p>The broken promises here aren’t terribly surprising, aggregated data is much harder to monetize. I already quoted Graham Cluley before with <a href="https://www.grahamcluley.com/week-avg-flogs-web-browsing-search-history/">his prediction</a> from 2015:</p>
<blockquote>
<p>But let’s not kid ourselves. Advertisers aren’t interested in data which can’t help them target you. If they really didn’t feel it could help them identify potential customers then the data wouldn’t have any value, and they wouldn’t be interested in paying AVG to access it.</p>
</blockquote>
<h2 id="conclusions"><a href="#conclusions"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Conclusions</h2>
<p>I looked into a week’s worth of data from a “limited insights” product sold by Jumpshot and I was already able to identify a large number of users, sometimes along with their porn watching habits. The way this data was anonymized by Avast is insufficient to say the least. Companies with full access to the “every click, on every site” product were likely able to identify and subsequently stalk the majority of the affected users. The process of identifying users was easy to automate, e.g. by looking for double encoded email addresses or planned Google Maps routes.</p>
<p>The only remaining question is: why is it that Avast was so vehemently denying selling any personally identifiable data? Merely a few days before deciding to shut down Jumpshot Avast’s CEO Ondřej Vlček repeated in a <a href="https://blog.avast.com/our-commitment-to-responsible-data-use">blog post</a>:</p>
<blockquote>
<p>We want to reassure our users that at no time have we sold any personally identifiable information to a third party.</p>
</blockquote>
<p>So far we only suspected, now we can all be certain that this statement isn’t true. To give them the benefit of doubt, how could they have not known? The issues should have been pretty obvious to anybody who took a closer look at the data. The whole scandal took months to unwind. Does this mean that throughout all that time Avast kept repeating this statement, giving it to journalists and spreading it on social media, yet nobody bothered to verify it? If we follow this line of thought then the following statement from the same blog post is clearly a bold lie:</p>
<blockquote>
<p>The security and privacy of our users worldwide is Avast’s priority</p>
</blockquote>
<p>I for my part removed all the raw and processed Jumpshot data in presence of witnesses after concluding this investigation. Given the nature of this data, this seems to be the only sensible course of action.</p>

            </div></div>]]></content:encoded>
      <guid>https://palant.info/2020/02/18/insights-from-avast/jumpshot-data-pitfalls-of-data-anonymization/</guid>
      <pubDate>Wed, 04 Aug 2021 08:01:35 +0000</pubDate>
      <source>https://palant.info/2020/02/18/insights-from-avast/jumpshot-data-pitfalls-of-data-anonymization/</source>
    </item>
    <item>
      <title>Who Were the Romans, Part V: Saving and Losing an Empire</title>
      <link>https://acoup.blog/2021/07/30/collections-the-queens-latin-or-who-were-the-romans-part-v-saving-and-losing-an-empire/</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
		
<p>This is the fifth and final part (<a href="https://acoup.blog/2021/06/11/collections-the-queens-latin-or-who-were-the-romans-part-i-beginnings-and-legends/">I</a>, <a href="https://acoup.blog/2021/06/25/collections-the-queens-latin-or-who-were-the-romans-part-ii-citizens-and-allies/">II</a>, <a href="https://acoup.blog/2021/07/16/collections-the-queens-latin-or-who-were-the-romans-part-iii-bigotry-and-diversity-at-rome/">III</a>, <a href="https://acoup.blog/2021/07/23/collections-the-queens-latin-or-who-were-the-romans-part-iv-the-color-of-purple/" data-type="URL" data-id="https://acoup.blog/2021/07/23/collections-the-queens-latin-or-who-were-the-romans-part-iv-the-color-of-purple/">IV</a>) of our series asking the question ‘Who were the Romans?’  How did they understand themselves as a people and the idea of ‘Roman’ as an identity? Was this a homogeneous, ethnically defined group, as some versions of pop folk history would have it, or was ‘Roman’ always a complex identity which encompassed a range of cultural, ethnic, linguistic and religious groups?  This week we are at last going to look at the end of the empire.</p>



<p>In the first four parts of this series, we established that by any reasonable measure, the Roman world ought to have been considered ‘diverse,’ from its very beginnings as a frontier town sitting at the crossroads of several different cultural zones in Italy through Rome’s experience incorporating the culturally and linguistically diverse peoples of pre-Roman Italy into its army and later into its citizen body, to the Roman Empire of the third and fourth centuries AD which encompassed a bewildering array of peoples from all over the Mediterranean basin.  The Romans didn’t merely conquer and subjugate a wide variety of peoples (though they did that), they also steadily incorporated those people in a process that was mostly motivated by expediency and the desire to wring the maximum amount of military power out of Rome’s conquests but which had the side effect of minting new Romans out of conquered peoples.</p>



<p>Consequently, by the  end of the first century BC, the <em>populus Romanus</em> had come to include the many different peoples of pre-Roman Italy and Cisalpine Gaul.  Over the decades that followed, Roman identity, rooted in the legal status of citizenship (but often having also linguistic and cultural expressions) would filter through the elite all over the Roman Empire, slowly expanding downward until in 212 the <em>Constitutio Antoniniana</em> extended citizenship to all free people in the Roman empire.  People that ‘joined’ the <em>populus Romanus</em> this way didn’t give up their other identities; as we’ve seen their identities tended to layer, with the Roman toga added on top (though as we’ve also seen, that Roman identity, once obtained, tended to be a very <em>important</em> identity).</p>



<p><strong>By this point, I should hope that the Hollywood vision of Rome as a culturally homogeneous society at any point in its history, speaking the Queen’s Latin, has been well and truly vanquished. </strong> Yet that vision of Rome ends up being used to support a particular vision of the causes for Rome’s rise and fall which, having now established the nature of our evidence for diversity in Rome, it is time to approach.  <strong>The argument –<a href="https://www.tandfonline.com/doi/full/10.1080/23311983.2017.1390915" data-type="URL" data-id="https://www.tandfonline.com/doi/full/10.1080/23311983.2017.1390915"> a form of it notably made in 2015 by Niall Ferguson</a> (conspicuously not a Roman historian, I might note) – goes thusly, that Rome was once homogeneous, that the superior power of a homogeneous society allowed Rome to expand, that expansion made Rome diverse and that this weakened Rome such that it fell.  Often it appears in the more simplistic form that ‘Rome fell because they let the barbarians in’ with lots of attendant political implications for current policy.</strong></p>



<p><strong>Now we already know at the outset that the front half of this argument – the assumption that Rome was initially homogeneous and able to expand because of that – is fatally flawed</strong>, because Rome was <em>never </em>homogeneous in that way.  But what about the second half of that argument?  Did letting all of these folks from Spain, Gaul, Britain, North Africa, the Levant, Greece, the Balkans and so on, did that somehow dilute the special alchemy that made the Romans successful?</p>



<p>But first, as always, if you like what you are reading here, please share it; if you really like it, you can support me on <a href="https://www.patreon.com/user?u=20122096">Patreon</a>. And if you want updates whenever a new post appears, you can click below for email updates or follow me on twitter (<a href="https://twitter.com/BretDevereaux">@BretDevereaux</a>) for updates as to new posts as well as my occasional ancient history, foreign policy or military history musings.</p>






<h2>The Gravity of the Situation</h2>



<p>I want to start with the observation I offer whenever I am asked (and being a Romanist, this happens frequently) ‘why did Rome fall?’ which is to note that in asking that question we are essentially asking the <em>wrong</em> question or at least a less interesting one.  This will, I promise, come back to our core question about diversity and the fall of Rome but first we need to frame this issue correctly, because Rome fell for the same reason all empires fall: gravity.</p>



<p>An analogy, if you will.  Imagine I were to build a bridge over a stream and for twenty years the bridge stays up and then one day, quite unexpectedly, the bridge collapses.  We can ask why the bridge fell down, but the fundamental force of gravity which caused its collapse was always working on the bridge.  As we all know from our physics classes, the force of gravity was always active on the bridge and so some other set of forces, channeled through structural elements was needed to be continually resisting that downward pressure.  <strong>What we really want to know is ‘what force which was keeping the bridge up in such an unnaturally elevated position stopped?’  </strong>Perhaps some key support rotted away?  Perhaps rain and weather shifted the ground so that what once was a stable position twenty years ago was no longer stable?  Or perhaps the steady work of gravity itself slowly strained the materials, imperceptibly at first, until material fatigue finally collapse the bridge.  <strong>Whatever the cause, we need to begin by conceding that, as normal as they may seem to us, bridges are not generally some natural construction, but rather a deeply unnatural one, which must be held up and maintained through continual effort</strong>; such a thing may fail even if no one actively destroys it, merely by lack of maintenance or changing conditions.</p>



<p><strong>Large, prosperous and successful states are always and everywhere like that bridge: they are <em>unnatural</em> social organizations, elevated above the misery and fragmentation that is the natural state of humankind only by great effort; gravity ever tugs them downward</strong>.  Of course when states collapse there are often many external factors that play a role, like external threats, climate shifts or economic changes, though in many cases these are pressures that the state in question has long endured.  Consequently, the more useful question is not why they fall, but why they stay up at all.</p>



<p>And that question is even more pointed for the Roman Empire than most.  While not the largest empire of antiquity, the Roman empire was <em>very</em> large (<a href="https://books.google.com/books?id=9mkLEAAAQBAJ&amp;pg=PA103#v=onepage&amp;q&amp;f=false" data-type="URL" data-id="https://books.google.com/books?id=9mkLEAAAQBAJ&amp;pg=PA103#v=onepage&amp;q&amp;f=false">Walter Scheidel figures that, as a percentage of the world’s population at the time, the Roman Empire was the fifth largest ever</a>, rare company indeed); while not the longest lasting empire of antiquity, it did last an uncommonly long time at that size.  It was also geographically positioned in a space that doesn’t seem particularly well-suited for building empires in.  While the Mediterranean’s vast maritime-highway made the Roman Empire possible, the geography of the Mediterranean has historically encouraged quite a lot of fragmentation, particularly (but not exclusively) in Europe.  Despite repeated attempts, no subsequent empire has managed to recreate Rome’s frontiers (the Ottomans got the closest, effectively occupying the Roman empire’s eastern half – with a bit more besides – but missing most of the west).</p>



<p>The Roman Empire was also, for its time, uncommonly prosperous.  As we’ll see, there is at this quite a lot of evidence to suggest that the territory of the Roman Empire enjoyed a meaningfully higher standard of living and a more prosperous economy during the period of Roman control than it did either in the centuries directly before or directly after (though we should not overstate this to the point of assuming that Rome was more prosperous than <em>any</em> point during the Middle Ages).  And while the process of <em>creating</em> the Roman empire was <em>extremely</em> violent and traumatic (again, a recommendation for G. Baker, <a href="https://amzn.to/36mlCiy"><em>Spare No One: Mass Violence in Roman Warfare</em></a> (2021) for a sense of just how violent), subsequent to that, the evidence strongly suggests that life in the interior Roman Empire was remarkably peaceful during that period, with conflicts pushed out of the interior to the frontiers (though I would argue this almost certainly reflects an overall decrease in the total amount of military conflict, not merely a displacement of it).</p>



<p>The Roman Empire was thus a deeply unnatural, deeply unusual creature, a hot-house flower blooming untended on a rocky hillside.  <strong>The question is not why the Roman empire eventually failed – all states do, if one takes a long enough time-horizon – but why it lasted so long in such a difficult position.</strong>  Of course this isn’t the place to recount <em>all</em> of the reasons why the Roman Empire held together for so long, but we can focus on a few which are immediately relevant to our question about diversity in the empire.</p>



<h2>Diversity in the Roman Army</h2>



<p><strong>As we’ve seen, there had always been non-Romans fighting alongside Roman citizens in the army</strong>, for as long as we have reliable records to judge the point.  In the Republic (until the 80s BC) these had consisted mostly of the <em>socii</em>, Rome’s Italian allies.  These were supplemented by troops from whatever allies Rome might have at the time, but there was a key difference in that the <em>socii</em> were integrated permanently into the Roman army’s structure, with an established place in the ‘org. chart,’ compared to the forces of allies who might fight under their own leaders with an <em>ad hoc</em> relationship to the Roman army they were fighting with.  The end of the Social War (91-87BC) brought the Italians into the Roman citizen body and thus their soldiers into the legions themselves; it marked the effective end of the <em>socii</em> system, which hadn’t been expanded outside of Italy in any case.</p>



<p>But almost immediately we see the emergence of a new system for incorporating non-Romans, this time provincial non-Romans, into the Roman army.  These troops, called auxilia (literally, ‘helpers’) first appear in the Civil Wars, particularly with Caesar’s heavy reliance on Gallic cavalry to support his legions (which at this time seem not to have featured their own integrated cavalry support, as they had earlier in the republic and as they would later in the empire).  The system is at this point very ad hoc and the auxiliaries here are a fairly small part of Roman armies.  But when Augustus sets out to institutionalize and stabilize the Roman army after the Battle of Actium (31BC) and the end of the civil wars, the <em>auxilia</em> emerge as a permanent, institutional part of the Roman army.  Clearly, they were vastly expanded; by 23 AD they made up half of the total strength of the Roman army (Tac. <em>Ann</em>. 4.5) a rough equivalence that seems to persist at least as far as the <em>Constitutio Antoniniana</em> in 212.</p>



<p>Of course it was no particular new thing for the Romans to attempt to use their imperial subjects as part of their army.  The Achaemenid army had incorporated a bewildering array of subject peoples with their own distinctive fighting styles, a fact that Achaemenid rulers liked to commemorate (see below).  The Seleucid army at Magnesia (189) which the Romans defeated also had numerous non-Macedonian supporting troops: Cappadocians, Galatians, Carians, Cilicians, Illyrians, Dahae, Mysians, Arabs, Cyrtians and Elamites.  At Raphia (217) the Ptolemaic army incorporated Egyptian troops into the phalanx for the first time, but also included Cretans, Greek mercenaries, Thracians, Gauls and Libyans, <em>inter alia</em>.  Most empires try to do this.</p>



<figure><figcaption>Via Wikipedia, a relief from the tomb of Xerxes I (r.486-465) at Naqsh-e Rostam showing the various ethnicities of his army.</figcaption></figure>



<p>The difference here is the relative <em>performance</em> that Rome gets out of these subject-troops (both the <em>socii</em> and the <em>auxilia</em>).  Take those examples.  Quite a number of the ethnicities on Xerxes monument both served in the armies of Darius III fighting against Alexander but then <em>swiftly </em>switched sides to Alexander after he won the battles – the Ionians, Egypt, and Babylon greeted Alexander as a liberator (at least initially) which is part of why the Achaemenid Empire could crumble <em>so fast</em> so long as Alexander kept winning battles.  Apart from <a href="https://en.wikipedia.org/wiki/Siege_of_Tyre_(332_BC)" data-type="URL" data-id="https://en.wikipedia.org/wiki/Siege_of_Tyre_(332_BC)">Tyre </a>and <a href="https://en.wikipedia.org/wiki/Siege_of_Gaza" data-type="URL" data-id="https://en.wikipedia.org/wiki/Siege_of_Gaza">Gaza</a>, the tough sieges and guerilla resistance didn’t start until he reached the Persian homeland.  The auxiliaries in the Seleucid army at Magnesia famously fell apart under pressure, whereas the Roman <em>socii</em> stuck in the fight as well as the legions; our sources give us no sense at any point that the <em>socii</em> were ever meaningfully weaker fighters than the legions (if anything, Livy sometimes represents them as more spirited, though he has an agenda here, as discussed).  And the Ptolemaic decision to arm their Egyptian troops in the Macedonian manner won the battle (turns out, Egyptians could fight just as well as Greeks and Macedonians with the right organization and training) but their subsequent apparent decision not to <em>pay</em> <em>or respect</em> those troops as well as their Macedonians seems to have led quite directly to the ‘Great Revolt’ which crippled the kingdom (there is some scholarly argument about this last point, but while I think Polybius’ pro-Greek, anti-Egyptian bias creeps in to his analysis, he is fundamentally right to see the connection, Plb. 5.107.  Polybius thinks it was foolish to arm non-Greeks, but the solution here to saving the Ptolemaic kingdom would have been arming the Egyptians <em>and then incorporating them into the system of rule</em> rather than attempting to keep up the ethnic hierarchy with a now-armed, angry and underpaid underclass.  The Greek-speakers-only-club system of Ptolemaic rule was unsustainable in either case, especially with Rome on the horizon).</p>



<p><strong>By contrast, the <em>auxilia</em> were mostly very reliable. </strong> The <a href="https://en.wikipedia.org/wiki/Revolt_of_the_Batavi" data-type="URL" data-id="https://en.wikipedia.org/wiki/Revolt_of_the_Batavi">one major exception comes from 69 AD</a> – the ‘Year of the Four Emperors’ to give some sense of its chaos – when the Batavian chieftain Julius Civilis (himself an auxiliary veteran and a Roman citizen) revolted and brought one <em>ala</em> and eight cohorts drawn from the Batavi (probably around 4,500 men or so) with him, out of an empire-wide total of c. 150,000 <em>auxilia</em> (so maybe something like 3.3% of the total <em>auxilia</em>).  Indeed, the <em>legions</em> had worse mutinies – the mutiny on the Rhine (Tac. <em>Ann.</em> 1.16ff in 14AD) had involved six legions (c. 30,000 troops, nearly a full quarter of Rome’s 25 legions at the time).  This despite the fact that the <em>auxilia</em> were often deployed away from the legions, sometimes in their own forts (you’ll see older works of scholarship suggest that the <em>auxilia</em> were kept logistically depend on the legions, but more recent archaeology on exactly where they were has tended to push against this view).  Indeed, the <em>auxilia</em> were often the <em>only</em> military forces (albeit in small detachments) in the otherwise demilitarized ‘senatorial’ provinces (which comprised most of the wealthy, populous ‘core’ of the empire); they could be trusted with the job, provided they weren’t the only forces in their own <em>home</em> provinces (and after 69, they never were).  <strong>And the <em>auxilia</em> fought hard and quite well.</strong>  The Romans occasionally won battles with nothing <em>but</em> the <em>auxilia</em>, was with the Battle of Mons Graupius (83 AD, Tac. <em>Agricola</em> 35ff) where the legions were held in reserve and never committed, the <em>auxilia</em> winning the battle effectively on their own.  Viewers of the Column of Trajan’s spiral frieze have long noted that the <em>auxilia</em> on the monument (the troop-types are recognizable by their equipment) do most of the fighting, while the legions mostly perform support and combat engineering tasks.  We aren’t well informed about the training the <em>auxilia</em> went through, but what we do know points to long-service professionals who were drilled every bit as hard as the <em>famously</em> well-drilled legions.  Consequently, they had <a href="https://acoup.blog/2020/05/15/collections-the-battle-of-helms-deep-part-iii-the-host-of-saruman/" data-type="URL" data-id="https://acoup.blog/2020/05/15/collections-the-battle-of-helms-deep-part-iii-the-host-of-saruman/">exactly the sort of professional cohesion that we’ve already discussed</a>.</p>



<p><strong>Why this difference in effectiveness and reliability?</strong>  The answer is to be found in the difference in the terms under which they served.  <strong>Rather than being treated as the disposable native auxiliaries of other empires, the Romans acted like the <em>auxilia</em> mattered…because they did.</strong></p>



<p>First of all, the <em>auxilia</em> were paid.  Our evidence here is imperfect and still much argued about, but it seems that <em>auxilia</em> were paid 5/6ths of the wages of the legionary counterparts, with the cavalry <em>auxilia</em> actually paid more than the infantry legionaries.  While it might sound frustrating to be systematically paid 1/6th less than your legionary equivalent, the legions were paid fairly well.  The <em>auxilia</em> probably made in wages about as much as a normal day-laborer, but the wage was guaranteed (something very much not the case for civilian laborers) and while the cost of their rations was deducted from their pay, that deduction was a fixed amount that seems to have been set substantially below the market value of their rations, building in another subsidy.  Most auxiliaries seem to have been volunteers, because the deal in being an auxiliary was good enough to attract volunteers looking to serve a full tour of duty (around 20 years; this was a long-service professional army now so joining it meant making a career out of it).</p>



<p>And most importantly, eventually (perhaps under Tiberius or shortly thereafter) the <em>auxilia</em> began to receive a special grant of citizenship on finishing that tour of duty, one which covered the soldier, and any children he might have had by his subsequent spouse (including children had, it seems, <em>before</em> he left the army; Roman soldiers in this period were legally barred from contracting legal marriages while serving, so the grant is framed so that it retroactively legitimizes any children produced in a quasi-marriage when the tour of service is completed).  Consequently, whereas a soldier being dragooned or hired as a mercenary into <em>other</em> multi-ethnic imperial armies might end his service and go back to being an oppressed subject, the Roman auxiliary, by virtue of his service, <em>became Roman</em> and thus essentially <em>joined the ruling class</em> at least in ethnic status.  Auxiliaries also clearly got a share of the loot when offensive warfare happened and while there is a lot of debate as to if they also received the <em>praemia</em> (the large retirement bonus legionaries got), epigraphically it is pretty clear that auxiliaries who were careful with their money could establish themselves fairly well after their service.  I should also note that what we see of auxiliaries suggests they were generally well armed (with some exceptions, which may have more to do with stereotyped depictions of certain kinds of ‘barbarians’ than anything else): metal helmets, mail shirts (an expensive and high quality armor for the period), oval shields, a spear and the <em>spatha</em> – a Roman version of the classic Gallic one-handed cutting sword – are the standard visual indicator in Roman artwork for generic ‘auxiliaries.’  That is actually a fairly high-end kit; it is no surprise that the <em>auxilia</em> could win battles with it.</p>



<figure><figcaption><a href="https://en.wikipedia.org/wiki/Auxilia#/media/File:Engineering_corps_traian_s_column_river_crossing.jpg" data-type="URL" data-id="https://en.wikipedia.org/wiki/Auxilia#/media/File:Engineering_corps_traian_s_column_river_crossing.jpg">Via Wikipedia</a>, Roman auxiliaries crossing a pontoon bridge during Trajan’s invasion of Dacia, as seen on Trajan’s Column (c. 110) in Rome.</figcaption></figure>



<p><strong>The attentive should already be noting many of the components of the old <em>socii</em> system now in a new form</strong>: the non-Roman troops serve under similar conditions with the Romans, get similar pay and rations (forts occupied by the <em>auxilia</em> show no deviation from the standard Roman military diet), a share of loot and glory and can finally be rewarded for loyal service by being inducted into the Roman citizen body itself (which could mean their sons might well enroll in the legions, a thing which does seem to have happened, as we do see a fair bit of evidence for ‘military families’ over multiple generations).</p>



<p>(For those looking for more detail on the <em>auxilia</em>, a lot of this is drawn from a book I have already recommended, Ian Haynes, <em><a href="https://amzn.to/3g7KLSW">Blood of the Provinces: The Roman Auxilia and the Making of Provincial Society from Augustus to the Severans</a></em> (2013).  Also still useful for the history of the <em>development</em> of the <em>auxilia</em> is D.B. Saddington, <em><a href="https://amzn.to/2VoGV12" data-type="URL" data-id="https://amzn.to/2VoGV12">The Development of the Roman Auxiliary Forces from Caesar to Vespasian</a></em> (1982); this is, alas, not an easy book to find as it is – to my knowledge – long out of print, but your library may be able to track down a copy.)</p>



<h2>Helping Helpers</h2>



<p>That frankly <em>unusual</em> structure for a multi-ethnic imperial army brought three principle benefits for the Roman army and consequently for the Roman empire itself.</p>



<p><strong>The most obvious of these is manpower</strong>.  Especially with a long-service professional army, capable and qualified recruits are in limited supply.  The size of the Roman army during the imperial period ranged from around 300,000 to around 500,000, but in 14 AD (the year of Augustus’ death) there were only 4,937,000 Roman citizens (Res Gestae 8.11), a figure which probably (a word I using the gloss over one of the most technical and complex arguments in the field) includes women and children.  Needless to say, keeping something close to a fifth of the adult male citizen population under arms continually, forever was simply never going to be feasible.  After his victory in 31 BC at Actium, Octavian (soon to be Augustus) had acted quickly to pare down the legions, disbanding some, merging others, until he reached a strength of just 28 (25 after the three legions lost in 9 AD were not replaced).  It was a necessary move, as the massive armies that had been raised during the fever-pitch climax of the civil wars simply could not be kept under arms indefinitely, nor could a short-term service conscript army be expected to garrison the hundreds of miles of Roman <em>limes</em> (‘frontier, border’) in perpetuity.</p>



<p>Harnessing the manpower of the provinces was simply the necessary solution – so necessary that almost every empire does it.  By their very nature, empires consist of a core which rules over a much larger subject region, typically with far greater population; securing all of that territory almost always requires larger forces than the core’s population is able or willing to provide, leading to the recruitment of auxiliaries of all kinds.  But whereas many imperial auxiliaries, as noted above, turn out to be potential dangers or weaknesses, Rome’s <em>auxilia</em> seem to have been fairly robustly ‘bought in’ on the system, allowing Rome to access motivated, loyal, cohesive and highly effective manpower, quite literally doubling the amount of military force at their disposal.  Which in turn mattered a great deal because the combat role of the <em>auxilia</em> was significant, in stark contrast to many other imperial armies which might use auxiliaries only in subsidiary roles.</p>



<p><strong>The <em>auxilia</em> also served to supply many of the combat arms the Romans themselves weren’t particularly good at</strong>.  The Romans had always performed very well as heavy infantry and combat engineers, but only passably as light infantry and truly poorly as shock cavalry; they generally hadn’t deploy meaningful numbers of their own missile cavalry or archers at all.  We’ve already talked a lot about how social institutions and civilian culture can be important foundational elements for certain kinds of warfare, and this is no less true with the Romans.  But by recruiting from subject peoples whose societies <em>did</em> value and practice the kinds of warfare the Romans were, frankly, bad at, the Roman skill-set could be diversified.  And early on, this is exactly what we see the <em>auxilia</em> being used for (along with also providing supplemental heavy infantry), with <em>sagitarii</em> (archers), <em>funditores</em> (slingers), <em>exploratores</em> (scouts) and cavalry (light, heavy and missile), giving the Romans access to a combined arms fighting force with considerable flexibility.  And the system clearly works – even accounting for exaggerated victories, it is clear that Roman armies, stretched over so long a frontier, were both routinely outnumbered but also routinely victorious anyway.</p>



<figure><figcaption><a href="https://en.wikipedia.org/wiki/Auxilia#/media/File:086_Conrad_Cichorius,_Die_Reliefs_der_Traianss%C3%A4ule,_Tafel_LXXXVI.jpg" data-type="URL" data-id="https://en.wikipedia.org/wiki/Auxilia#/media/File:086_Conrad_Cichorius,_Die_Reliefs_der_Traianss%C3%A4ule,_Tafel_LXXXVI.jpg">Via Wikipedia</a>, another panel from the Column of Trajan in Rome.  You can see auxiliary archers in the top left wearing scale armor.  In the bottom center, you can see the legionaries, with their distinctive squared-off shields (the scutum) and their segmented armor (today called the lorica segmentata, though this is not an ancient term); to their right you can see more auxiliaries, wearing what may be mail under a textile cover and using oval shields.  They are fighting Dacians, who occupy the right half of the panel.</figcaption></figure>



<p>As Ian Haynes notes, the ethnic distinctiveness of various <em>auxilia</em> units does not seem to have lasted forever, though in some cases distinctive dress, equipment and fighting styles lasted longer.  Most <em>auxilia</em> were posted far from their regions of origin and their units couldn’t rely on access to recruits from their ‘homeland’ to sustain their numbers over the long haul (although some number of recruits would almost certainly come from the military families of veterans settled near the forts).  But that didn’t mean the loss of the expertise and distinctive fighting styles of the <em>auxilia</em>.  Rather skills, weapons and systems which worked tended to get diffused through the Roman army (particularly in the <em>auxilia</em>, but it is hard not to notice that eventually the <em>spatha</em> replaces the <em>gladius</em> as the sword of the legions).  As Ovid quips, <em>Fas est et ab hoste doceri</em>, “It is right to learn, even from the enemy” (<em>Met.</em> 4.428); the Romans do that a lot.  The long-service professional nature of these units presumably made a lot of this possible, with individual <em>cohortes</em> and <em>alae</em> becoming their own pockets of living tradition in the practice of various kinds of fighting and acclimating new recruits to it.  <strong>Consequently, not only did the Roman army get access to these fighting-styles, because the <em>auxilia</em> were actually <em>integrated</em> into the military system rather than merely attached to it, they also got the opportunity to adopt or imitate the elements of the fighting styles that worked</strong>.</p>



<p><strong>Finally, the <em>auxilia</em> system also minted new Romans</strong>.  We’ve already mentioned that <em>auxilia</em> veterans received Roman citizenship on retirement, but that wasn’t the extent of it.  We can see in inscriptions that the degree of cultural fluency that soldiers in the <em>auxilia</em> gained with Roman culture was high; they often adopted Roman or Romanized names and seem to have basically always learned Latin (presumably because their Roman officers wouldn’t have spoken their language).  While some units of the <em>auxilia</em> kept distinctive national dress as a sort of uniform, most of the <em>auxilia</em> seem to have adopted a style of dress that, while distinct from the legions, was generally in keeping with the Roman tradition of military dress (which was not <em>quite</em> the same as Roman civilian dress).  They also partook of the Roman military diet (Roman soldiers kept a similar diet all over the empire, even if that meant shipping thousands of amphora of olive-oil and sour wine to northern England) which would have given them a diet in common with many work-a-day Romans too.  Once retired, <em>auxilia</em> soldiers tended to settle where they served (rather than returning to their ‘home’ provinces), which meant settling in frontier provinces where their citizenship set them apart as distinctively <em>Roman</em>, wherever they may have come from.</p>



<figure><figcaption><a href="https://www.britishmuseum.org/collection/object/G_1930-0419-1" data-type="URL" data-id="https://www.britishmuseum.org/collection/object/G_1930-0419-1">From the British Museum</a>, a Roman military diploma (inv. 1930,0419.1), the official document granting citizenship to an auxiliary (here a fellow named Gemellus) at the end of his tour of service.  This one is dated July 17, 122 AD and was found in Brigetio (modern Szőny, Hungary).</figcaption></figure>



<p>Exactly how many <em>auxilia</em> would have retired like this requires a degree of number crunching.  Given a 20-year tour of service and zero mortality, we might expect around 7,500 men to pass through the <em>auxilia</em> each year.  But of course, mortality wasn’t zero and so we have to expect that of our c. 20-year-old recruits, some number are going to die before retirement.  Using some model life tables (following B. Frier, “Demography” in <em>CAH^2 XI</em> (2000)), we should figure that very roughly one third of our recruits will have died before reaching discharge.  We then we need to adjust our recruitment figures to retain the same total strength and we get something like 9,000 new recruits each year to keep a strength of c. 150,000 with mortality counted for and 20 year tours.  That gives us roughly 6,000 <em>auxilia</em> living to retirement each year.  That may <em>seem</em> a small number, but that graduate accretion matters when it runs for decades and centuries and the newly enfranchised family units (recall that the citizenship grant covers children and sort-of-kind-of his spouse*) tend to settle on the frontiers, which is a really handy place to have communities of citizens.  If we assume that these new citizen families mostly reproduced themselves (or more correctly that they went extinct or split with multiple children at roughly the same rate with no natural population growth), then we’d expect this process to produce perhaps something like 1.5 <em>million</em> new citizen households up until the <em>Constitutio Antoniniana</em>.  Being <em>very</em> back of the envelope then, we might – once we account for women and children descendants of those soldiers – assume that on the eve of the general grant of citizenship in 212, there were perhaps 4 million Romans whose citizenship status was a product of service in the <em>auxilia</em> somewhere in their history; perhaps representing something like 7% of the entire population (including non-free persons).  Were we to assume larger households (which seems wise, given that retired auxiliaries are probably more likely than average to be in an economic position to have a larger family), that figure would be even higher.</p>



<p>(*Note on the coverage of the spouse.  The grant of citizenship covered any biological children of the discharged auxiliary but did not extend citizenship to his wife.  It <em>did</em> however, give an auxiliary the right to contract a lawful marriage with effectively any free woman, <em>including non-citizens</em> and the children resulting from such a union would be citizens themselves.  Consequently, it extended one of the core privleges of citizenship to the non-citizen wife of a discharged auxiliary: the right to bear citizen children.  Since the wife would be part of the retired auxiliary’s household (and then later, if he predeceased her, potentially in the household of her male citizen children) she’d be legally covered in many cases because a legal action against her would generally be an action against her husband/child.  Given that a number of the rights of citizens simply didn’t apply to women in the Roman world (e.g. office holding), this system left the wife of a retired auxiliary with many, but not all, of the privileges of citizenship, so long as her husband and her marriage survived.  That said, the legal status remained vested in her husband or her children, which made it more than a little precarious.  One of these days, we can talk more about the structure of the Roman <em>familia</em>.)</p>



<p><strong>That is a very meaningful number of new Romans</strong>.  And those figures don’t account for some of the other ways Roman citizenship tended to expand through communities both through manumission but also the political networks citizenship created (your Latin-speaking former-auxiliary citizen neighbors are a lot more likely to be able to help intercede to get <em>you</em> citizenship or get your community recognized as a <em>municipia</em> with that attendant citizenship grant). <strong> And not only are those new Romans by legal status, but new Romans who have, by dint of military training and discipline, absorbed quite a lot of Roman culture</strong>.  <strong>As best we can tell, they tended to view the Roman Empire as <em>their</em> polity</strong>, rather than as a foreign or oppressive entity.  They were ‘bought in’ as it were.  Again, this does not seem to have been the Roman intent, but rather an opportunistic, self-serving response to the need to maintain the loyalty of these troops; citizenship was, after all, a free benefit the emperor might bestow at no cost to the treasury (since citizens who lived outside of Italy still owed taxes) or himself.</p>



<p>Of course that fits the <em>auxilia</em> in to a later pattern in the provinces which becomes perhaps most apparent as the Roman Empire begins to collapse…</p>



<h2>Kicking, Gouging and Screaming</h2>



<p>The fall of the Roman Empire in the West (please, right now, just mentally add the phrase ‘in the west’ next to every ‘the fall of Rome’ and similar phrase here and elsewhere) is complicated.  I don’t mean it is complicated in its causes or effects (though it is that too), I mean it is complicated in its raw events: the who, what, where and when of it.  Most students are taught a fairly simple version of this because most of what they need to actually learn is the cause and the effects and so the actual ‘fall’ part is a sort of black box where Huns, Vandals, Goths, plague, climate and economic decline go in and political fragmentation, more economic decline and the European Middle Ages come out.  The fall itself ends up feeling like an event rather than a process because it is compressed down to a single point, the black box where all of the causes become all of the effects.  That is, frankly, a defensible way to teach the topic at a survey level (where it might get at most a lecture or two either at the end of a Roman History survey or the beginning of a Medieval History survey) and it is honestly more or less how I teach it.</p>



<p><strong>But if you want to actually try to say something <em>intelligent</em> about the whole thing, you need to grapple with what actually happened, rather than the classroom black-box model designed for teaching efficiency rather than detail.</strong>  We are…not going to do that today…though I will have some bibliography here for those who want to. <strong> The key thing here is that the ‘Fall of Rome’ (in the West) is not an event, but a <em>century long</em> process from 376 to 476. Rome power (in the West) contracts for a lot of that, but it expands in periods as well, particularly under the leadership of Aetius (433-454) and Majorian (457-461); there are points where it would have really looked like the Romans might actually be able to recover</strong>.  Even in 476 it was not obvious to anyone that Roman rule had actually ended; Odoacer, who had just deposed what was to be the last Roman emperor in the west promptly offered the crown to Zeno, the Roman emperor in the East (there is argument about his sincerity but <a href="https://amzn.to/2VlzAPq" data-type="URL" data-id="https://amzn.to/2VlzAPq">James O’Donnell argues</a> – very well, though I disagree on some key points – that this represented a real opportunity for Rome to rise from defeat in a new form yet again).</p>



<figure><figcaption><a href="https://en.wikipedia.org/wiki/Fall_of_the_Western_Roman_Empire#/media/File:MajorianEmpire.png" data-type="URL" data-id="https://en.wikipedia.org/wiki/Fall_of_the_Western_Roman_Empire#/media/File:MajorianEmpire.png">Via Wikipedia</a>, a map of the fragmentation of the Western Roman Empire, but also noting Majorian’s considerable reconquests.  One wonders, had he not been killed by Ricimer in 461 what he might have achieved.</figcaption></figure>



<p>Glancing even further back historically, this wasn’t even the first time the Roman Empire had been on the brink of collapse.  Beginning in 238, the Roman Empire had suffered a long series of crippling civil wars and succession crises collectively known as the <a href="https://en.wikipedia.org/wiki/Crisis_of_the_Third_Century" data-type="URL" data-id="https://en.wikipedia.org/wiki/Crisis_of_the_Third_Century">Crisis of the Third Century</a> (238-284).  At one point, the empire was <em>de facto</em> split into three, with one emperor in Britain and Gaul, another in Italy, and the client kingdom of Palmyra essentially running the Eastern half of the empire under their queen Zenobia.  <strong>Empires do not usually survive those kinds of catastrophes, but the Roman Empire survived the Crisis, recovered all of its territory (save Dacia) and even enjoyed a period of relative peace afterwards, before trouble started up again.</strong></p>



<p><strong>The <em>reason</em> that empires do not generally survive those kinds of catastrophes is that generally when empires weaken, they find that they contain all sorts of people who have been waiting, sometimes patiently, sometimes less so, for any opportunity to break away</strong>.  The rather sudden collapse of the (Neo-)Assyrian Empire (911-609 BC) is a good case study.  After having conquered much of the Near East, the Assyrians fell into a series of succession wars beginning in 627; their Mesopotamian subjects smelled blood and revolted in 625.  That was almost under control by 620 when the Medes and Persians, external vassals of the Assyrians, smelled blood too and invaded, allying with the rebelling Babylonians in 616.  <strong>Assyria was effectively gone by 612 with the loss and destruction of Ninevah; they had gone from the largest empire in the world at that time <em>or at any point prior</em> to non-existent in 15 years</strong>.  While the Assyrian collapse is remarkable for its speed and finality, the overall process is much the same in most cases; once imperial power begins to wane, revolt suddenly looks more possible and so the downward slope of collapse can be very steep indeed (one might equally use the case study of decolonization after WWII as an example: each newly independent country increased the pressure on all of the rest).</p>



<p><strong>Yet there is no great rush to the doors for Rome</strong>.  Instead, as Guy Halsall puts it in <em><a href="https://amzn.to/3f8miwW" data-type="URL" data-id="https://amzn.to/3f8miwW">Barbarian Migrations and the Roman West</a></em> (2007), “The West did not drift hopelessly towards its inevitable fate.  It went down kicking, gouging and screaming.”  Among the kicked and gouged of course were Attila and his Huns.  Fought to a draw at the Battle of the Catalaunian Plains, his empire disintegrated after his death two years later under pressure from both Germanic tribes and the Eastern Roman Empire (and the standard tendency for Steppe empires to fragment); of his three sons, Ellac was killed by revolting Germanic peoples who had been subject to the Huns, Dengizich by the (Eastern) Romans (we’re told his head was put on display in Constantinople) and the last, Ernak just disappears in our narrative after the death of Dengizich.  The Romans, it turns out, did eventually get down to business to defeat the Huns.  But the Romans doing all of that kicking, gouging and screaming were not the handful of old families from the early days of the Repulic; most of those hard-fighting Romans were people who in 14 AD would have been provincials.  And indeed, the Roman Empire would survive, in the East, <em>where Rome wasn’t</em>, making for a Roman Empire that by 476 consisted effectively <em>entirely</em> of ‘provincial’ Romans.</p>



<p>Instead what we see are essentially three sets of actions by provincial elites who in any other empire would have been leading the charge for the exits.  There were the kickers, gougers and screamers, as Halsall notes.  There were also, as Ralph Mathisen, <a href="https://amzn.to/3rFVau6" data-type="URL" data-id="https://amzn.to/3rFVau6"><em>Roman Aristocrats in Barbarian Gaul</em> </a>(1993) has noted, elites who – seeing the writing on the wall – made no effort to hasten the collapse of the empire but instead retreated into their estates, their books and their letters; these fellows often end up married into and advising the new ‘barbarian’ kings who set up in the old Roman provinces (which in turn contributes quite a bit to the preservation and continued influence of Roman law and culture in the various fragmented successor states of the early Middle Ages).  Finally, there were elites so confident that the empire would survive – because it always had! – that they mostly focused on improving their position within the empire, even at the cost of weakening it, not because they wanted <em>out</em>, <em>but because ‘out’ was inconceivable to them</em>; both Halsall and also James O’Donnell, <em><a href="https://amzn.to/3zM0fUB" data-type="URL" data-id="https://amzn.to/3zM0fUB">The Ruin of the Roman Empire</a></em> (2009) document many of these.  If I may continue my analogy, when the exit door was yawning wide open, almost no one walked through; some tried to put out the burning building they were in, others were content to be at the center of the ruins.  But no one actually <em>left</em>.</p>



<p>During the Crisis of the Third Century, that set of responses had been crucial for the empire’s survival and for brief moments in the 400s, it looked like they might even have saved it again.  <strong>For all of the things that brought the Roman Empire down, it is striking that ‘internal revolts’ of long-ruled peoples weren’t one of them</strong>.  And that speaks to the power of Rome’s effective (if, again, largely unintentional) management of diversity.  <strong>The Roman willingness to incorporate conquered peoples into the core citizen body and into ‘Roman-ness’</strong> <strong>meant that even by 238 to the extent that the residents of the Empire could even <em>imagine</em> its collapse, they saw that potentiality as a disaster, rather than as a liberation</strong>.  That gave the empire tremendous resiliency in the face of disaster, such that it took a century of unremitting bad luck to bring it down and even then, it only managed to take down half of it.</p>



<p>(As an aside, <strong>those provincial Romans were <em>correct</em> in the judgement that the collapse of the empire would mean disaster</strong>.  The running argument about the fall of the Roman Empire is generally between the ‘decline and fall’ perspective, which presents the collapse of the Roman Empire as a Bad Thing and the ‘change and continuity’ perspective, which both stresses continuity after the collapse but also tends to try minimize the negative impacts of it, even to the point of suggesting that the average Roman peasant might have been better off in the absence of heavy Roman taxes.  That latter view is particularly common among many medievalists, who are understandably quite tired of the unfairly poor reputation their period gets.  This is an argument that for some time lived in the airy space of narrative and perspective where both sides could put an argument out.  Unfortunately for some of the change-and-continuity arguments about living standards, archaeology has a tendency to give us <em>data</em> that is somewhat less malleable.  That archaeological data shows, with a high degree of consistency, that while there is certainly some continuity between the Late Antique and the early Middle Ages the fall of Rome (in the West) killed <em>lots</em> of people (precipitous declines in population in societies without reliable birth control; probably this is mostly food scarcity, not direct warfare) and that living standards also declined <em>to a degree that the results are archaeologically visible</em>.  As Brian Ward-Perkins notes in <em><a href="https://amzn.to/377rZXK" data-type="URL" data-id="https://amzn.to/377rZXK">The Fall of Rome and the End of Civilization</a></em> (2005), the collapse causes <em>cows to shrink</em>, speaking to sudden scarcity of winter fodder (which in turn likely speaks to a general reduction in available nutrition).  Some areas were worse hit than others; Robin Flemming<em>, <a href="https://amzn.to/3BOQLd8" data-type="URL" data-id="https://amzn.to/3BOQLd8">Britain After Rome</a></em> (2010) notes, for instance, that in post-Roman Britain, <em>pot-making</em> technology was lost (because ceramic production had been focused in cities which had been largely depopulated out of existence).  The fall of Rome might have been good for some people, but the evidence is, I think, at this point inescapable that it was <em>quite bad </em>for most people.  Especially, one assumes, all of the people who got depopulated.)</p>



<figure><figcaption><a href="https://en.wikipedia.org/wiki/Fall_of_the_Western_Roman_Empire#/media/File:Europe_and_the_Near_East_at_476_AD.png" data-type="URL" data-id="https://en.wikipedia.org/wiki/Fall_of_the_Western_Roman_Empire#/media/File:Europe_and_the_Near_East_at_476_AD.png">Via Wikipedia</a>, the Roman world after the deposition of Romulus Augustulus in 476; fragmentation in the West, though Rome survives in the East.  Again, speaking to the complexity of the collapse, each of those successor ‘barbarian’ kingdoms has their own complicated story about how they ended up with that particular chunk of the empire.</figcaption></figure>



<h2>Foederati Failures</h2>



<p><strong>But if the Roman Empire (in the West) went down fighting, <em>why did it</em></strong><em> </em><strong><em>collapse</em>?</strong>  Of course there is no simple answer to that question.  The mass migrations of the fourth and fifth century clearly played a very large role, but then the Romans had defeated other such migrations (recall the Cimbri and the Teutones) before.  There are strong indicators that other factors, unrelated to our current topic were also at play: the empire had been economically weakened by the Crisis of the Third Century, which may have disrupted a lot of the trade and state functions that created the revenue to fund state activity.  At the same time, the Crisis and the more challenging security situation after it meant that Roman armies grew larger and with them the burden of paying and feeding the soldiers which further hurt the economy.  Meanwhile, long exposure to Roman armies on the frontiers of the empire had begun to erode the initially quite vast qualitative advantage the Romans enjoyed; the gap between Roman and ‘barbarian’ military capabilities began to shrink (although it never really vanished altogether in this period).  But some of the causes <em>do </em>bear on our topic but in quite the other direction from what the Niall Fergusons of the world might assume.</p>



<p>Let’s start with the <em>foederati</em>.</p>



<p>After the <em>Constitutio Antoniniana</em>, there was no longer much need for the <em>auxilia</em>, as all persons in the empire were citizens, and so the structure distinction between the legions and other formations fades away (part of this is also the tendency of the legions in this period to be progressively split up into smaller units called <em>vexillationes</em>, meaning that the unit-sizes wouldn’t have been so different).  But during the fourth century, with frontier pressures building, the Romans again looked for ways to utilize the manpower and fighting skill of non-Romans.  <strong>What is striking here is that whereas in some ways </strong>(discussed above) <strong>the <em>auxilia</em> had represented almost a revival of the attitudes which had informed the system for the <em>socii</em>, the new system that emerged for using foreign troops, called <em>foederati</em> (‘treaty men’)</strong> <strong>did <em>not </em>draw on the previously successful <em>auxilia-</em>system</strong> (which, to be clear, by this point had been effectively gone for more than a century).  Instead, the Romans signed treaties with Germanic-speaking kings, exchanging chunks of (often depopulated, war-torn frontier) land in exchange for military service.  Since these troops were bound by treaty (<em>foedus</em>) they were called <em>foederati</em>.  They served in their own units, under their own leaders, up to their kings.  Consequently, all of the mechanisms that encouraged the <em>auxilia</em> to adopt Roman practices and identify with the Roman Empire were lost; these men might view Rome as a friendly ally (at times) but they were never encouraged to think of themselves as Roman.</p>



<p>The reason for this different system of recruitment seem to be rooted in financial realities.  The Roman army had already been expanded during the Crisis of the Third Century and only grew more under Diocletian and Constantine, probably by this point being between 400,000 and 500,000 men (compared to 300,000-350,000 earlier in the empire).  Moreover, Diocletian had opted to reform the empire’s administration with a much more intensive, top-down, bureaucratic approach, which imposed further costs.  Taxes had become heavy (although elites were increasingly allowed to dodge them), the economy was weak and revenues were short.  The value of the <em>foederati</em> was that the empire didn’t have to <em>pay</em> them; they were handed land (again, in war-torn frontier zones) and expected to use that to pay for their military support.  At the time, it must have seemed a brilliant work-around to get more military power out of a dwindling tax-base.</p>



<figure><figcaption><a href="https://en.wikipedia.org/wiki/Tetrarchy#/media/File:Venice_%E2%80%93_The_Tetrarchs_03.jpg" data-type="URL" data-id="https://en.wikipedia.org/wiki/Tetrarchy#/media/File:Venice_%E2%80%93_The_Tetrarchs_03.jpg">Via Wikipedia</a>, the porphyry statues of the tetrarchs, the four emperors under Diocletian, now in Venice.  While the artist is drawing attention to their notional unity (thus all of the hugging), it is also notable how much the Third Century had changed the job of emperors.  During the first two centuries, Roman emperors were more often depicted in their civilian dress than as soldiers, but here the martial imagery, including the hands gripping tight the hilts of swords, is strongly pronounced.  The emperor was, by this point, a soldier first and a civilian administrator second.</figcaption></figure>



<p>(I feel the need to note that I increasingly regard Diocletian (r. 284-305) as a ruinous emperor, even though he lacked the normal moralizing character flaws of ‘bad emperors.’  While he was active, dedicated and focused, almost all of his reforms turned out to be quite bad ideas in the long run even before one gets to the <a href="https://en.wikipedia.org/wiki/Diocletianic_Persecution" data-type="URL" data-id="https://en.wikipedia.org/wiki/Diocletianic_Persecution">Great Persecution</a>.  His currency reforms were catastrophic, his administrative reforms were top-heavy, his tax plan depended on a regular census which was never regular and the <a href="https://en.wikipedia.org/wiki/Tetrarchy" data-type="URL" data-id="https://en.wikipedia.org/wiki/Tetrarchy">tetrarchy </a>was doomed from its inception.  Diocletian was pretty much a living, “Well, You Tried” meme.  That said, to be clear, Diocletian wasn’t responsible for the <em>foederati</em>; it’s not quite clear who the first <em>foederati</em> were – they may have been the Franks in 358, which would make <a href="https://en.wikipedia.org/wiki/Julian_(emperor)" data-type="URL" data-id="https://en.wikipedia.org/wiki/Julian_(emperor)">Julian </a>(as a ‘Caesar’ or junior-emperor under Constantius) the culprit for this bad idea – he had a surplus of those too.)</p>



<p>The problem, of course, is right there: the status of the <em>foederati</em> made it impossible for them to ever fully integrate into the empire.  They had, after all, their own kings, their own local laws and served in their own military formations.  While, interestingly, they would eventually adopt Latin from the local population which had already done so (leading to French, Spanish and Italian) they could never <em>become Roman</em>.  That wasn’t always their choice, either!  <strong>As O’Donnell<em> </em>(<em>op. cit.</em>) notes, many of these <em>foederati</em> wanted to be ‘in’ in the Roman Empire; it was more frequently <em>the Romans</em> who were busy saying ‘no.</strong>‘  It is striking that this occurs in a period where social class in the Roman world was <em>generally</em> calcifying.  Whereas citizenship had been an expanding category, after the <em>Constitutio Antoniniana</em>, the legal categories of <em>honestiores </em>and <em>humiliores</em> (lit. ‘respectable’ and ‘humble’ people, but in practice, ‘wealthy’ and ‘commoners’) largely replaced citizenship as the legal dividing lines of Roman society.  These were far less flexible categories, as economic social mobility in the ancient world was never very high.  Even there, the tax reforms of Diocletian (with some ‘patches’ under Constantine) began, for tax purposes, to tie tenant farmers (‘<em>coloni</em>‘) to their land, essentially barring both physical and economic mobility in the name of more efficient tax collection in a system that strongly resembled later medieval serfdom.</p>



<p><strong>Nevertheless, the consequence of this system of organization was that as often as the <em>foederati</em> provided crucial soldiers to Roman armies, they were just as frequently the problem Roman armies were being sent to address</strong>.  Never fully incorporated into the Roman army and under the command of their own kings, they proved deeply unreliable allies.  Pitting one set of <em>foederati </em>against the next could work in the short-term, but in the long term, without any plan to permanently incorporate the <em>foederati</em> into Roman society, fragmentation was inevitable.  <strong>The Roman abandonment of the successful older systems for managing diverse armies </strong>(on account that they were too expensive) <strong>turned the <em>foederati </em>from a potential source of vital manpower into the central cause of imperial collapse in the West.</strong></p>



<h2>Losing the Habit</h2>



<p><strong>This trend towards calcification had been matched by the loss of Rome’s (admittedly opportunistic and unevenly applied) religious tolerance</strong>.  This is often attributed to Christianity itself, but is perhaps better understood in light of the increasing demands of emperors during and after the Crisis of the Third Century to insist on unity through uniformity.  <strong>The first empire-wide systemic persecution of Christians, the <a href="https://en.wikipedia.org/wiki/Decian_persecution" data-type="URL" data-id="https://en.wikipedia.org/wiki/Decian_persecution">Decian Persecution</a> (250 AD) was exactly this – an effort to have all Romans everywhere sacrifice for the safety of the emperor as an act of unity</strong> to strengthen his reign which rather backfired because it seems not to have occurred to Decius that Christians (of whom, by 250, there were many) would be unable to participate.  <strong>Diocletian likewise launched the Great Persecution in 303 as part of a program to stress unity in worship and try to bind the fractured Roman Empire together</strong>, particularly by emphasizing the cults of Jupiter and Hercules.  From that perspective, Christians were a threat to the enforced, homogeneous unity Diocletian wanted to foster and thus had to be brought back or removed, though of course in the event Christianity’s roots were by 303 far too deep for it to be uprooted.</p>



<p>That is part of the context where we should understand Constantine (r. 306-337).  Constantine is famous for declaring the toleration of Christianity in the empire and being the first emperor to convert to Christianity (only on on his death-bed).  What is less well known is that, having selected Christianity as his favored religion,<strong> Constantine – seeking <em>unity</em> again – promptly set out to unify his new favored religion, by force if necessary</strong>.  A schism had arose as a consequence of Diocletian’s persecution and – now that Christianity was in the good graces of the emperor – both sides sought Constantine’s aid in suppressing the other in what became known as the <a href="https://en.wikipedia.org/wiki/Donatism" data-type="URL" data-id="https://en.wikipedia.org/wiki/Donatism">Donatism controversy</a>, as the side which was eventually branded heretical supported a Christian bishop named Donatus.  Constantine, after failing to get the two groups to agree settled on persecuting one of them (the Donatists) out of existence (which didn’t work either).</p>



<p>It is in that context that later Christian emperor’s efforts to unify the empire behind Christianity (leading to the<a href="https://en.wikipedia.org/wiki/Edict_of_Thessalonica" data-type="URL" data-id="https://en.wikipedia.org/wiki/Edict_of_Thessalonica"> Edict of Thessalonica </a>in 380) ought to be understood – <strong>as the culmination of, by that point, more than a century of on-again, off-again efforts by emperors to try to strengthen the empire by enforcing religious unity</strong>.  By the end of the fourth century, the Christian empire was persecuting pagans and Jews, not even a full century after it had been persecuting Christians.</p>



<p><strong>These efforts to violently enforce unity through homogeneity had the exact opposite effect. </strong> Efforts to persecute Arian Christians (who rejected the Nicene Creed) created further divisions in the empire; they also made it even more difficult to incorporate the newly arriving Germanic peoples, who had mostly converted to the ‘wrong’ (Arian) Christianity.  Meanwhile, in the fifth century, the church in the East splintered further, leading to the ‘Nestorian’ (the term is contested) churches of Syria and the Coptic Church in Egypt on the ‘outs’ with the official (Eastern) Roman Church and thus also facing persecution after the Council of Ephesus in 431.  The resentment created by the policy of persecution in the East seems to have played a fairly significant role in limiting the amount of local popular resistance faced by the Muslim armies of the Rashidun Caliphate during the conquests of Syria, the Levant and Egypt in the 630s, since in many cases Christian communities viewed as ‘heretical’ by Constantinople could actually expect potentially better treatment under Muslim rule.  Needless to say, this both made the Muslim conquests of those regions easier but also go some distance to explaining why Roman/Byzantine reconquest was such a non-starter.  <strong>Efforts to enforce unity in the empire had, perhaps paradoxically, made it more fragile rather than more resilient.</strong></p>



<h2>Conclusions</h2>



<p>Reaching back to our initial question, we now have some answers.  <strong>While the Romans are often presented in popular culture as a homogeneous society consisting of white Northern Europeans who uniformly speak their Latin with a decided British accent, the truth is both more complex and more interesting</strong>.  At every stage of their development, the Romans were a diverse bunch.  Initially this reflected Rome’s origins as a frontier town, positioned at the meeting point of several different cultural, linguistic and religious groups.  The Romans themselves understood their own deep past to have been diverse, with Rome drawn together out of peoples from all over Italy and while their founding myths aren’t particularly <em>accurate</em>, on this point the archaeology backs up the fundamental truth that earliest Rome was a fusion-society.  That diversity only grew as Rome expanded, first incorporating the different peoples of Italy and then the different peoples of the Mediterranean more broadly.</p>



<p>Contrary to the popular image, Rome was a diverse society, by any definition, from the foundation of the Republic (if not earlier) to the collapse of the empire in the West.  This was true when it came to language, culture, and religion.  It was also true when it came to skin-color, although this tended to be quite a bit less important to the Romans than it is to many people today.  <strong>While popular media tends to portray the Romans as uniformly white, typically using British actors, the <em>actual</em> Romans stretched close to the full range of human skin-colors</strong>.  There was no particular arrangement of skin color, hair color or type , no particular ‘<a href="https://en.wikipedia.org/wiki/Phenotype" data-type="URL" data-id="https://en.wikipedia.org/wiki/Phenotype">phenotype</a>‘ which was distinctively Roman.  There were fair-skinned Romans, dark-skinned Romans and Romans at every point in between.  That diversity of appearance was true within Roman Italy but became radically <em>more</em> true in Rome’s Mediterranean-spanning empire.  Instead, Roman identity was marked by a legal status, citizenship, which defined belonging in the group; visually, Roman citizenship was conveyed by clothing (the <em>toga</em> in particular), rather than skin, hair, facial structure, etc.  <strong>The ‘Queen’s Latin’ vision of a Rome homogeneously white is wrong in all respects</strong>.</p>



<p>Consequently the theory of empire that is often built up on that cracked foundation, which supposes that the strength of a homogeneous Roman society forged their empire and that the diversity of that empire doomed it is fundamentally flawed, based on an amateurish and catastrophically mistaken understanding of Roman history.  Rome was <em>never</em> homogeneous and so there is no pristine, unmixed Roman society to hearken back to.  Again, <em>the Romans themselves knew this</em> and declared it openly.</p>



<p><strong>Instead it was Rome’s opportunistic but well-honed ability to effectively manage diverse populations which led to their success</strong>.  Doubtless this skill had at least some of its origins in the fact that Roman political leaders <em>did not</em> come from a monoculture; the relatively ‘hands off’ strategy (with a strong emphasis on local self-government) Rome had to take to deal with the diversity of Italy provided an effective training ground for managing a vast empire.  The Romans do not seem to have ever intended to forge a vast, multi-cultural polity out of their imperial conquests; they were, for the most part, in it for the loot, taxes and military glory.  But that Italian toolkit and the habits of politics in Rome’s diverse Republic were things every Roman aristocrat brought with them when they went abroad as a general or a governor (coincidentally, the same job in the Republic and in the Empire, but that’s a story for another day).  Indeed, <strong>diversity was the root of Roman success.  What made the Roman Empire possible, both to create and to hold, was the willingness of the Romans to opportunistically <em>include</em> conquered peoples in the Roman project</strong>.</p>



<p>In the end though, when Rome’s conquests ceased, and with the frontiers defended against migrations of fresh peoples into the empire, the Romans clearly got out of the habits that had won them the empire.  Emperors more strongly emphasized unity through conformity, particularly in religion, and when the Romans once again had need of non-Romans in their armies, they made the crucial mistake of keeping them separate, rather than incorporating them as they had before.  <strong>Diversity did not destroy the Roman Empire, but <em>intolerance</em> of diversity contributed greatly to its fall</strong>.</p>



<p>Of course part of the reason we are discussing all of this is that these examples – often in their deeply flawed, ‘Queen’s Latin’ form – are mobilized as support for political ends.  I am not a policy expert, so I am not going to suggest what particular policy these conclusions lend themselves too.  But I <em>am</em> a Roman history expert and so <strong>when policy makers are thinking about the examples of history to try to craft effective policies, they should consider the <em>actual Romans</em> rather than the image of Rome crafted by the BBC and Hollywood</strong>.  Those Romans were not neatly homogeneous, they did not neatly map on to modern racial categories, but nor were they enlightened multiculturalists.  They were frequently shrewd and self-interested empire builders who recognized that it actually is true that diversity is strength.  </p>



<p>That Rome, the <em>real</em> Rome, the one that exists in our literary sources, in the archaeological record, in Roman artwork (and in my classroom), <strong>the real Rome was always diverse</strong>.</p>



<p>That’s why they won.</p>
	</div></div>]]></content:encoded>
      <guid>https://acoup.blog/2021/07/30/collections-the-queens-latin-or-who-were-the-romans-part-v-saving-and-losing-an-empire/</guid>
      <pubDate>Mon, 02 Aug 2021 16:41:11 +0000</pubDate>
      <source>https://acoup.blog/2021/07/30/collections-the-queens-latin-or-who-were-the-romans-part-v-saving-and-losing-an-empire/</source>
    </item>
    <item>
      <title>Leaked Document Says Google Fired Dozens of Employees for Data Misuse</title>
      <link>https://www.vice.com/en/article/g5gk73/google-fired-dozens-for-data-misuse</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><div><picture><source media="(min-width: 1000px)" srcset="https://video-images.vice.com/articles/610a8fcf2bd14700971031b8/lede/1628082129648-google.jpeg?crop=1xw:0.842xh;0xw,0.046xh&amp;resize=20:*"/><source media="(min-width: 700px)" srcset="https://video-images.vice.com/articles/610a8fcf2bd14700971031b8/lede/1628082129648-google.jpeg?crop=1xw:0.842xh;0xw,0.046xh&amp;resize=20:*"/><source media="(min-width: 0px)" srcset="https://video-images.vice.com/articles/610a8fcf2bd14700971031b8/lede/1628082129648-google.jpeg?crop=1xw:0.842xh;0xw,0.046xh&amp;resize=20:*"/></picture></div><p>Image: Olly Curtis/Future via Getty Images</p></div><div data-component="BodyComponentRenderer"><div><p><a href="https://www.vice.com/en/topic/cyber"></a></p><p>Hacking. Disinformation. Surveillance. CYBER is Motherboard&#39;s podcast and reporting on the dark underbelly of the internet.</p></div><p><span data-component="TextBlock"><p>Google has fired dozens of employees between 2018 and 2020 for abusing their access to the company&#39;s tools or data, with some workers potentially facing allegations of accessing Google user or employee data, according to an internal Google document obtained by Motherboard.</p></span><span data-component="TextBlock"><p>The document provides concrete figures on an often delicate part of a tech giant&#39;s operations: investigations into how company&#39;s own employees leverage their position inside the company to steal, leak, or abuse data they may have access to. Insider abuse is a problem across the tech industry. Motherboard previously <a href="https://www.vice.com/en/article/bjp9zv/facebook-employees-look-at-user-data"><span>uncovered instances at Facebook</span></a>, <a href="https://www.vice.com/en/article/xwnva7/snapchat-employees-abused-data-access-spy-on-users-snaplion"><span>Snapchat</span></a>, <a href="http://vice.com/en/article/j5w4xx/myspace-employees-spied-on-users-with-internal-tool-overlord"><span>and MySpace</span></a>, with employees in some cases using their access to stalk or otherwise spy on users.</p></span><span></span><span data-component="TextBlock"><p>The document says that Google terminated 36 employees in 2020 for security related issues. Eighty-six percent of all security-related allegations against employees included mishandling of confidential information, such as the transfer of internal-only information to outside parties.</p></span><span data-component="TextBlock"><p>10 percent of all allegations in 2020 concerned misuse of systems, which can include accessing user or employee data in violation of Google&#39;s own policies, helping others to access that data, or modifying or deleting user or employee data, according to the document. In 2019, that figure was 13 percent of all security allegations.</p></span></p><blockquote data-component="QuoteBlock"><p><em><strong>Do you know about any company employees leveraging their access to user data? We&#39;d love to hear from you. Using a non-work phone or computer, you can contact Joseph Cox securely on Signal on +44 20 8133 5190, Wickr on josephcox, or email joseph.cox@vice.com.</strong></em></p></blockquote><p><span data-component="TextBlock"><p>Google terminated 26 people in 2019 and 18 in 2018 related to security incidents, the person who provided the document told Motherboard. Motherboard granted the person anonymity to speak more candidly about Google issues. The document says that other measures Google can take with employees that mishandled data can include warnings, training, and coaching.</p></span><span data-component="TextBlock"><p>A Google spokesperson told Motherboard in a statement that &#34;The instances referred to mostly relate to inappropriate access to, or misuse of, proprietary and sensitive corporate information or IP.&#34;</p></span><span data-component="TextBlock"><p>&#34;Regarding user data, we tightly restrict employee access through a number of industry leading safeguards, including: limiting access to user data to necessary individuals, requiring a justification to access such data, multi-stage review before access is granted to sensitive data, and monitoring for access anomalies and violations,&#34; the statement added. &#34;The number of violations, whether deliberate or inadvertent, is consistently low. Every employee gets training annually, we investigate all allegations, and violations result in corrective action up to and including termination. We are transparent in publicizing the number and outcome of our investigations to our employees and have strict processes in place to secure customer and user data from any internal or external threats.&#34;</p></span><span data-component="TextBlock"><p>In 2010, Google fired engineer David Barksdale for leveraging his position as a member of a technical group to access the accounts of four minors, <em><a href="https://www.gawker.com/5637234/gcreep-google-engineer-stalked-teens-spied-on-chats" target="_blank"><span>Gawker </span></a></em><a href="https://www.gawker.com/5637234/gcreep-google-engineer-stalked-teens-spied-on-chats" target="_blank"><span>reported at the time</span></a>. Barksdale accessed a 15 year-old boy&#39;s Google Voice call logs, as well as contact lists and chat transcripts and unblocked himself from a teen who had cut communications with him, the report added.</p></span><span data-component="TextBlock"><p><em><strong>Subscribe to our cybersecurity podcast, <a href="https://itunes.apple.com/gb/podcast/cyber/id1441708044?mt=2" target="_blank">CYBER</a>.</strong></em></p></span></p></div><div><div><div><p><h3>ORIGINAL REPORTING ON EVERYTHING THAT MATTERS IN YOUR INBOX.</h3></p><p>By signing up to the VICE newsletter you agree to receive electronic communications from VICE that may sometimes include advertisements or sponsored content.</p></div></div></div></div>]]></content:encoded>
      <guid>https://www.vice.com/en/article/g5gk73/google-fired-dozens-for-data-misuse</guid>
      <pubDate>Wed, 04 Aug 2021 13:25:45 +0000</pubDate>
      <source>https://www.vice.com/en/article/g5gk73/google-fired-dozens-for-data-misuse</source>
    </item>
    <item>
      <title>Machine Learning with Julia on AWS SageMaker</title>
      <link>https://beta.datachef.co/blog/machine-learning-with-julia-on-aws-sagemaker/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___beta_datachef_co_blog_machine-learning-with-julia-on-aws-sagemaker_/image.jpg" /> 
<div id="readability-page-1" class="page"><article>
    <header>
      
      
      <h2>
        June 16, 2021
        </h2>
      
      <figure>
        
      </figure>
      
      <hr/>
    </header>
    <section id="post-body">
      <h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#why-you-should-run-your-julia-program-on-sagemaker">Why you should run your Julia program on SageMaker?</a></li>
<li><a href="#julia-on-aws">Julia on AWS</a>
<ul>
<li><a href="#how-to-use-julia-in-sagemaker-notebook-instances">How to use Julia in SageMaker Notebook Instances</a></li>
<li><a href="#automation-with-cloudformation">Automation with CloudFormation</a></li>
</ul>
</li>
<li><a href="#hands-on-data-science-with-julia">Hands-on Data Science with Julia</a>
<ul>
<li><a href="#iris-dataset">Iris Dataset</a></li>
<li><a href="#monitor-what-percentage-of-provisioned-resources-have-been-utilized-with-cloudwatch">Monitor what percentage of provisioned resources have been utilized with CloudWatch</a></li>
</ul>
</li>
<li><a href="#differential-equations-in-julia">Differential Equations in Julia</a></li>
<li><a href="#julia-aws-sdk">Julia AWS SDK</a></li>
</ul>


<p>Amazon <a href="https://aws.amazon.com/sagemaker/">SageMaker</a> is a fully managed service that provides every developer and data scientist with the ability to quickly build, train, and deploy machine learning (ML) models without worrying about the infrastructure. <a href="https://julialang.org/">Julia</a> is a high-level, high-performance, dynamic programming language. While it is a general-purpose language and can be used to design various applications, many of its features are well suited for numerical analysis and computational science. By running Julia on SageMaker, you will be able to get the most out of this programming language as you can easily access high-performance instances.</p>


<h2 id="how-to-use-julia-in-sagemaker-notebook-instances">How to use Julia in SageMaker Notebook Instances</h2>
<p>An Amazon SageMaker <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html">notebook instance</a> is a machine learning (ML) compute instance running the Jupyter Notebook App. SageMaker manages provisioning-related resources. You can use Jupyter notebooks in your notebook instance to prepare and process data, write code to train/validate models, and deploy them as SageMaker endpoints. You can create multiple notebooks within your notebook instance.</p>
<p>To create a notebook instance, you should go to Notebook instance in the AWS SageMaker console, and click on the Create Notebook instance button:</p>
<figure>
     
</figure>

</section>
  </article></div>]]></content:encoded>
      <guid>https://beta.datachef.co/blog/machine-learning-with-julia-on-aws-sagemaker/</guid>
      <pubDate>Wed, 04 Aug 2021 09:46:16 +0000</pubDate>
      <source>https://beta.datachef.co/blog/machine-learning-with-julia-on-aws-sagemaker/</source>
    </item>
    <item>
      <title>Are dynamic languages going to replace static languages? (2003)</title>
      <link>https://www.artima.com/weblogs/viewpost.jsp?thread=4639</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div width="100%">
<tbody><tr>
<td>
<div>
<p><a href="https://www.artima.com/weblogs/index.jsp">Artima Weblogs</a> | 

<a href="https://www.artima.com/weblogs/index.jsp?blogger=unclebob">Robert C. Martin&#39;s Weblog</a> | 

<a href="https://www.artima.com/forums/flat.jsp?forum=106&amp;thread=4639">Discuss</a> | 
<a href="mailto:?subject=Are Dynamic Languages Going to Replace Static Languages?&amp;body= %0AArtima Weblogs %0AAre Dynamic Languages Going to Replace Static Languages? %0Aby Robert C. Martin %0A%0Ahttps://www.artima.com/weblogs/viewpost.jsp?thread=4639">Email</a> | 
<a href="https://www.artima.com/weblogs/viewpostP.jsp?thread=4639">Print</a> | 
<a href="https://www.artima.com/weblogs/bloggers.jsp">Bloggers</a> | 
Previous | 
<a href="https://www.artima.com/weblogs/viewpost.jsp?thread=4738" title="One per Pixel.">Next</a>
</p></div>
</td>
</tr>
</tbody></div></div>]]></content:encoded>
      <guid>https://www.artima.com/weblogs/viewpost.jsp?thread=4639</guid>
      <pubDate>Wed, 04 Aug 2021 03:36:30 +0000</pubDate>
      <source>https://www.artima.com/weblogs/viewpost.jsp?thread=4639</source>
    </item>
    <item>
      <title>Law school applicants surge 13%, biggest increase since dot-com bubble</title>
      <link>https://www.reuters.com/legal/legalindustry/law-school-applicants-surge-13-biggest-increase-since-dot-com-bubble-2021-08-03/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_reuters_com_legal_legalindustry_law-school-applicants-surge-13-biggest-increase-since-dot-com-bubble-2021-08-03_/image.jpg" /> 
<div id="readability-page-1" class="page"><p id="primary-image-caption" data-testid="primary-image-caption">Signage is seen outside of the Brooklyn Law School in Brooklyn, New York City, U.S., September 14, 2020. REUTERS/Andrew Kelly</p><div><div><div><ul><li>Law schools struggled to manage a crush of applicants with high LSAT scores</li><li>Some schools will start the year with larger-than-expected 1L classes</li></ul></div></div><p data-testid="paragraph-0">(Reuters) - This was the year everyone wanted to go to law school.</p><p data-testid="paragraph-1">The number of people applying for admission to law school this fall surged nearly 13%, making it the largest year-over-year percentage increase since 2002, according to the latest data from the Law School Admission Council.</p><p data-testid="paragraph-2">And they were an impressive bunch. The number of people applying with LSAT scores in the highest band of 175 to 180 more than doubled from 732 last year to 1,487 this year.</p><p data-testid="paragraph-3">In total, 71,048 people applied to American Bar Association-accredited law schools this cycle, up from 62,964 at this point in 2020. That’s still significantly lower than the historic high of 100,601 applicants in 2004, but it’s by far the largest national applicant pool of the past decade.</p><p data-testid="paragraph-4">“This was the cycle that surprised everyone,” said Mike Spivey of Spivey Consulting, whose firm assists clients in the law school admissions process. “In some cycles, applicants are surprised. In some cycles, law schools are surprised. But no one was able to anticipate the incredible spike of high LSAT scores.”</p><p data-testid="paragraph-5">Many law schools struggled to manage the unexpected number of high-scoring applicants and will start the upcoming school year with larger 1L classes than they planned for, Spivey added.</p><p data-testid="paragraph-6">Experts attribute the crush of applications to a number of factors, particularly the slowdown in the entry-level job market caused by the COVID-19 pandemic. Law school and other graduate programs historically become more popular when jobs are tougher to come by in slow economies. Law school applicants shot up nearly 18% in 2002, amid the bursting of the so-called dot-com bubble. The number of people applying also climbed nearly 4% in 2009, amid the Great Recession.</p><p data-testid="paragraph-7">But current events separate from the economy also prompted more people to consider a law degree this cycle, said Susan Krinsky, the council’s executive vice president for operations. The death of George Floyd, the national reckoning over systemic racism and inequality, and the death of iconic U.S. Supreme Court Justice Ruth Bader Ginsburg all focused attention on the rule of law and the role lawyers play in pushing for a more equitable society. Election years also tend to yield more law school applicants, she noted.</p><p data-testid="paragraph-8">“Just seeing what was going on in the world and all the disparities — so much was happening where a legal education can really change things for somebody,” Krinsky said.</p><p data-testid="paragraph-9">The switch from the in-person LSAT to the shorter, online LSAT-Flex in June of 2020 helps explain some of the increase in applicants with high LSAT scores, Spivey noted. The Law School Admission Council has said the two versions are comparable and that internal data show aspiring lawyers had more time to study for the admissions test during the pandemic, yielding higher scores. LSAT tutors have also noted that, at minimum, a shorter test taken in the comfort of one’s own home is bound to be less stressful and more manageable than a longer exam taken at a testing center.</p><p data-testid="paragraph-10">The council has said the LSAT will remain online through June 2022, but the final LSAT-Flex was given in June and is being replaced this month by a remote exam with four sections instead of three, which is still one section less than the traditional in-person LSAT.</p><p data-testid="paragraph-11">Spivey predicts the application pool for next fall will remain robust, due in part to people deferring this year at schools with over-enrolled 1L classes, and those who got shut out in this competitive year trying again.</p><p data-testid="paragraph-12">“You’re going to see a competitive cycle early on,” he said. “And I think schools are going to go incredibly slowly in admit decision-making. They got burned this year.”</p><p data-testid="paragraph-13">Read more:</p><p data-testid="paragraph-14"><a href="https://www.reuters.com/article/lawyer-lsat-scores/lsat-maker-says-it-lost-about-140-online-test-takers-scores-idUSL2N2F12WE" target="_blank">LSAT maker says it lost about 140 online test takers&#39; scores</a></p><p data-testid="paragraph-15"><a href="https://www.reuters.com/article/lawyer-school-trump-bump/the-trump-bump-lawyers-are-coming-they-could-change-the-legal-profession-idUSL1N2I92U6" target="_blank">The &#39;Trump bump&#39; lawyers are coming. They could change the legal profession.</a></p><p data-testid="paragraph-16"><a href="https://www.reuters.com/legal/government/most-law-students-remote-classes-didnt-make-grade-report-2021-06-16" target="_blank">For most law students, remote classes didn&#39;t make the grade - report</a></p></div></div>]]></content:encoded>
      <guid>https://www.reuters.com/legal/legalindustry/law-school-applicants-surge-13-biggest-increase-since-dot-com-bubble-2021-08-03/</guid>
      <pubDate>Wed, 04 Aug 2021 00:23:32 +0000</pubDate>
      <source>https://www.reuters.com/legal/legalindustry/law-school-applicants-surge-13-biggest-increase-since-dot-com-bubble-2021-08-03/</source>
    </item>
    <item>
      <title>Galois Groups and the Symmetries of Polynomials</title>
      <link>https://www.quantamagazine.org/how-galois-groups-used-polynomial-symmetries-to-reshape-math-20210803/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_quantamagazine_org_how-galois-groups-used-polynomial-symmetries-to-reshape-math-20210803_/image.jpg" /> 
<div id="readability-page-1" class="page"><div data-reactid="206"><section data-reactid="207"><section data-reactid="208"><div data-reactid="209"><div data-reactid="211"><div data-reactid="212"><h6 data-reactid="213"><a href="https://www.quantamagazine.org/tag/group-theory/" data-reactid="214">group theory</a></h6><!-- react-empty: 216 --><div data-reactid="224"><p>By focusing on relationships between solutions to polynomial equations, rather than the exact solutions themselves, Évariste Galois changed the course of modern mathematics.</p></div></div></div></div></section></section></div><div data-reactid="260"><div data-reactid="261"><div data-reactid="262"><div data-reactid="263"><section data-reactid="319"><div data-reactid="320"><div data-reactid="321"><div data-reactid="322"><p>Before being mortally wounded in a duel at age 20, Évariste Galois discovered the hidden structure of polynomial equations. By studying the relationships between their solutions — rather than the solutions on their own — he created new concepts that have since become an essential part of many branches of mathematics.</p>
<p>No one knows why Galois found himself on a Paris dueling ground early in the morning of May 30, 1832, but the night before, legend has it that he stayed up late finishing his last manuscripts. There he wrote:</p>
<blockquote><p><small>Go to the roots of these calculations! Group the operations. Classify them </small><small>according to their complexities rather than their appearances! This, I believe, is the mission of future mathematicians. This is the road on which I am embarking in this work.</small></p></blockquote>
<p>Galois’ edict emerged from a mathematical predicament. In the 1500s, mathematicians had studied polynomials like <em>x</em><sup>2 </sup>− 2 and <em>x</em><sup>4 </sup>− 10<em>x</em><sup>2 </sup>+ 22. They had tried to find simple formulas that would allow them to calculate the roots of those polynomials — the values of <em>x</em> that make the equation equal zero — but could only find them when the highest exponent was no greater than 4.</p>

<p>Beyond that, Galois himself proved that no such formulas exist. So he devised a new way of studying roots: Instead of calculating them exactly, he realized he could study the algebraic relationships between them — focusing on their complexities, rather than their appearances.</p>
<p>In spirit, his perspective was similar to considering a shape’s different symmetries. These are the various ways of reorienting the shape so that it still looks the same, such as rotating a square by 180 degrees. Symmetries between a polynomial’s roots are ways of swapping them so that they maintain the same algebraic relationship.</p>
<p>And just as some shapes have more symmetries than others (a circle has infinitely many; a square has just eight), you can rearrange the roots of some polynomial equations more freely than you can rearrange the roots of others.</p>
<p>“Some ways of rearranging the roots can be incompatible with the rules of algebra. In this sense, the roots might not be entirely interchangeable with one another,” said <a href="http://math.stanford.edu/~conrad/">Brian Conrad</a> of Stanford University.</p>
<p>The extent to which roots can be swapped with each other while maintaining algebraic consistency is a subtle property that tells mathematicians a lot about how to recognize features of polynomials that can’t be seen just by looking at them. It’s easiest to see with examples. Let’s take a look at two, each of which has three roots (since the highest exponent of each is 3):</p>
<p><em>f</em>(<em>x</em>) = <em>x</em><sup>3</sup> − 7<em>x</em> + 5</p>
<p><em>g</em>(<em>x</em>) = <em>x</em><sup>3</sup> − 7<em>x</em> + 7</p>
<p>On paper, they’re nearly identical. But behind the scenes, the roots of one can be rearranged in more ways than the roots of the other.</p>
<p>Let’s focus on <em>f</em>(<em>x</em>) first. Here, we have three roots: <em>a</em>, <em>b</em> and <em>c</em>. We can combine them algebraically to make a new value by taking the product of pairs of roots and adding them together. For all cubic polynomials — those with 3 as their highest exponent — with a coefficient of 1 for the cubed term, it’s known that this particular algebraic expression made from the roots always equals the coefficient of the linear term, or the term that is raised to the first power. In our example, this is −7.</p>
<p>We get this algebraic equation:</p>
<p><em>ab</em> + <em>ac</em> + <em>bc</em> = −7.</p>
<p>Now let’s rearrange the roots, leaving <em>c</em> alone, but switching <em>a</em> and <em>b</em>. We get:</p>
<p><em>ba</em> + <em>bc</em> + <em>ac</em> = −7.</p>
<p>Rearranging the roots in this way preserves the algebraic relationship between them: The equation is still true because multiplication and addition are commutative, meaning that swapping the order of things — like shuffling the roots around — doesn’t change the answer. In fact, for this example, all six possible ways of rearranging the roots (including the one where they don’t change) preserves the relationship:</p>
<p><strong><em>a, b, c: ab</em></strong><strong> + <em>ac</em> + <em>bc</em> = </strong><strong>−</strong><strong>7</strong></p>
<p>Now let’s look at the second polynomial,<em> g</em>(<em>x</em>) = <em>x</em><sup>3</sup> − 7<em>x</em> + 7. If we call the roots <em>r</em>, <em>s</em> and <em>t</em>, then an analogous equation to the one for <em>f</em>(<em>x</em>) holds as well:</p>
<p><em>rs</em> + <em>rt</em> + <em>st</em> = −7.</p>
<p>This will be true for any cubic polynomial whose initial term is <em>x</em><sup>3</sup> and whose linear term is −7<em>x</em>. And again, all six of the possible arrangements still equal −7. But curiously, for <em>g</em>(<em>x</em>), not all of them are considered symmetries of the polynomial.</p>
<p>This is because the algebraic relationships among its roots are more complex: There is an additional special algebraic relationship that its roots satisfy. The special relationship is (<em>r</em> − <em>t</em>)(<em>r</em> − <em>s</em>)(<em>t</em> − <em>s</em>) = 7 (when you assume <em>r </em>is less than <em>s</em>, and <em>s</em> is less than <em>t</em>). Only three of the six possible rearrangements of its roots preserve both algebraic relationships: <em>rs</em> + <em>rt</em> + <em>st</em> = 7 and (<em>r</em> − <em>t</em>)(<em>r</em> − <em>s</em>)(<em>t</em> − <em>s</em>) = 7:</p>
<p><strong><em>r</em>, <em>s</em>, <em>t: </em>(<em>r </em></strong><strong>− <em>t</em>)(<em>r</em> </strong><strong>− <em>s</em>)(<em>t</em> </strong><strong>− <em>s</em>) = 7<br/>
</strong><em>  s</em>, <em>r</em>, <em>t: </em>(<em>s </em>− <em>t</em>)(<em>s </em>− <em>r</em>)(<em>t </em>− <em>r</em>) = −7</p>
<p>The three rearrangements in bold preserve all algebraic relationships among the roots, even beyond these two. Consequently, these three rearrangements are considered to be the symmetries of the polynomial.</p>
<p>It’s not obvious at a glance that the two polynomials have different levels of complexity, but it becomes visible when you adopt the perspective Galois invented.</p>

<p>Galois packaged his way of thinking in new objects — which came to be called Galois groups — that encode the complexity of the algebraic relationships between the roots of a given polynomial. Within these relationships, rearrangements of roots can be applied one after another, but they can be undone to get back to where you started — just as you can apply the symmetries of a square and then undo them to get back to the exact position you began with.</p>
<p>This idea reflects the general concept of a group in mathematics, which is a collection of symmetries, whether they apply to a square or the roots of a polynomial. Galois groups were the first instances of the concept of a group, and Galois’ ideas blossomed into what today is a powerful, ubiquitous area of research called group theory.</p>
<p>Galois groups provide a powerful perspective from which to study polynomial equations. If you know the Galois group of a polynomial, then the behavior of its roots can be understood by accessing many of the tools of group theory. The insights you’ll gain through this approach are far more illuminating than the ones you can get by performing algebra on the polynomial itself.</p>
<p>“[With Galois groups] you get this one piece of information, and it spreads and tells you so much more,” said <a href="https://www2.math.upenn.edu/~harbater/">David Harbater</a> of the University of Pennsylvania.</p>

<p>For instance, the Galois group immediately tells you whether a polynomial can be solved at all, and it allows you to compare the underlying structure of different polynomials. Galois groups can also be used to study various mathematical objects in algebra and number theory in ways that open up solutions to problems that aren’t otherwise available.</p>
<p>“Turning a question about polynomials into a question about groups opens up the door to many other mathematical operations and techniques that cannot be readily described in the original language of polynomials,” said Conrad.</p>
<p>This expansiveness has allowed Galois groups to play a central role in many of the most celebrated mathematical projects over the last century or so. They featured in Gerd Faltings’ 1983 proof of the Mordell conjecture and Andrew Wiles’ 1994 proof of Fermat’s Last Theorem.</p>

<p>Galois groups are also at the heart of some of the most exciting ongoing work in mathematics today. As <em>Quanta</em> explained in a <a href="https://www.quantamagazine.org/with-a-new-shape-mathematicians-link-geometry-and-numbers-20210719/">recent feature story</a>, they are the linchpin of the sprawling Langlands program, which turns a question about polynomials into a more sophisticated and revealing question about the relationship between Galois groups and another special class of groups.</p>
<p>Though Évariste Galois’ life was cut short, his greatest achievement will continue to advance mathematics for centuries to come — though it’s hard to predict exactly how.</p>
<p>“[Galois groups] just have a way of appearing in surprising places,” said <a href="https://people.math.wisc.edu/~jose/">Jose Rodriguez</a> of the University of Wisconsin, Madison.</p>
</div></div></div></section></div></div></div></div></div>]]></content:encoded>
      <guid>https://www.quantamagazine.org/how-galois-groups-used-polynomial-symmetries-to-reshape-math-20210803/</guid>
      <pubDate>Wed, 04 Aug 2021 08:50:51 +0000</pubDate>
      <source>https://www.quantamagazine.org/how-galois-groups-used-polynomial-symmetries-to-reshape-math-20210803/</source>
    </item>
    <item>
      <title>My small revenge on Apple</title>
      <link>https://javierantonsblog.blogspot.com/2021/08/my-small-revenge-on-apple.html</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___javierantonsblog_blogspot_com_2021_08_my-small-revenge-on-apple_html/image.jpg" /> 
<div id="readability-page-1" class="page"><div>

<main id="main" role="main" tabindex="-1">
<div id="page_body" name="Page body">
<div data-version="2" id="Blog1">
<div>
<article>
<div>
<div>

<h3>
My small revenge on Apple
</h3>


<div id="post-body-3952147506922650364">
<p>Once upon a time, <a href="https://www.groupsapp.online/">Groups</a> was an app that accepted two &#34;social logins&#34;: Google and Facebook. All was well.</p><p>Apple, a company known for enclosing its customers in gardens full of roses and big walls, decided this was a threat to their business model.</p><p>How could developers have their prisoner&#39;s name and email addresses? What would be next, sending personalized emails offering alternative payment methods? This simply could not be</p><p>But now Apple had to convince their fans that their &#34;Apple ID&#34; was better than the one they were used to. And so they used their two favorite methods: <b>hype </b>and <b>extortion</b></p><p><u><b>Hype</b></u></p><p><a href="https://lh3.googleusercontent.com/-P6FaRmVyC-A/YQpEaMKKKnI/AAAAAAAAEZ8/DZPpOGnKODAnbvDm1MLNND9IdUQDmtaWACLcBGAsYHQ/image.png"></a></p></div>

</div>
</div>


</article>
</div>
</div><div data-version="2" id="PopularPosts1">
<h3>
Popular posts from this blog
</h3>
<div>
<div role="feed">
<article role="article">
<h3><a href="https://javierantonsblog.blogspot.com/2021/07/my-app-just-got-removed-with-no-prior.html">My app just got removed with no prior warning</a></h3>

<div>
<p><a href="https://javierantonsblog.blogspot.com/2021/07/my-app-just-got-removed-with-no-prior.html">

</a>
</p>
<div>
<p>
 My app  just got de-listed from Google Play with no prior warning. The reason: My app correctly offered a privacy policy link in its Google Play listing ( https://www.groupsapp.online/privacy-policy ) and it also showed a link within the app&#39;s login menu: Furthermore, a link to the website (where the privacy policy can also be found) was offered in the help menu: App: Website: So the only case where the privacy policy could not be found (in the app), was when a user had already been offered to see the privacy policy and had proceeded to log in I agree that it should always be accessible (which it is via the &#34;website&#34; button) Anyway, I proceeded to upload a version that includes a permanent link to the privacy policy, but then was presented with the below (forgot to take a screenshot of this one): It can take up to 7 days to process your request These are my conclusions:     1) Unexpectedly pulling an app with no warning  and then imposing a 7-day review period is not rea
</p>
</div>

</div>
</article>
</div>
</div>
</div></div>
</main>
</div></div>]]></content:encoded>
      <guid>https://javierantonsblog.blogspot.com/2021/08/my-small-revenge-on-apple.html</guid>
      <pubDate>Wed, 04 Aug 2021 08:41:53 +0000</pubDate>
      <source>https://javierantonsblog.blogspot.com/2021/08/my-small-revenge-on-apple.html</source>
    </item>
    <item>
      <title>Eight Sleep (YC S15) Is Hiring a Data Scientist</title>
      <link>https://www.ycombinator.com/companies/eight-sleep/jobs/slZD7gD-full-stack-engineer-web-focus</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><section><div><div><a href="https://www.ycombinator.com/companies/eight-sleep"><p></p></a></div><div><div><div><p><strong>Experience</strong></p><p><span>Any (new grads ok)</span></p></div></div><div><p>Apply to Eight Sleep and hundreds of other fast-growing YC startups with a single profile.</p><p><a onclick="return applyNowClick(event)" href="https://account.ycombinator.com/authenticate?continue=https%3A%2F%2Fwww.workatastartup.com%2Fapplication%3Fapply_to_job_id%3D45501&amp;defaults%5BsignUpActive%5D=true&amp;defaults%5Bwaas_company%5D=964">Apply to role ›</a></p></div></div><h2><strong>About the role</strong></h2><div><div><p>Eight Sleep is the first sleep fitness company — we design products at the forefront of sleep innovation. Our mission is to make people’s sleep count for more, using innovative technology, detailed design, and proven science and data to personalize and improve each night for everybody—changing the way people sleep forever and for better. Backed by leading Silicon Valley investors including Khosla Ventures and Y Combinator, it was named by Fast Company in 2018 as one of the Most Innovative Companies in Consumer Electronics.</p>
<p>We recently launched our latest temperature-regulated smart bed, the Pod Pro. It&#39;s an absolute game changer, improving people&#39;s health and happiness by changing the way they sleep. Early feedback is great, but we still have a long way to go toward achieving our mission.</p>
<p>That is why Eight Sleep is looking for a talented Frontend Engineer to join our expanding Growth team, with responsibilities spanning the consumer website, marketing and analytics systems. If you are a growth-oriented engineer and enjoy finding creative ways to solve business problems using your unique skill set, this is the right opportunity for you.</p>
<p><strong>How you&#39;ll contribute</strong></p>
<ul>
<li>Work with an interdisciplinary team on complex business problems to drive growth</li>
<li>Analyze and understand user behavior in order to optimize key performance metrics and enable a more personalized experience across all marketing channels.</li>
<li>Help design and implement A/B tests to improve conversion, optimize site flow, and drive revenue</li>
<li>Integrate data from various back-end services and databases to help build a richer picture of user behavior and help us spot areas for improvement.</li>
<li>Collaborate with designers to develop polished, intuitive, and well optimized experiences.</li>
<li>Identify and implement tools and systems to help scale our growing business.</li>
</ul>
<p><strong>About you</strong></p>
<ul>
<li>You have the desire to make a significant impact as part of a rapidly growing, dynamic startup.</li>
<li>You consider yourself to be a generalist engineer - willing to roll up your sleeves and use whatever language or framework will best help you get the job done.</li>
<li>You should feel just as comfortable diving into the backend when you are working on a feature as you do polishing the UI.</li>
<li>You often write small scripts that help automate parts of your work and are constantly finding new tools that make you more efficient.</li>
<li>You have a BA/BS in Computer Science, or commensurate experience from a coding bootcamp or similar.</li>
<li>You love thinking about things from the customer’s perspective and trying to understand what motivates and drives decision making.</li>
<li>You have side projects outside of work that you’re excited to tell us about.</li>
</ul>
<p><strong>What you&#39;ll need to succeed</strong></p>
<ul>
<li>2+ years of experience developing frontend applications in a production environment at a startup using modern JS frameworks such as React</li>
<li>2+ years of experience using Typescript</li>
<li>Solid understanding of how web applications work including performance, security, session management, accessibility, and best development practices.</li>
<li>Familiarity with some of the following technologies: React, Next.JS, AWS Lambdas + S3, Node.js, Typescript.</li>
<li>Experience with web analytics tools (e.g., Google Analytics, Amplitude) and Search Engine Optimization</li>
<li>Knowledge of relational database systems, Object Oriented Programming and various scripting languages.</li>
<li>Intuitive understanding of how to manipulate, analyze and interpret highly complex data</li>
<li>Knowledge of design tools (Figma, Sketch, Photoshop and Shopify) is a plus.</li>
</ul>
<p><strong>Why you’ll love Eight Sleep</strong></p>
<ul>
<li>We’re a tight-knit, passionate team that’s working to improve people’s lives by improving the way they sleep</li>
<li>Leadership is committed to employees’ wellness and career development</li>
<li>You’ll get a better night sleep every night; all full-time employees receive the Pod</li>
<li>Flexible, generous PTO</li>
<li>100% employer contribution for medical/dental/vision insurance</li>
</ul>
</div></div><h2><strong>Why you should join Eight Sleep</strong></h2><div><div><p>Eight Sleep is the first sleep fitness company. At Eight Sleep we design products at the forefront of sleep innovation. Our mission is to power human potential through better sleep, using innovative technology, detailed design, and proven science and data to personalize and improve each night for everybody—changing the way people sleep forever and for better. Backed by leading Silicon Valley investors including Khosla Ventures and Y Combinator, it was named by Fast Company in 2018 as one of the Most Innovative Companies in Consumer Electronics.</p>
<p>We&#39;ve just launched our temperature-regulated smart bed, the Pod. It&#39;s an absolute game changer, improving people&#39;s health and happiness by changing the way they sleep. Early feedback is great, but we still have a long way to go toward achieving our mission.</p>
</div></div></div></section></div></div>]]></content:encoded>
      <guid>https://www.ycombinator.com/companies/eight-sleep/jobs/slZD7gD-full-stack-engineer-web-focus</guid>
      <pubDate>Wed, 04 Aug 2021 12:00:39 +0000</pubDate>
      <source>https://www.ycombinator.com/companies/eight-sleep/jobs/slZD7gD-full-stack-engineer-web-focus</source>
    </item>
    <item>
      <title>Where are the robotic bricklayers?</title>
      <link>https://constructionphysics.substack.com/p/where-are-the-robotic-bricklayers</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___constructionphysics_substack_com_p_where-are-the-robotic-bricklayers/image.jpg" /> 
<div id="readability-page-1" class="page"><div><p>When researching construction, you invariably discover that any new or innovative idea has actually been tried over and over again, often stretching back decades. One of these new-but-actually-old ideas is the idea of a mechanical bricklayer, a machine to automate the construction of masonry walls.</p><p>It’s easy to see the appeal of this idea - masonry construction seems almost perfectly suited for mechanization [0]. It’s extremely repetitive - constructing a masonry building requires setting tens or hundreds of thousands of bricks or blocks, each one (nearly) identical, each one set in the same way. It doesn’t seem like it would require physically complex movements - each brick gets a layer of mortar applied, and is simply laid in place next to the previous one. And because each brick and mortar joint is the same size, placement is almost deterministic - each brick is the same fixed distance from the previous one.</p><p>On top of this, masonry, especially block masonry, is one of the most physically punishing construction tasks, since it requires hours and hours of repetitively moving extremely heavy objects. All together masonry seems like the perfect candidate for a task to hand over to a machine, and it’s something people have been attempting for over 100 years.</p><h3>Early Attempts</h3><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F24266b51-506d-47bd-8141-d30144d4984d_609x393.png"></a><figcaption>John Thompson’s 1904 bricklaying machine</figcaption></figure></div><p>The first attempts at machine-laid masonry date back to the turn of the century - we can find patents issued for mechanical bricklayers in <a href="https://books.google.com/books?id=o8JJAQAAMAAJ&amp;pg=PA82&amp;dq=brick+laying+machine&amp;hl=en&amp;newbks=1&amp;newbks_redir=0&amp;sa=X&amp;ved=2ahUKEwj94fKjq_3xAhVIrJ4KHVkRB944ChDoATAAegQIBhAC#v=onepage&amp;q=brick%20laying%20machine&amp;f=false">1899</a>, <a href="https://patentimages.storage.googleapis.com/ac/13/fa/b5f479fda30195/US772191.pdf">1904</a>, and <a href="https://books.google.com/books?id=NT_nAAAAMAAJ&amp;pg=PA169&amp;dq=brick+laying+machine&amp;hl=en&amp;newbks=1&amp;newbks_redir=0&amp;sa=X&amp;ved=2ahUKEwj474rBqv3xAhVOj54KHYetAQ0Q6AEwBnoECAsQAg#v=onepage&amp;q=brick%20laying%20machine&amp;f=false">1924</a>, all by different individuals (some of whom worked on their idea for years - John Thompson, who was was awarded the 1904 patent, had additional patents in 1918 and 1926). These machines would (theoretically) run along the top of a wall, set down a layer of mortar, and place bricks one at a time. These machines couldn’t sense anything about their environment, or measure where a brick needed to go - they simply extruded a layer of mortar and mechanically placed a brick at regular intervals. It’s unclear how many of these machines ever made it beyond the drawing board, but at least one of these (John Knight’s) was used to build a brick wall that <a href="https://www.britainbycar.co.uk/farnham/489-john-henry-knight">allegedly still stands today</a>.</p><p>This same concept would reappear several times over the next few decades - patents can be found for similar machines issued in the 60s and 70s. This video from 1967 of the “Motor Mason &#39;&#39; shows one in action - it’s not all that different from the early 1900s efforts:</p><p id="youtube2-4MWald1Goqk" data-attrs="{&#34;videoId&#34;:&#34;4MWald1Goqk&#34;,&#34;startTime&#34;:null,&#34;endTime&#34;:null}"><iframe src="https://www.youtube-nocookie.com/embed/4MWald1Goqk?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p><p>These attempts at mechanical bricklayers never made it far beyond the demonstration stage, and never found any sort of commercial success.</p><h3>The 1980s: Bricklaying Robots</h3><p>Beginning in the late 1980s and early 90s, we start to see attempts at mechanizing masonry based around robotic arms. Unlike the previous machines, which were purely mechanical, these machines had an information processing component. Instead of mindlessly repeating the same motions, these arms would combine a high degree-of freedom robotic arm with sensors and control systems to “see” the brick, see where it needed to go, and deliberately grab it and place it there.</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F079e277f-0ee9-41ee-9768-12ccfafd6fce_755x474.png"></a><figcaption>Schematic of the ROCCO system, via <a href="https://www.iaarc.org/publications/proceedings_of_the_11th_isarc/first_results_of_the_development_of_the_masonry_robot_system_rocco_a_fault_tolerant_assembly_tool.html">IAARC</a></figcaption></figure></div><p>For a while this was an entire sub-field of academic research - we can find examples in <a href="https://www.sciencedirect.com/science/article/abs/pii/0921889088900206">Slocum 1988</a>, <a href="https://cris.vtt.fi/en/publications/outlines-of-two-masonry-robot-systems">Lehiten 1989</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/0926580595000097">Rihani 1996</a>, <a href="https://ascelibrary.org/doi/abs/10.1061/(ASCE)0893-1321(1993)6:1(19)">Altobelli 1993</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/0926580595000151">Pritschow 1996</a>, the <a href="http://www.iaarc.org/publications/proceedings_of_the_5th_isarc/a_robotized_wall_erection_system_with_solid_components.html">SMAS in Japan</a>, and the <a href="https://www.iaarc.org/publications/proceedings_of_the_11th_isarc/first_results_of_the_development_of_the_masonry_robot_system_rocco_a_fault_tolerant_assembly_tool.html">ROCCO system</a>. But despite all the effort expended, these attempts saw about the same level of success as the previous, purely mechanical masonry machines. Most didn’t get past the level of technical descriptions (“here’s how you COULD build a masonry robot”), a few reached the level of prototypes, but essentially no progress was made beyond that. At least one researcher gave up, declaring that general purpose construction robots would be infeasible for the foreseeable future. The single example I can find of a system that <em>might</em> have been used in production is Multistone 8000, a German system that could, with human assistance, automatically assemble masonry wall panels in a factory-like environment.</p><h3>Current Attempts</h3><p>Over the years, masonry has declined in importance as a construction technology in the developed world, and with it the interest in automating it. Unlike with concrete 3D printing, where there are dozens of attempts to develop the technology, I can only find a handful of current efforts to automate masonry.</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3975fb2b-12f1-407a-b215-a96cfa777141_719x436.png"></a><figcaption>Hadrian X, via <a href="https://www.fbr.com.au/">FBR</a></figcaption></figure></div><p>The most advanced of these seems to be the <a href="https://www.fbr.com.au/">Hadrian system</a> by Fastbrick Robotics. Hadrian uses a hollow boom mounted to a truck, through which masonry blocks are transported (sort of like a concrete pump truck). When the block reaches the end of the boom, it’s sprayed with an industrial adhesive (instead of conventional mortar) and grabbed by a robot arm, which places it into the correct position.</p><p>The reach of this boom, combined with it’s vehicle mounting, means that Hadrian has a lot fewer restrictions as to where it can place blocks than other mechanized masonry systems. It’s capable of placing blocks in tight hallways or complex corners, and Hadrian can construct all the walls of a small building with just a few moves of the vehicle. Hadrian can currently set around 200 blocks an hour, but they’re aiming to be able to do 1000 blocks an hour or more (the blocks it sets are different from the block masonry used in the US, but in US masons can set somewhere in the neighborhood of 400 blocks a day).</p><p>Hadrian has been in development since 2006, and has only recently started to be used on commercial jobsites - they’ve built the block walls for 3 or 4 buildings in Australia so far. It seems like they’ve had some struggles as a company (not surprising for a hardware system that’s been in development for over 15 years), and they had what appears to be a fairly serious round of layoffs in 2020. But over the last few months there’s been a slow trickle of projects built using the system.</p><p>But the most commercially successful mechanized masonry system I’m aware of is the <a href="https://www.construction-robotics.com/sam-2/">SAM</a>, semi autonomous mason. SAM is a masonry robot built by Construction Robotics, and has been in use on commercial projects since 2015. Unlike Hadrian, which lays block, SAM lays normal clay bricks - it consists of a robot arm, mortar dispenser and conveyor belt, mounted to a wheeled chassis. The robot arm grabs a brick, applies a layer of mortar to it, and places it into position on the wall, based on an internal “brick map” indicating the location of every brick to be set. Once the brick is in place, it repeats the process, traversing back and forth as it builds up the wall (human masons are required to set bricks at the ends of the wall). SAM has a series of sensors to compensate for the movement of the platform and ensure it’s placing bricks level, and can work with bricks of all different sizes (though it won’t build CMU block walls). It gets mounted to a movable scaffolding that’s raised gradually as the wall is completed.</p><p>Like Hadrian, it also seems like SAM has had some struggles. It can perform well on very long stretches of wall, but on short walls it has trouble outperforming human masons. It can’t turn corners, and it can’t finish the masonry joints. At its best it seems to lay brick around 5 times faster than a human can, but it requires masons to follow behind it to clean the joints and occasionally level the bricks, and an on-site technician to handle technical issues. The <a href="https://www.amazon.com/dp/B07THDGSPG?psc=1&amp;ref=ppx_pop_dt_b_asin_title">book written about the development of SAM</a> documents one challenge after another, and ends with them struggling to find buyers for it. The promotional material on Construction Robotics’ website suggests that it’s no longer their main focus - more emphasis is given to their other product, MULE.</p><p>Other than SAM and Hadrian, there are a few other mechanical bricklayers in various stages of development. Craftsmac, a company out of India, just recently announced a <a href="https://www.youtube.com/watch?v=6WFZmu4rs8E">robotic mason</a> used for building CMU walls - it appears somewhat similar to SAM, a wheeled chassis with a robot arm, conveyor, and mortar mixer mounted to it. This <a href="https://www.youtube.com/watch?v=uIwmtyvoFmw&amp;t=58s">UK system</a> for robotically laying bricks uses a track mounted system gantry system, which both lets it turn corners and eliminates the problem of how to move the robot from floor to floor (trading it for the time and expense of having to assemble the system upfront). <a href="https://rob-technologies.com/robotic-brickwork#brickwork-use-cases-section">ROB</a> uses an off-the-shelf robot arm to build highly variable masonry panels (though it doesn’t seem to be able to handle mortar). You also see the occasional academic effort. Overall the list of efforts in this space is pretty short.</p><p>One area where we do see some commercial success with mechanical bricklaying is with brick roads - a variety of companies offer machines that can “print” a section of road made of bricks. As we’ve <a href="https://constructionphysics.substack.com/p/why-did-agriculture-mechanize-and">seen previously</a>, road construction is slightly easier to mechanize than building construction.</p><h3>Masonry Assistants</h3><p>A slightly different category of machines aimed at improving masonry productivity is what I’ll call “masonry assistants”. These are machines designed to assist with physically lifting the block (they seem to be more common with block than brick) and taking the strain off the mason, while still allowing the mason to manipulate it into position</p><p>Masonry assistants date from at least 1994, when the military experimented with <a href="https://apps.dtic.mil/sti/citations/ADA302081">MAMA</a>, the Mechatronically Assisted Mason’s Aid. It consisted of a gripper attached to a track mounted boom arm. The mason could use the gripper to move the block into place without having to physically lift it. Since then several companies have developed variations of the boom mounted gripper idea - the Layher Balance, Rimatem, and Assistance System Steinherr were a few I can find records of. The only system like this I’m aware of currently for sale is the MULE, sold by Construction Robotics (they seem to have had much more success with it than with SAM).</p><p>But the most interesting of these masonry assistant systems might be the <a href="https://www.masonrymagazine.com/blog/2020/06/01/exoskeletons-for-bricklayers-science-fiction-is-now-reality/">exoskeleton in development by FRACO,</a> just released last year. It was adapted from a model developed for the military, and has various passive and active lifting mechanisms designed to reduce the strain on the mason’s muscles when manipulating blocks.</p><p>Of course, a machine to aid the moving of heavy objects isn’t exactly revolutionary - a commenter on the agricultural productivity article pointed out that one of the most important advancements in masonry productivity was the telescopic handler, which eliminated having to manually move pallets of block into place on the jobsite.</p><h3>Why aren’t mechanical masons already here?</h3><p>Masonry seemed like the perfect candidate for mechanization, but a hundred years of limited success suggests there’s some aspect to it that prevents a machine from easily doing it. This makes it an interesting case study, as it helps define <em>exactly</em> where mechanization becomes difficult - what makes laying a brick so different than, say, hammering a nail, such that the latter is almost completely mechanized and the former is almost completely manual?</p><p>There seems to be a few factors at work. One is the fact that a brick or block isn’t simply set down on a solid surface, but is set on top of a thin layer of mortar, which is a mixture of water, sand, and cementitious material. Mortar has sort of complex physical properties - it’s a non-newtonian fluid, and it’s viscosity increases when it’s moved or shaken. This makes it difficult to apply in a purely mechanical, deterministic way (and also probably makes it difficult for masons to explain what they’re doing - watching them place it you can see lots of complex little motions, and the mortar behaving in sort of strange not-quite-liquid but not-quite-solid ways). And since mortar is a jobsite-mixed material, there will be variation in it’s properties from batch to batch.</p><p>Masonry machines have constantly struggled with the mortar aspect of masonry; many of them simply ignored the aspect of the problem. The academic studies of the late 80s and early 90s were often based on using mortarless walls, wall systems that don’t require mortar joints (such as surface bonded masonry), or mortar alternatives that behaved a little more predictably (which is what Hadrian ended up using). In his 1996 paper, Pritschow comes right out and says that trying to solve the problem of handling mortar is too difficult. The folks that have figured out how to reliably apply mortar still can’t quite manage to produce a clean mortar joint - they simply slather the mortar on there, requiring workers to follow behind and clean it up. In some ways efforts like SAM don’t look all that different from the 50-year-old Motor Mason in this regard.</p><p>Mortar joints make setting blocks more complicated. Whereas a nailgun can apply force to a nail and get a fairly uniform result every time (and if it can’t, it’s not critical - a nail will still function if its driven slightly askew), setting a block on a layer of field-mixed non-newtonian fluid isn’t so forgiving. Without some feedback from the environment (measuring the levelness of the block that’s been set), it’s hard to be sure that the wall is being built level. Human masons are constantly checking the levelness of their blocks with strings or field levels to ensure the wall remains true as it gets built, and making slight adjustments as needed; a mechanical mason needs some way to do the same. SAM seems to have MOSTLY solved this problem, but every so often still needs the workers following behind to tap the blocks level.</p><p>This is one of the main things that separates driving a nail from setting a block - the necessity of making adjustments based on feedback from the environment. Things like nailguns, circular saws, and other power tools are in some sense more like the masonry assistants - they perform some purely physical task, while leaving all the information processing and precise placement work in the hands of humans. A nailgun isn’t responsible for figuring out where a nail needs to go, and moving itself into position - it simply does the physical task of striking the nail.</p><p>The history of routers and milling machines offers an instructive parallel. The first ones were developed in the late 1800s/early 1900s, and the capability for programmatically controlling them was developed in the late 40s/early 50s. But it’s only recently where we’ve had the capability of incorporating real-time feedback, enabling products like the <a href="https://www.shapertools.com/en-us/">Shaper Origin</a> (a handheld router that autocorrects human movements ). Robustly getting a machine to react based on its surrounding environment remains a complex problem, even if the machine is <em>physically</em> capable of doing it.</p><p>There’s also a few other ancillary problems making masonry machines tricky: </p><ul><li><p>Bricks and blocks are large and heavy, which means larger, more expensive machinery is needed to manipulate them (especially if you want to manipulate them <em>quickly</em>, as force scales with acceleration). </p></li><li><p>In the US, masonry walls are constructed with a large amount of reinforcement, which would be difficult to handle for a simple block-based machine (on Hadrian’s projects, the reinforcement was installed manually)</p></li><li><p>For <a href="https://constructionphysics.substack.com/p/why-its-hard-to-innovate-in-construction">risk-aversion</a> reasons, it’s hard to get contractors on board with your technology [1]. A <a href="https://www.youtube.com/watch?v=2msCMjuqeiY">surprisingly candid video</a> from Construction Robotics goes into detail about how difficult it was to convince a particular client to use their system, and the book about the company details many more difficult sells.</p></li></ul><h3>Should we expect to see mechanized masonry get more popular?</h3><p>This is a tough question to answer. Fundamentally, it seems like it depends on what progress in robotics, software, computer vision, and other technologies (which I’ll lump together as “automation”) looks like.</p><p>For all it’s difficulties, masonry has been more successfully automated than other building systems - it’s one of the few systems where commercial robots for building it are actually on the market for general use. So progress in automation technology might plausibly benefit masonry construction most of all, as it’s arguably the farthest along. If automated masonry systems got smaller, faster, could more easily handle corners and finish joints, it might start to look like a very attractive system.</p><p>However, it could just as easily be a rising tide lifts all boats scenario - advancing automation technology could easily be applied to other building systems (wood, steel, etc.) as well. It becomes a question of a question of what the relative improvement curves look like - what gets better the fastest, how long does that stay true?</p><p>Blocks are quite heavy, which means the machines to manipulate them will likely always be more expensive than the machines for manipulating a lighter building system. And even large blocks individually make up a small fraction of the overall building. So automated masonry systems might end up being more expensive and have lower production rates than other building types. We could see a situation where there’s a small window of time where automated masonry becomes extremely popular/competitive, but then is quickly overshadowed as the technology gets adapted for other building systems.</p><p>[0] - There are, roughly, two different kinds of masonry. Brick masonry, which is made from clay bricks, and concrete masonry (CMU, or concrete masonry units), which is made from hollow concrete blocks. To a first approximation, in the US brick masonry is purely decorative and doesn’t support the weight of the building. Block masonry is used as the load bearing structure.</p><p>[1] - The SAM book details quite a few instances of cascading failures caused by process changes. On one job, SAM is unable to set bricks because the wall is unbraced, and the bricklaying platform is much heavier than the ones the firm was used to using, and the wall has insufficient bracing force. The entire project is slowed down. On another project, SAM is slowed down by the masons unexpectedly changing the size of the bricks, which forces them to recreate the brick-map used to program the machine.</p><p><em>Feel free to contact me!</em></p><p><em>email: <a href="mailto:briancpotter@gmail.com">briancpotter@gmail.com</a></em></p><p><em>LinkedIn: <a href="https://email.mg1.substack.com/c/eJxd0kuPsjwYBuBfM-wwbaUcFizEz8OIioyC4oaUUrDKyVIU_fUvzuy-pGmbPE97p-lFiWR5LV52U7dS-UyxfDXMrtizLZiUTChdy0TMU3uMoQmBqaQ2MBA1EoW3cSYYKwkvbKXpkoJTInldfZoh0AA2lYvNkGWlOLEAIJmBNENDSWpkWqYhwlJM9L9M0qWcVZTZ7MHEq66YUtgXKZv2azz5QvNh_MaMyhyO2i5pJaG3Ea3LoUA_xdWzuM0W3mU68X-C-GBsmyDff3fHTV-Kcubvmw7855-f1zXNEmf32kd3nj1gdSCzoim2j81Pil7h1bue9vm3bgVTfkRncL-msAF3ZxLQvtGTR7dSD2-Ps8s1dsVZ3lQh9URbuLi8OJHeh5jQw2m3MRs0PIfNqvSws8ojX0RFM9u-GRYp8-_H7Q7-eOUml11zckOp8pW5jPVx5X3j0mLz7bvOFv3NlGroFHupz3BPHuE53ufeOvIJeIt38XTm8_lkN1WffpJuppdggnPvuQxo6MvZiaqF4_ZiFwptvb3eMirmaJcWT7xWPXlN6lPUhpbxCDRgwkf3dmDs3SL3HFau3qxg1J0KFUXPtRPcw7aXW7DnfeF7E989BcsMcxc64N66csyN4wksk8XSTc97lSP1sRl357iPkAd8hdsIIDhIgRCBYTeCIwIwgQkxDR2CMYMZNsc4Y9BKgKXpOra-NPD__1WEnQhOKtrUH4tDR_7r4FMaxMXDWnYVl6-YVSQpWGpL0TFF_nH-NRPnrGJiYJ7GRNpQR7qpmyY2Bol_9j60LQABGmNlyE7r4VRl07pqh7voB3RzebWctv8AnbARWw">https://www.linkedin.com/in/brian-potter-6a082150/</a></em></p></div></div>]]></content:encoded>
      <guid>https://constructionphysics.substack.com/p/where-are-the-robotic-bricklayers</guid>
      <pubDate>Tue, 03 Aug 2021 21:43:38 +0000</pubDate>
      <source>https://constructionphysics.substack.com/p/where-are-the-robotic-bricklayers</source>
    </item>
    <item>
      <title>Where Do Models Go Wrong? Parameter-Space Saliency Maps for Explainability</title>
      <link>https://arxiv.org/abs/2108.01335</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___arxiv_org_abs_2108_01335/image.jpg" /> 
<div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
    <p><a href="https://arxiv.org/pdf/2108.01335">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  Conventional saliency maps highlight input features to which neural network
predictions are highly sensitive. We take a different approach to saliency, in
which we identify and analyze the network parameters, rather than inputs, which
are responsible for erroneous decisions. We find that samples which cause
similar parameters to malfunction are semantically similar. We also show that
pruning the most salient parameters for a wrongly classified sample often
improves model behavior. Furthermore, fine-tuning a small number of the most
salient parameters on a single sample results in error correction on other
samples that are misclassified for similar reasons. Based on our parameter
saliency method, we also introduce an input-space saliency technique that
reveals how image features cause specific network components to malfunction.
Further, we rigorously validate the meaningfulness of our saliency maps on both
the dataset and case-study levels.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Roman Levin [<a href="https://arxiv.org/show-email/9a5a977d/2108.01335">view email</a>]
      </p></div></div>]]></content:encoded>
      <guid>https://arxiv.org/abs/2108.01335</guid>
      <pubDate>Wed, 04 Aug 2021 02:25:49 +0000</pubDate>
      <source>https://arxiv.org/abs/2108.01335</source>
    </item>
    <item>
      <title>GAN-generated facial images that are capable of impersonating multiple IDs</title>
      <link>https://www.unite.ai/master-faces-that-can-bypass-over-40-of-facial-id-authentication-systems/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_unite_ai_master-faces-that-can-bypass-over-40-of-facial-id-authentication-systems_/image.jpg" /> 
<div id="readability-page-1" class="page"><div id="mvp-content-main"><p>Researchers from Israel have developed a neural network capable of generating ‘master’ faces – facial images that are each capable of impersonating multiple IDs. The work suggests that it’s possible to generate such ‘master keys’ for more than 40% of the population using only 9 faces synthesized by the StyleGAN Generative Adversarial Network (GAN), via three leading face recognition systems.</p><p>The <a href="https://arxiv.org/pdf/2108.01077.pdf">paper</a> is a collaboration between the Blavatnik School of Computer Science and the school of Electrical Engineering, both at Tel Aviv.</p><p>Testing the system, the researchers found that a single generated face could unlock 20% of all identities in the University of Massachusetts’ Labeled Faces in the Wild (<a href="http://vis-www.cs.umass.edu/lfw/">LFW</a>) open source database, a common repository used for development and testing of facial ID systems, and the benchmark database for the Israeli system.</p><div id="attachment_176962"><p></p><p id="caption-attachment-176962"><em>The Israeli system workflow, which uses the StyleGAN generator to iteratively seek out ‘master faces’.</em> Source: https://arxiv.org/pdf/2108.01077.pdf</p></div><p>The new method improves on a similar <a href="https://www.unite.ai/a-universal-facial-id-master-key-through-machine-learning/">recent paper</a> from the University of Siena, which requires a privileged level of access to the <a href="https://www.unite.ai/what-is-machine-learning/">machine learning</a> framework. By contrast, the new method infers generalized features from publicly available material and uses it to create facial characteristics that straddle a vast number of identities.</p><h3><strong>Evolving Master Faces</strong></h3><p><a href="https://arxiv.org/pdf/1812.04948.pdf">StyleGAN</a> is initially used in this approach under a black box optimization method focusing (unsurprisingly) on high dimensional data, since it’s important to find the broadest and most generalized facial features that will satisfy an authentication system.</p><p>This process is then repeated iteratively to encompass identities that were not encoded in the initial pass. In varying test conditions, the researchers found that it was possible to obtain authentication for 40-60% with only nine generated images.</p><div id="attachment_176963"><p><a href="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-M8dyDKvP/w:auto/h:auto/q:auto/https://www.unite.ai/wp-content/uploads/2021/08/master-faces.jpg"></a></p><p id="caption-attachment-176963"><em>Successive groups of ‘master faces’ obtained in the research across various Coverage Search methods, including LM-MA-ES. The Mean Set Coverage (MSC, a metric for accuracy) is noted under each image.</em></p></div><p>The system uses an evolutionary algorithm coupled with a neural predictor that estimates the likelihood of the current ‘candidate’ to generalize better than the p-percentile of candidates generated in previous passes.</p><div id="attachment_176964"><p></p><p id="caption-attachment-176964"><em>The filtering of generated candidates in the architecture of the Israeli system.</em></p></div><h3><strong>LM-MA-ES</strong></h3><p>The project uses the Limited-Memory Matrix Adaptation (<a href="https://arxiv.org/pdf/1705.06693.pdf">LM-MA-ES</a>) algorithm developed for a 2017 initiative led by the Research Group on Machine Learning for Automated Algorithm Design, an approach that’s well-suited for high-dimensional black box optimization.</p><p>The LM-MA-ES outputs candidates randomly. Though this is well-suited to the intent of the project, an additional component is needed to deduce which faces are the best candidates for cross-identity authentication. Therefore the researchers created a ‘Success Predictor’ neural classifier to sieve the flood of candidates into the best-fit faces for the task.</p><div id="attachment_176965"><p></p><p id="caption-attachment-176965"><em>Rationale of the Success Predictor used in the Israeli facial identification spoofing project.</em></p></div><h3><strong>Evaluation</strong></h3><p>The system was tested against three CNN-based face descriptors: <a href="https://arxiv.org/pdf/1704.08063.pdf">SphereFace</a>, <a href="https://arxiv.org/pdf/1503.03832.pdf">FaceNet</a> and <a href="https://www.scalr.ai/post/facial-detection-and-recognition-with-dlib">Dlib</a>, each system architecture containing a similarity metric and a loss function, which are useful in validating the system’s accuracy scores.</p><p>Success Predictor is a <a href="https://www.unite.ai/what-are-neural-networks/">feed-forward neural network</a> comprising three fully-connected layers. The first of these uses <a href="https://openreview.net/forum?id=d-XzF81Wg1">BatchNorm regularization</a> to ensure consistency of data prior to activation. The network uses <a href="https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218">ADAM</a> as the optimizer, with an ambitious learning rate of 0.001 over batches of 32 input images.</p><div id="attachment_176966"><p></p><p id="caption-attachment-176966"><em>Output from the three architectures.</em></p></div><p>All three algorithms tested were trained for 26,400 fitness function calls using the same set of five seeds.</p><p>The researchers had established by this point that longer training processes did not benefit the system; effectively, the Israeli approach is seeking to derive key data from an early stage of model training, where only the highest features have yet been discerned. It’s worth noting that this is something of a gift, in terms of framework economy.</p><p>Having established baseline results with Facebook’s Python-based <a href="https://github.com/facebookresearch/nevergrad">NeverGrad</a> gradient-free optimization environment, the system was profiled against a number of  algorithms, including various brands of the <a href="https://www.metabolic-economics.de/pages/seminar_theoretische_biologie_2007/literatur/schaber/Storn1997JGlobOpt11.pdf">Differential Evolution</a> heuristic.</p><p>The researchers found that a ‘greedy’ approach based on Dlib outperformed its competitors, succeeding in creating nine master faces capable of unlocking 42%-64% of the test dataset. Application of the system’s Success Predictor further improved these very favorable results.</p><p>The paper contends that ‘face based authentication is extremely vulnerable, even if there is no information on the target identity’, and the researchers consider their initiative a valid approach to a security incursion methodology for facial recognition systems.</p></div></div>]]></content:encoded>
      <guid>https://www.unite.ai/master-faces-that-can-bypass-over-40-of-facial-id-authentication-systems/</guid>
      <pubDate>Wed, 04 Aug 2021 09:20:36 +0000</pubDate>
      <source>https://www.unite.ai/master-faces-that-can-bypass-over-40-of-facial-id-authentication-systems/</source>
    </item>
    <item>
      <title>A Soviet Prisoner&#39;s View on What&#39;s Important</title>
      <link>https://butwhatfor.substack.com/p/speech-at-the-stadium</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___butwhatfor_substack_com_p_speech-at-the-stadium/image.jpg" /> 
<div id="readability-page-1" class="page"><div><p><em>Welcome to all our new subscribers! I write a weekly newsletter, sent out every Sunday, about anything, as long as it’s interesting. If you enjoy the newsletter, please share it with a few friends/colleagues.</em></p><p>Last week, I shared a few excerpts of <a href="https://www.butwhatfor.com/charlie-munger/">Charlie Munger</a>’s <a href="https://www.butwhatfor.com/charlie-munger-the-psychology-of-human-misjudgment/">Psychology of Human Misjudgment</a> as someone had reached out saying that they thought it was a useful, albeit long, speech. That article got forwarded around a bit and resulted in one of the highest subscriber-per-view stats of any email I have sent out.</p><p>I think that means many people found it interesting.</p><p>So I thought I would do something similar again this week with another transcript that I posted on the <a href="https://www.butwhatfor.com/">main website</a> earlier this year — <a href="https://en.wikipedia.org/wiki/Joseph_Brodsky">Joseph Brodsky</a>’s Speech at the Stadium.</p><p>For those interested, <a href="https://www.butwhatfor.com/joseph-brodsky-speech-at-the-stadium/">the full speech transcript can be found here</a>. </p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1103534-c7b0-4901-b989-74f21cd6deef_1280x720.jpeg"></a></figure></div><h3>Who is Joseph Brodsky?</h3><p>Joseph Brodsky was a Russian poet and Noble Prize recipient.</p><p>He was born in Leningrad in 1940 and held numerous jobs following high school — working in a morgue, in a weapons factory, as a sailor, and on a geological exploration team in Asia. He started writing poetry at the age of 18, but only three years later, and while on that expedition far away from home, Brodsky decided to make poetry his full-time career and returned home to Leningrad.</p><p>Shortly thereafter he started becoming well-known in Russian literary circles, while at the same time being considered controversial. Well-known and controversial were an unfortunate combination in the Soviet Union at the time, and Brodsky found himself in and out of Soviet-run mental hospitals and prisons. </p><p>Fortunately, he was able to leave the Soviet Union in 1972 through the help of a few American poetry friends and eventually became a U.S. citizen in 1977. He went on to hold teaching positions at Columbia University and Mount Holyoke College and served as <a href="https://en.wikipedia.org/wiki/United_States_Poet_Laureate">Poet Laureate of the United States</a> from 1991 to 1992. Further, he won the Noble Prize in Literature in 1987 for “an all-embracing authorship, imbued with clarity of thought and poetic intensity.”</p><h3>Famous Poet, You Say?</h3><p>Title: <strong>1 January 1965</strong></p><p>Translated by <a href="https://en.wikipedia.org/wiki/George_Kline">George L. Kline</a></p><blockquote><p>The Wise Men will unlearn your name. </p><p>Above your head no star will flame. </p><p>One weary sound will be the same— </p><p>the hoarse roar of the gale. </p><p>The shadows fall from your tired eyes </p><p>as your lone bedside candle dies, </p><p>for here the calendar breeds nights</p><p>till stores of candles fail. </p><p>What prompts this melancholy key? </p><p>A long familiar melody. </p><p>It sounds again. So let it be. </p><p>Let it sound from this night. </p><p>Let it sound in my hour of death— </p><p>as gratefulness of eyes and lips</p><p>for that which sometimes makes us lift </p><p>our gaze to the far sky. </p><p>You glare in silence at the wall. </p><p>Your stocking gapes: no gifts at all. </p><p>It&#39;s clear that you are now too old </p><p>to trust in good Saint Nick; </p><p>that it&#39;s too late for miracles. </p><p>—But suddenly, lifting your eyes </p><p>to heaven&#39;s light, you realize: </p><p>your life is a sheer gift.</p></blockquote><h3>When Was the Speech Given?</h3><p>On December 18, 1988, Brodsky delivered this commencement address to 2,000 or so graduates of the University of Michigan. The speech ranges from practical advice on concrete skills that must be learned to more philosophical anecdotes on how to approach challenges you face. </p><p>For almost a decade following his address, the speech was unpublished publicly and more-or-less forgotten. This changed when it was published for the first time posthumously in <a href="https://amzn.to/2Vi9z3H">On Grief and Reason</a>, a collection of Brodsky’s later-life works. Originally titled “<a href="http://www.umich.edu/~bhlumrec/c/commence/1988-Brodsky.pdf">Some Tips</a>,” the speech is now often referred to as Brodsky’s “Speech at the Stadium,” the title given in that collection.</p><p>If you have made it this far, please take a moment to share the article with someone that might find it interesting — I appreciate your support.</p><p data-attrs="{&#34;url&#34;:&#34;https://newsletter.butwhatfor.com/p/speech-at-the-stadium?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;,&#34;class&#34;:null}"><a href="https://newsletter.butwhatfor.com/p/speech-at-the-stadium?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share"><span>Share</span></a></p><h3>Learn to be Articulate</h3><blockquote><p>Try to build and treat your vocabulary the way you are to treat your checking account. Pay every attention to it and try to increase your earnings. The purpose here is not to boost your bedroom eloquence or your professional success — although those, too, can be consequences — nor is it to turn you into parlor sophisticates. The purpose is to enable you to articulate yourselves as fully and precisely as possible; in a word, the purpose is your balance…</p><p>On a daily basis, a lot is happening to one’s psyche; the mode of one’s expression, however, often remains the same. Articulation lags behind experience… Sentiments, nuances, thoughts, perceptions that remain nameless, unable to be voiced and dissatisfied with approximations, get pent up within an individual and may lead to a psychological explosion or implosion.</p></blockquote><h3>Make What You Can Better</h3><blockquote><p>Try not to set too much store by politicians. Not so much because they are dumb or dishonest, which is more often than not the case, but because of the size of their job, which is too big even for the best among them, by this or that political party, doctrine, system or a blueprint thereof. All they or those can do, at best, is to diminish a social evil, not eradicate it. No matter how substantial an improvement may be, ethically speaking it will always be negligible, because there will always be those — say, just one person — who won’t profit from this improvement…</p><p>No matter how fairly the man you’ve elected will promise to cut the pie, it won’t grow in size; as a matter of fact, the portions are bound to get smaller. In light of that, or, rather, in dark of that — you ought to rely on your own home cooking, that is, on managing the world yourselves — at least that part of it that lies within your reach, within your radius.</p></blockquote><h3>Focus on Being You, Not What Someone Else Is</h3><blockquote><p>To this it should be added that the rich and famous these days, too, come in throngs, that up there on the top it’s very crowded. So if you want to get rich or famous or both, by all means go ahead, but don’t make a meal of it. To covet what somebody else has is to forfeit your uniqueness; on the other hand, of course, it stimulates mass production. </p><p>But as you are running through life only once, it is only sensible to try to avoid the most obvious cliches, limited editions included. The notion of exclusivity, mind you, also forfeits your uniqueness, not to mention that it shrinks your sense of reality to the already-achieved.</p><p>Far better than belonging to any club is to be jostled by the multitudes of those who, given their income and their appearance, represent — at least theoretically — unlimited potential. Try to be more like them than like those who are not like them.</p></blockquote><h3>Do Not View Yourself as a Victim</h3><blockquote><p>Of all the parts of your body, be most vigilant over your index finger, for it is blame-thirsty. A pointed finger is a victim’s logo — the opposite of the V-sign and a synonym for surrender. No matter how abominable your condition may be, try not to blame anything or anybody: history, the state, superiors, race, parents, the phase of the moon, childhood, toilet training, etc… The moment that you place blame somewhere, you undermine your resolve to change anything; it could be argued even that that blame-thirsty finger oscillates as wildly as it does because the resolve was never great enough in the first place.</p><p>After all, a victim status is not without its sweetness. It commands compassion, confers distinction, and whole nations and continents bask in the murk of mental discounts advertised as the victim’s conscience — but try to resist it. However abundant and irrefutable is the evidence that you are on the losing side, negate it as long as you have your wits about you, as long as your lips can utter “No…”</p><p>On the whole, try to respect life not only for its amenities but for its hardships, too. They are a part of the game, and what’s good about a hardship is that it is not a deception.</p></blockquote><h3>It’s OK to Let Go of People and Things</h3><blockquote><p>Try not to pay attention to those who will try to make life miserable for you. There will be a lot of those — in the official capacity as well as the self-appointed. Suffer them if you can’t escape them, but once you have steered clear of them, give them the shortest shrift possible. Above all, try to avoid telling stories about the unjust treatment you received at their hands; avoid it no matter how receptive your audience may be…</p><p>What your foes do derives its significance or consequence from the way you react. Therefore, rush through or past them as though they were yellow and not red lights. Don’t linger on them mentally or verbally; don’t pride yourself on forgiving or forgetting them — worse come to worse, do the forgetting first.</p></blockquote><p>If you made it all the way through the article, please take a moment to share it with someone that might find it interesting, or consider becoming a supporter of the newsletter by being a paying subscriber — I appreciate your support and interest in the newsletter very much.</p><p data-attrs="{&#34;url&#34;:&#34;https://newsletter.butwhatfor.com/p/speech-at-the-stadium?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;,&#34;class&#34;:null}"><a href="https://newsletter.butwhatfor.com/p/speech-at-the-stadium?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share"><span>Share</span></a></p><p>Take care and have a great rest of the week,</p><p>— EJ</p><h3>See <a href="https://newsletter.butwhatfor.com/">here</a> for an archive of recent newsletter articles</h3><p>A few selected examples…</p><h3><a href="https://newsletter.butwhatfor.com/p/an-attack-on-pearl-harbor">We Only Ever Talk About the Third Attack on Pearl Harbor</a></h3><p>“It was a winter Sunday morning and the island of Oahu was asleep — its military at Pearl Harbor less alert than they might have been any other day of the week.</p><p>The winter trade winds blow steadily from the northeast against the Hawaiian island, rushing along and then up and over the 3,000-foot Koolau Range, with the moisture they carry being wrung out along the way. That moisture often forms into towering clouds, creating a dark wall of rain and weather.”</p><h3><a href="https://newsletter.butwhatfor.com/p/planning-the-attack-on-pearl-harbor">Planning the Attack on Pearl Harbor</a></h3><p>“<a href="https://newsletter.butwhatfor.com/p/an-attack-on-pearl-harbor">The successful Japanese attack on Pearl Harbor</a> brought the United States into the war, but prevailing wisdom in Japan at the time said that the entrance was an inevitable eventuality. Many feared that a full-on attack by the United States had the potential to hobble Japanese war efforts elsewhere and even bring about a Japanese defeat. Thus, the attack was more of a desperate gamble to buy Japan time to secure a larger geography from which to extract natural resources and defend itself.</p><p>Japan’s strategy in the lead up to the December 7th attack was as impressive as the attack itself, providing a reminder that underestimating what you are up against, as the United States did with Japan at the time, can give the other side an advantage over you.”</p><h3><a href="https://newsletter.butwhatfor.com/p/perseverance-is-great-but-dont-forget">Perseverance is Great, But Don’t Forget to Prepare</a></h3><p>“After numerous terrorism incidents in the 1970s, the United States realized its armed forces had a blind spot when it came to counterterrorism. Green Beret <a href="https://en.wikipedia.org/wiki/Charles_Alvin_Beckwith">Colonel Charlie Beckwith</a>, who had served alongside the British Army&#39;s counter-terrorism unit the Special Air Service (also known as &#34;<a href="https://en.wikipedia.org/wiki/Special_Air_Service">SAS</a>&#34;) in Malaysia, had been pushing for such a group since the 1960s.</p><p>Now that terrorism was a proven threat killing Americans and the opportunity to be proactive had passed, the U.S. Army decided it was a good time to commission its own SAS-like force - namely, <a href="https://www.wearethemighty.com/articles/this-long-forgotten-unit-was-the-direct-predecessor-to-delta-force#:~:text=The%20US%20Army&#39;s%20highly%20secretive,American%20counterterrorist%20force%20in%20existence.">Delta Force</a>.</p><p>It is in the lead-up to the formation of Delta Force that we meet Eric Haney.”</p><p><a href="https://www.butwhatfor.com/">But What For?</a> Writing about anything, as long as it’s interesting</p><p data-attrs="{&#34;url&#34;:&#34;https://newsletter.butwhatfor.com/p/speech-at-the-stadium?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;,&#34;class&#34;:null}"><a href="https://newsletter.butwhatfor.com/p/speech-at-the-stadium?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share"><span>Share</span></a></p><p data-attrs="{&#34;url&#34;:&#34;https://newsletter.butwhatfor.com/p/speech-at-the-stadium/comments&#34;,&#34;text&#34;:&#34;Leave a comment&#34;,&#34;class&#34;:null}"><a href="https://newsletter.butwhatfor.com/p/speech-at-the-stadium/comments"><span>Leave a comment</span></a></p></div></div>]]></content:encoded>
      <guid>https://butwhatfor.substack.com/p/speech-at-the-stadium</guid>
      <pubDate>Mon, 02 Aug 2021 11:18:22 +0000</pubDate>
      <source>https://butwhatfor.substack.com/p/speech-at-the-stadium</source>
    </item>
    <item>
      <title>Testing quantum mechanics in space</title>
      <link>https://www.nature.com/articles/d41586-021-02091-8</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_nature_com_articles_d41586-021-02091-8/image.jpg" /> 
<div id="readability-page-1" class="page"><div>
                    <figure>
 <div>
  <p></p><figcaption>
   <p><span>Christina Koch on the International Space Station, with hardware used to probe the quantum effects of gases chilled to almost absolute zero.</span><span>Credit: NASA</span></p>
  </figcaption>
 </div>
</figure><p>Where does the shift from quantum to classical reality take place? To find out, physicists are testing whether molecules of ever-larger size behave like waves. When these particles are shot through narrow slits, they produce a striped interference pattern, just like light or water waves would.</p><p>The holder of the current size record for this wave behaviour is a molecule thousands of times smaller than a speck of dust or a bacterium (these span tens to hundreds of nanometres or more). Called oligoporphyrin, it is made of 2,000 atoms, measures 5–6 nanometres across and weighs around 25,000 atomic mass units<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup><sup>,</sup><sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> (amu; 1 amu is one-twelfth the mass of an atom of carbon-12). The technological implications of finding even larger objects displaying quantum behaviour are tantalizing.</p><p>But there are limits to what can be done in the laboratory<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>. Quantum-matter interferometers are complex, bulky and difficult to calibrate. The apparatus must be shielded from outside gases, light and vibrations. The larger a particle gets, the more likely it is to interact with its surroundings, washing out its quantum behaviour. It takes longer to produce interference, because the quantum waves spread more slowly. That means keeping the particle stable for longer.</p><p>Gravity is a limitation. Tabletop experiments can run for only a few seconds before the particles fall onto the bench. Particles larger than a few tens of nanometres would require tens of seconds to generate an interference pattern (for example, it would take 100 seconds for particles with a mass of 10<sup>11</sup> amu to reveal fringes). Introducing lasers or magnetic fields to buoy up the particles adds noise and complications.</p><p>The answer is to work in space (see ‘Three stages in space’).</p><figure>
 <div>
  <p></p><figcaption>
   <p><span>Source: Source: Belenchia <i>et al</i>.</span></p>
  </figcaption>
 </div>
</figure><h2><b>Orbital offers</b></h2><p>In microgravity, test particles would float freely for minutes. They would fall towards Earth at the same rate as the satellite they were in. An equivalent experiment on Earth lasting 100 seconds would be like controlling a particle while it drops from a height of 50 kilometres.</p><p>Major challenges must be overcome. The whole set-up should be designed for the specific particles being deployed — taking into account their sizes and electric charges, for example. And it must be able to operate in the harsh environment of space, showered with cosmic rays, solar wind and ionizing radiation. The experiment’s size and weight must be constrained. The satellite’s motion needs to be considered, and noise (such as vibrations from engines) will have to be minimized.</p><p>Interest in taking quantum technology into space is growing. Yet, so far, most nations have focused on devices that have commercial or security applications. For example, in 2016, the Chinese satellite Micius demonstrated quantum-encrypted communication between Beijing and Vienna<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>. A <a href="https://www.dlr.de/content/en/articles/news/2017/20170123_maius-1-first-bose-einstein-condensate-generated-in-space_20337.html" data-track="click" data-label="https://www.dlr.de/content/en/articles/news/2017/20170123_maius-1-first-bose-einstein-condensate-generated-in-space_20337.html" data-track-category="body text link">German team in 2017</a> and a <a href="https://coldatomlab.jpl.nasa.gov" data-track="click" data-label="https://coldatomlab.jpl.nasa.gov" data-track-category="body text link">NASA collaboration in 2020</a> produced a Bose–Einstein condensate in space — a quantum system with potential for sensing and metrology.</p><p>A quantum interferometer for large particles would be much more complicated. Putting one in orbit would require great technical, technological and scientific leaps. Here we set out the main research challenges and make the case for establishing a billion-dollar international collaboration to achieve this breakthrough.</p><p>Quantum physicists and space engineers need to do the following.</p><h2><b>Select particles</b></h2><p>Physicists need to assess the mass, size and shape of the test particles that would be used, as well as their chemical, electrical and optical properties. All of these will dictate the experiment’s design. The payload must be able to handle particles with a range of masses and sizes, to track how quantum behaviour scales. Charges will need to be controlled down to the level of individual electrons, to minimize noise. The nanoparticles should be able to interact with the lasers used to control and detect them, yet not absorb stray light.</p><p>Glass nano-beads are good test candidates. These nanometre-sized spheres made of silica or hafnium dioxide are already widely used in ground-based experiments. Other materials, such as gold or diamond, might also be suitable.</p><p>A large number of experimental runs will be needed to assure the particles’ quality and to prove that the experiment works. Techniques will need to be developed for repeating experiments reliably and efficiently, under stable conditions and with minimal interventions. Physicists will need to design automated methods for loading, capturing and reusing the nano-particles. Promising approaches being explored include using piezoelectric transducers to catapult particles to the spot where they will be used, or nebulizers that spray particles stored in solution.</p><h2><b>Choose gratings and detectors</b></h2><p>A grating — a series of slits — must be placed along the path of the particles to reveal their quantum behaviour (see ‘Quantum test’). This is typically a solid mask. However, such masks can trap large particles, reducing the grating’s effectiveness. Optical gratings are an alternative widely used on the ground, in which laser light acts as a grid. Precise modelling will be required to understand how the particles interact with the light. Industry will need to develop lasers that remain stable for lengthy experiments, as well as modulators that can change light intensity in milliseconds.</p><p>Finally, the particles’ positions must be detected after they have passed through the grating. At a minimum, devices should be able to measure the locations to within one-tenth of the distance between interference fringes. Capturing light scattered by the particle is an established technique that can be translated into space.</p><figure>
 <div>
  <p></p><figcaption>
   <p><span>Source: Belenchia <i>et al</i>.</span></p>
  </figcaption>
 </div>
</figure><h2><b>Address cooling, vacuum and noise</b></h2><p>Before test particles enter the interferometer, they must be cooled to states that have minimal energy and motion (around 10<sup>–6</sup> kelvin). This can be done using lasers, with methods used on the ground. The whole experimental apparatus would also need to be cooled down. Temperatures of 0.1 K have been achieved in space using cryostat devices based on mixtures of helium isotopes. However, the amount of helium required limits the mission’s lifetime. Researchers need to weigh up such trade-offs.</p><p>Ultra-high vacuum conditions are also essential. Collisions with gas molecules or dust motes, brought from Earth, would disrupt the nanoparticles’ quantum behaviour. A vacuum pressure of around 10<sup>–11</sup> pascals or less would be needed to hold nanospheres with masses larger than 10<sup>11</sup> amu stable for the 100 seconds it takes to measure fringes, for example. Although this is possible on the ground, it is much harder in an enclosed chamber in space, because vacuum pumps generate vibrations. One solution is to coat the inside of the chamber with films that catch stray gas particles, such as an alloy based on titanium, zirconium and vanadium, which is being developed at CERN, Europe’s particle-physics lab near Geneva, Switzerland.</p><p>All sources of noise must be minimized. These include mechanical vibration, interactions with gases, and showers of solar radiation, micro-meteoroids and ions. Quantum physicists working with engineers at the European Space Agency (ESA) have drawn up technical plans for dealing with each of these, for future missions<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>. However, the techniques have yet to be demonstrated together in a single experiment.</p><p>Techniques for isolating vibrations have been developed, for example, by the LISA Pathfinder mission. This spacecraft, launched in 2015, tested technologies for the Laser Interferometer Space Antenna (LISA), an ESA-led gravitational-wave observatory planned for launch in 2034. LISA Pathfinder controlled noise well enough to measure relative accelerations between two masses (each weighing 2 kg) of up to 10<sup>–14</sup> <i>g</i>, where <i>g</i> is the acceleration caused by the gravitational pull of Earth (see <a href="http://go.nature.com/3zkawmp" data-track="click" data-label="http://go.nature.com/3zkawmp" data-track-category="body text link">go.nature.com/3zkawmp</a>). So far, this is one of the best performances reported. By comparison, that’s like tracking a mass as it takes one day to travel one-tenth of a millimetre. Interferometric quantum experiments will need to reach similar levels with much smaller masses.</p><h2><b>Find stable orbits</b></h2><p>Space engineers must establish which orbits to put the satellite in to avoid jostling the particles. The main aim is to minimize acceleration and other changes in gravitational forces.</p><p>Suitable paths have been suggested. For example, the spacecraft effectively ‘hovers’ at points where the gravitational pulls and centrifugal forces of its orbital motion balance each other. ESA supports the idea of a satellite orbiting around such a point 1.5 million kilometres directly ‘behind’ Earth as viewed from the Sun — known as L2. Earth is always visible at this position, making communication easier. (The Planck satellite currently sits here, as will the James Webb Space Telescope.)</p><p>Alternatives include the L1 point, where the gravitational forces of the Sun and Earth on the satellite are opposed. At L1, also about 1.5 million kilometres from Earth, the satellite orbits the Sun at the same angular speed as Earth does. (This spot is currently home to the Solar and Heliospheric Observatory satellite.)</p><h2><b>Invest and collaborate</b></h2><p>When implemented in space, the costs of testing the limits of quantum superposition — being in two or more different physical states at the same time, which is the fundamental property of quantum systems — will exceed current national research budgets. LISA Pathfinder cost more than €430 million (US$508 million), the Chinese Micius was more than $100 million and the NASA-led experiment taking a Bose–Einstein condensate on the (already very costly) International Space Station was about $70 million. By comparison, UK support for its Quantum Technologies for Fundamental Physics programme is only £31 million ($43 million). Tabletop interferometry experiments cost just a few million euros.</p><p>Instead, a supranational collaboration needs to be established with an overall budget of at least €1 billion, including Earth-based activities. By comparison, this is the entire budget of the <a href="https://digital-strategy.ec.europa.eu/en/policies/quantum-technologies-flagship" data-track="click" data-label="https://digital-strategy.ec.europa.eu/en/policies/quantum-technologies-flagship" data-track-category="body text link">EU Quantum Flagship programme</a>. Skills need building and collaborations strengthening: between public and private sectors, academics, agencies and companies that have track records at the interface of quantum and space technology (such as Airbus Defense and Space in Portsmouth, UK, OHB System in Bremen, Germany and Thales Alenia Space in Cannes, France)<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup>.</p><p>Europe has taken the lead. Large-particle interferometry is one of three priority areas (together with cold atoms and entangled photon experiments) in the most recent ESA Intermediate Strategic Report, issued in 2017. A dedicated mission might be launched by the late 2030s<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>. Large-mass quantum tests are slated for later medium-class missions, as highlighted in ESA’s longer-term road map, Voyage 2050, announced in June.</p><p>Since 2017, the European Union has invested €500,000 in QTSpace, a project aimed at building a quantum space community involving researchers and companies from 46 countries (<a href="http://www.qtspace.eu" data-track="click" data-label="http://www.qtspace.eu" data-track-category="body text link">www.qtspace.eu</a>). The <a href="http://www.qtspace.eu/?q=quantumspaceship" data-track="click" data-label="http://www.qtspace.eu/?q=quantumspaceship" data-track-category="body text link">Quantum Space Network</a> initiative — a parallel body to the Quantum Community Network established within the EU Quantum Flagship programme — is liaising with policymakers, funding bodies and ESA. Other nations, scientists, agencies and companies now need to get on board. Fragmenting efforts will only delay progress. European researchers have already seen how pooling effort through the EU has sped up the pace of advances.</p><p>Critics will say that it’s unnecessary to set up yet another billion-dollar programme in fundamental physics, especially in a world grappling with COVID-19 and climate change. We contend that the payback could be vast in terms of new knowledge and technologies, even within a decade. Investments in the space sector will contribute to the recovery of the global economy.</p><p>Such a dialogue should begin at the next European Quantum Technology Virtual Conference, being held online at the end of November. Membership of the Quantum Space Network and Quantum Community Network should be extended to representatives of NASA and the Micius team at the Chinese Academy of Sciences, as well as to other major players in the quantum space race, from Canada to Singapore and Japan.</p><p>Protection of intellectual property and sharing of technologies and data security should be high on the agenda. These issues have hindered the establishment of a transcontinental framework in quantum tech before. In this respect, the recent EU decision to allow non-EU countries such as the United Kingdom and Israel to bid for Horizon Europe funding in quantum and space programmes is a positive signal.</p>
                </div></div>]]></content:encoded>
      <guid>https://www.nature.com/articles/d41586-021-02091-8</guid>
      <pubDate>Wed, 04 Aug 2021 00:09:16 +0000</pubDate>
      <source>https://www.nature.com/articles/d41586-021-02091-8</source>
    </item>
    <item>
      <title>PhD student in Switzerland expelled after criticizing the CCP in Twitter</title>
      <link>https://www.reddit.com/r/europe/comments/oxmgg2/phd_student_in_switzerland_expelled_from_his/</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div data-test-id="comment"><div><p>If you want to see a company that didn&#39;t kowtow, look at Cover, the company behind the VTuber Group Hololive.</p><p>Two of their members read out their YouTube analytics, which included the region Taiwan. There was no political intent, no attempt to make a statement or something. They just read what it said.</p><p>Yet, Chinese nationalists got immensely upset. They went on break for a month while the Chinese &#34;fans&#34; called for them to be sacked. They came back, and not only did Hololive emphasize their support for them, they straight up pulled out of China, a market they already had a foot in and let their entire Chinese branch go.</p><p>Sadly, this wasn&#39;t the end. Chinese nationalists waged a war over months in trying to bring Cover down in retaliation. This included harassment and spam on some talents stream chats. They also straight-up made a planned collaboration with ASUS go bust.</p><p>Going against China is pretty hard, and that&#39;s all before the Chinese government even did anything. Not even sure if the official knew or cared about this. But just a horde of rabid Chinese nationalist can ruin your day.</p></div></div></div>]]></content:encoded>
      <guid>https://www.reddit.com/r/europe/comments/oxmgg2/phd_student_in_switzerland_expelled_from_his/</guid>
      <pubDate>Wed, 04 Aug 2021 12:33:00 +0000</pubDate>
      <source>https://www.reddit.com/r/europe/comments/oxmgg2/phd_student_in_switzerland_expelled_from_his/</source>
    </item>
    <item>
      <title>The surreal experience of my first developer job</title>
      <link>https://bennuttall.com/the-surreal-experience-of-my-first-developer-job/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___bennuttall_com_the-surreal-experience-of-my-first-developer-job_/image.jpg" /> 
<div id="readability-page-1" class="page"><div>
  <div>
    
    
<p>Nearly ten years ago I graduated with a degree in Mathematics &amp; Computing, with a keen interest in pursuing a career involving maths and programming, but with little idea how. First and foremost I had decided to stay in Manchester after uni, rather than risk getting stuck at my parents’ if I moved back home. So I rented a flat with a friend from uni and started looking for a job. I had spent a lot of time at uni learning web development – not so much from the course, but self-taught as the course was quite far behind. I was happily making websites with the LAMP stack, I felt pretty capable at it and I knew there were plenty of such jobs in the Manchester area – so I started applying for “Web developer” and “PHP developer” and similar positions in all sorts of local companies.</p>



<p>The first interview I had was a general developer job in a small company in Stockport – the job required Microsoft Access experience, and for some reason this was still something I considered myself to have, and was willing to promise to others I had, which baffles me now. I ended up having a really great chat with the company boss at the interview. He asked some great interview questions about data analysis, and we spent about 90 minutes in conversation – he was fascinated by what I’d studied at uni – particularly data visualisation and dynamical systems &amp; chaos. I did get offered the job but he was too late – I’d already had my second interview and they asked me to start immediately.</p>



<p>My second interview was with a company called <em>App Start</em> in an office building in an old fashioned industrial estate in a horrible part of Salford (if you think of Media City as <em>Salford</em>, the rest Salford would be a shock). I was pushing it for time due to underestimating the walk from the train station, and I arrived a bit flustered from the rush and trying to find the place. I was greeted by a man in beach shorts and flip-flops. I was dressed in a full suit, shirt, tie and smart shoes, and I told him “I’m here for an interview with Chris” – his response “Oh, Chris doesn’t work here any more” should have set off alarm bells, but hey, I was young and naive.</p>



<div><figure></figure></div>



<div><figure><figcaption>Clifton Business Park, Salford. The real factory of dreams.</figcaption></figure></div>



<p>The guy in flip-flops, Alex, seemed to be in charge now. He asked which role I’d applied for – I said “web developer” and he invited me in. There were about 15-20 people working at desktop PCs. There was a big sofa. The floor was bare concrete. In between whiteboards with passwords written on them you could see the walls were graffitied. The “interview” involved him trying to find my CV in Chris’s inbox, and me showing him some websites I’d made. He asked if I could come in the next day, to do a day’s work as a trial – so I did.</p>



<p>The next day was a Friday – and I seem to recall building a prototype of a social networking site – although I’m not sure why. Anyway – he seemed to think I was good enough so he offered me the job – starting on Monday – which was great for me – I’d landed a full-time job starting straight away, meaning I could pay the rent and start my post-uni life in Manchester. But once I accepted, he asked “are you free tomorrow?” – I hesitated but answered the question – “yes”. “Can you come in?” and because I wasn’t sure what else to say, I said yes. Again, this should have been a sign.</p>



<p>The owner was a young entrepreneur called Andrew, who I was told had made millions through his property development and management company, <em>Fresh Start Living</em>. He decided to get into the “app” market (remember, this was 2011 – iPhone apps were huge) and so he’d decided to give his IT guy (Alex) a budget to hire some developers to make a load of apps. Surely one of them would be a huge success and he’d make millions.</p>



<h2>Monday to Friday? 9 to 5?</h2>



<p>I went back again on the Saturday to find no sign of anyone in the office. I waited around a bit but didn’t know what to do. I didn’t have a contact number for Alex. After a while, he showed up. He’d been in the Fresh Start Living side of the building. We did a day’s work on our own in the office. He gave me a lift home and I was due to start work for real on the Monday.</p>



<p>I arrived on Monday, and was told about the project I’d be working on. The short version is that I was told we had to have it finished before anyone could go home. We were there until 9pm. That was my first day as a full-time employee.</p>



<div><figure><figcaption>My desk. A desktop PC running Windows. Google+ and the original Tweetdeck. Beer.</figcaption></figure></div>



<h2>Skill Buy</h2>



<p>So on my first day, Alex told me the project I’d be working on was called <em>Skill Buy</em>, and proceeded to explain the concept. The idea was similar to an auction site that was popular at the time where you could bid on a product, but you paid for every bid – so if you saw an iPhone with a winning bid of 1p, you could bid 2p, but if you were outbid, you’d lost your 2p – eventually someone would win it at a bargain price at the expense of everyone who’d bid before them and the company would make a fortune. They’re called penny auctions and you can read about them in this paper: <em><a href="https://irep.ntu.ac.uk/id/eprint/26656/1/PubSub3120_Griffiths.pdf"><strong>Are online penny auctions a form of gambling?</strong></a></em> Anyway – the idea I’d been told about was similar, but instead of bidding, you’d play a game (a <em>Flash</em> or <em>Unity</em> game made by the game devs I was working with) – and it worked like an arcade game – the winner gets the prize. You’d buy credits, spend your credits playing games, and if you were the top scorer at the end of the game’s countdown timer, you’d win the prize. Actually you’d win the prize at 99% discount because they seemed to think that got around gambling laws.</p>



<p>Without a web developer on staff, getting this idea off the ground had proved tricky, and Alex had been using his own jack-of-all-trades skills to knock something together. He had some experience managing an e-commerce site (I later found out that he ran his own online sex toy shop…) so he’d put together a Skill Buy prototype using the PHP e-commerce framework <em>Magento</em>. He would add products (prizes) in the site’s admin area, and hacked away at the templates to show countdown timers and things. Somehow, Alex had promised the site would be ready to show Andrew the next day – and at some point in the afternoon he started to stress out and said to the whole room “no-one’s going home until this is done”. One of the game devs, had made a target shooting practice game in <em>Unity</em>, and we had to figure out how to embed it in the site and get the game to submit the player’s score to the site’s PHP back-end. We had no idea where to start, but we worked on it all afternoon, well into the evening, and we got it working to whatever standard Alex had required, and finally got to go home.</p>



<div><figure><figcaption>The shooting range game</figcaption></figure></div>



<p>Over the next few weeks I worked on the site, I grew fed up of being stuck in this e-commerce framework – having to work with its hyper-normalised MySQL database (the <a href="https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model">EAV model</a>), and the fact the product I was building wasn’t really a shop – so I asked Alex if he minded me writing something custom instead. I got my own custom PHP/MySQL site up-and-running pretty quickly, and it didn’t take long before we launched the site. I was happy enough writing crappy PHP, implementing questionable security protections (feeble attempts to block people from sending in fake scores), and learning things like how to integrate Facebook Login, PayPal payments, using AJAX, and working with the game devs to integrate with the scoring system. I actually quite enjoyed working there for a little while! It was liberating, knowing I was using my self-taught web dev skills on my own terms – and getting paid to do so.</p>



<div><figure><figcaption>Alex McDaid (beard) and Liam Daly</figcaption></figure></div>



<div><figure><figcaption>Shocking coverage from the Manchester Evening News, which you can still <a href="https://www.manchestereveningnews.co.uk/business/innovation/appstart-launches-online-arcade-skill-872914">read online</a></figcaption></figure></div>



<p>The apps company was being run by Alex and a 19-year-old kid called Liam, whose Uncle was involved in the property business. Somehow Liam had been given an experimental side-project business to help run as his “placement year” at uni.</p>



<div><figure><figcaption>Skill Buy</figcaption></figure></div>



<p>One thing that might strike you as odd is the bizarre graphics. The designer had made all these cow-in-a-field graphics for an iPhone game that was planned – but they’d shelved it and told her to re-use them on the website. She <em>hated </em>that they were being used like this – and it didn’t make sense to anyone else either.</p>



<p>Alex had a tendency of regularly asking if you were free on Saturday. The first few times I said yes, and just worked an extra day – I think I assumed I would automatically get paid overtime – but I didn’t. Eventually I just learned to say no.</p>



<p>A little while after I joined, a guy called Bob turned up and took some kind of product manager job, similar to Alex and Liam. Like Liam, he seemed nice enough but somewhat out of place. He was a friend of Alex’s. He used to stare at Google Analytics all day and come over and tell me things about bounce times and referrals. He used to leave after lunch, for whatever reason, so he earned the nickname <em>Half-a-job Bob.</em> He didn’t last long though. One day, he’d been in a meeting with Andrew, who’d obviously seen through him and had enough. I came to discover that when Andrew decided he didn’t like someone, rather than fire them, he’d have a go at them and make them feel so angry they’d want to leave. Bob marched through the door, slammed it behind him, muttering loudly to himself “he’s not talking to me like that” and declared to the room that he was “taking no more of Andrew’s shit” and stormed out.</p>



<figure><ul><li><figure></figure></li><li><figure></figure></li><li><figure></figure></li></ul></figure>



<p>We’d launched the site, and they’d started running Facebook ads promoting it. Alex even ordered a load of printed materials and signs to put up. I remember once, when giving me a lift home, he pulled up and got out of the car to cable tie a Skill Buy banner to a railing at a set of traffic lights. <a href="http://whatsthefunctionality.blogspot.com/2011/10/skillbuycouk-and-what-it-is.html">We had people playing</a> – you got a few free credits after signing up, and some were actually buying credits too. The cheaper gimmicky prizes had short timers on them, and we had winners claim prizes – which Liam shipped out. We had some big prizes on there too, like a Rolex watch, a year’s free accommodation, and a house. The idea was that Andrew could give away use of some of Fresh Start’s rental properties and even a house, if the website made enough revenue. The accommodation and the house were on a 3-month timer, and <em>surely</em> that would be enough time to build up the site user base.</p>



<div><figure><figcaption>We ran a stall at the university fresher’s fair to try to get some student interest. We ran a special game where people played “Avoid the Hangover” on a projected screen to win “a mountain of beer” (or “mountin” if you saw our printed leaflets).</figcaption></figure></div>



<p>After a while, Andrew decided to make a change. Instead of buying credits, you would buy a ticket for a game, and only when enough people had bought a ticket would the game open, to guarantee the revenue covered the cost of the prize. This basically halted all activity on the site. Rather than people playing games and competing for prizes, they were just waiting, and hardly anyone was interested in buying tickets. It meant that the site was no longer spending any money on prizes, so it didn’t have any outgoings, but there wasn’t any sign of it getting the number of users it needed.</p>



<p>While searching my inbox for anything related to App Start, I found this email from Andrew, forwarded to me by Alex:</p>



<blockquote><p>Hi Ben – These are the skill buy changes required by andrew.</p></blockquote>



<div><figure><figcaption>One day, Andrew brought in a friend who had his own skip company – and he introduced me and said I would make him a website for his skip company.</figcaption></figure></div>



<h2>Pigs Might Fly</h2>



<p>Meanwhile, the rest of the team was working on iPhone games. There were some really talented designers and developers there – and they produced some great games they were rightly very proud of. They made a game called <em>Splat</em> (a bit like whack-a-mole) which was one of those addictive games, and they managed to get a local radio station to interview them about it. They also made a game called <em>Pigs Might Fly</em> – and they went all out with the marketing for this one. Alex got some large pig signs printed and attached them to helium balloons and sent them flying around Manchester City centre. They got everyone to text the radio station saying things like “I’ve just seen a flying pig float past my car in Manchester” and they read a few of them out and asked if anyone knew what it was about. Someone then called in to the radio show and went on air to promote the app. I’ve just found the company <a href="https://twitter.com/AppStartUK">Twitter</a>, <a href="https://www.flickr.com/photos/appstart/">Flickr</a> and <a href="https://www.youtube.com/user/AppstartLtd/videos">YouTube</a> accounts are still live and it’s all there:</p>



<figure><ul><li><figure></figure></li><li><figure></figure></li><li><figure></figure></li><li><figure></figure></li></ul></figure>



<p>A point of contention was that Andrew wanted them to knock out as many apps as possible, and wait to see if any of them got traction. But they wanted to choose an idea and spend a few months working on it, to do a decent job.</p>



<h2>An app a day</h2>



<p>Once I’d finished converting Skill Buy to its new format, it didn’t need any ongoing work until it had the users it needed, so I was asked to join the rest of the team building apps. We were assigned to groups of three: two developers and a designer in each. We had to make 10 apps per group every week. So each developer had to make an app a day, with the designer creating any assets we needed. We just listed off a load of crappy ideas for things that you could make without any effort. They were mostly either a multiple-choice quiz, or a button that did something silly. I was using PhoneGap to bootstrap the apps, create the icons and metadata, and just writing the app logic in PHP scripts which were hosted on the Skill Buy server. The apps looked terrible, they were pointless and had no potential. But we were hitting our targets. Andrew also fed us his own ideas, such as the “Ask an expert” series, where we’d build an app you could submit questions to, and an “expert” would respond. He seemed to think he knew some experts.</p>



<p>Around this time, the same thing that happened to Bob had happened to Alex – he just came in and shouted “I’m done, Cya!” and walked out. We knew he must have been on the receiving end of Andrew’s routine.</p>



<p>I had a similar experience with Andrew myself. One day, the web developer in the property business was off sick and I was asked to cover for him, because Andrew had been promised that some new feature was going to be ready by the end of that day. I was briefed by Steph from the property business, and I had a Google Chat open with the developer. I did what was asked, and Steph was happy with it. I was called in to present it to Andrew, who seemed to believe that this was not was he’d asked for – so he ridiculed the work, called it a waste of time, told me to “go back to apps” and proceeded to yell at Steph as I was leaving.</p>



<p>So we continued for a little while. We made our crappy apps. We hit our targets. We told our 19-year-old boss that we can’t go on like this, that none of these apps were going to succeed. I decided it was time to move on. I waited to the end of the month, made sure I’d been paid and I politely told Liam I was leaving. They had never given me a contract, so I didn’t have a notice period or anything. I just left and started looking for something new. I bumped into one of the designers a month or so later and he said that one day Andrew just closed them down and they all lost their jobs.</p>



<p>It’s a shame my first real job only lasted about 4 months, but it was a wild ride and a crazy experience. I haven’t really thought about that time much until now. Looking back, it feels so strange and the story seems so surreal, so I felt like writing about it.</p>



<div><figure><figcaption>The history of Skill Buy over the years – provided by archive.org</figcaption></figure></div>



<h2>Andrew Camilleri, the Gazerati, and a suspended prison sentence</h2>



<p>Andrew was quite the character, but he was nothing compared to Gaz. I think Gaz was Andrew’s oldest and best friend. When Andrew made his money, he hired Gaz to be his driver / bouncer. Gaz looked like a bouncer. He was one of those people who you were terrified of at sight – but he was genuinely pleasant and nice to talk to. One day, we heard a vehicle driving around the car park making a lot of noise. Andrew had just purchased a <em>Maserati</em> as a “present” for Gaz – to drive him around in, and they were doing laps of the industrial estate. Naturally, this car became known as the <em>Gazerat</em>i.</p>



<p> While researching online for this article, I came across a news article entitled <em><a href="https://www.placenorthwest.co.uk/news/property-developer-given-suspended-prison-sentence/">Property developer given suspended prison sentence</a></em>, which starts:</p>



<blockquote><p>Andrew John Camilleri, a businessman formerly associated with wound-up property company Fresh Start Living, has been unanimously convicted by a jury at Manchester Crown Court of making false representations in an Individual Voluntary Arrangement proposal.</p><cite>placenorthwest.co.uk</cite></blockquote>



<p>It also states that in early 2011, Andrew made false representations to creditors in attempt to wipe out debts of £9m. So rather than being a millionaire, he was actually the opposite – a negative millionaire. And it seems Fresh Start Living, like App Start Ltd, is no more.</p>
  </div>
</div></div>]]></content:encoded>
      <guid>https://bennuttall.com/the-surreal-experience-of-my-first-developer-job/</guid>
      <pubDate>Wed, 04 Aug 2021 09:04:07 +0000</pubDate>
      <source>https://bennuttall.com/the-surreal-experience-of-my-first-developer-job/</source>
    </item>
  </channel>
</rss>
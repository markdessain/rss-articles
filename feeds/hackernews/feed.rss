<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Hacker News</title>
    <link>https://news.ycombinator.com/</link>
    <description>Links for the intellectually curious, ranked by readers.</description>
    <image>
      <url></url>
      <title></title>
      <link></link>
    </image>
    <item>
      <title>Launch HN: Slip (YC S21) – Build and sell interactive programming courses</title>
      <link>https://news.ycombinator.com/item?id=28141462</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><td colspan="2"></td><td>Hi HN, I&#39;m Kenneth and I&#39;m the founder of Slip (<a href="https://www.slip.so" rel="nofollow">https://www.slip.so</a>), a marketplace for programmers to build and sell courses, including interactive elements like in-browser code execution, popular programming embeds (CodeSandbox, StackBlitz, Replit repls (Coming soon)), and videos.<p>Instead of spending 3 or more months building their own course platform, developers can use Slip to create engaging interactive courses and make more money faster from their knowledge</p><p>In January, I built vim.so in 3 days, and made $11k in my first month. I even did a Show HN for it (<a href="https://news.ycombinator.com/item?id=25846347" rel="nofollow">https://news.ycombinator.com/item?id=25846347</a>). I was able to do it in 3 days because I had previously spent 3 months building an interactive course for Python fundamentals. That previous experience reduced the time it took me to build a new course, which was the only reason it made sense to do. 3 days of hacking was low-risk enough that when I had the idea for vim.so, it made sense to actually try and see.</p><p>The results blew me away and actually changed my life. If sales continue at the current rate, I&#39;ll make about $50k this year with vim.so. This experience gave me confidence that I could build something and sell it on the internet. It helped give me credibility as a developer, and got me connected with lots of other cool folks building cool things.</p><p>After launching vim.so, I started getting lots of inbound requests to build other interactive courses on various topics: Ruby, Git, Bash, etc. At first I thought I&#39;d just build all these myself but quickly realized other folks could teach these topics at a much deeper level than I could. But why weren&#39;t they building these courses? It&#39;s because it&#39;s currently too hard to make an interactive programming course. After maybe the 5th Twitter DM asking me for an interactive Git course, I decided to start a platform that helps other devs do the same thing I did with vim.so.</p><p>The main tool in Slip is an online course editor that allows you to build a course with a variety of &#34;block types&#34;. You can use markdown, videos, code snippets, figma embeds, CodeSandbox Embeds, and executable code snippets. Code executions happen in remote one-off Docker containers. Code snippets are built using the open-source Ace Editor react component.</p><p>The editor is free to use. We take a 10% cut of sales made via our site (plus processing fees). We handle payments via Stripe and accept and remit VAT taxes for the author. Slip also has features to help authors make more money with their courses. For selected courses, we can run a presale campaign. We also publish and feature courses directly on our site that meet a certain quality bar.</p><p>Some devs who have rolled their own interactive course platform have spent more than 6 months just on that part! If we can remove that 6 months of non-content work, more devs will be able to build better educational materials. I&#39;ve met multiple folks making over 6 figures a year teaching programming courses. Slip will be a success if we can help many more people do that, a lot more easily.</p><p>If you have any experience building educational programming courses or ideas on what programming courses are lacking today, or have any thoughts to share, I&#39;d love to hear from you!</p></td></div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 12:59:30 +0000</pubDate>
      <source>https://news.ycombinator.com/item?id=28141462</source>
    </item>
    <item>
      <title>GitHub’s Engineering Team has moved to Codespaces</title>
      <link>https://github.blog/2021-08-11-githubs-engineering-team-moved-codespaces/</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

	<p><em>Today, GitHub is making <a href="https://github.com/features/codespaces" target="_blank" rel="noopener">Codespaces</a> available to Team and Enterprise Cloud plans on github.com. Codespaces provides software teams a faster, more collaborative development environment in the cloud. Read more on our <a href="http://github.com/features/codespaces" target="_blank" rel="noopener">Codespaces page</a>.</em></p>
<hr/>
<p>The GitHub.com codebase is almost 14 years old. When the first commit for GitHub.com was pushed, Rails was only two years old. AWS was one. Azure and GCP did not yet exist. This might not be long in COBOL time, but in internet time it’s quite a lot.</p>
<p>Over those 14 years, the core repository powering GitHub.com (github/github) has seen over a million commits. The vast majority of those commits come from developers building and testing on macOS.</p>
<div id="attachment_59288"><p></p><p id="caption-attachment-59288"><em><span>A classic commit message for a classic commit</span></em></p></div>
<p>But our development platform is evolving. Over the past months, we’ve left our macOS model behind and moved to Codespaces for the majority of GitHub.com development. This has been a fundamental shift for our day-to-day development flow. As a result, the Codespaces product is stronger and we’re well-positioned for the future of GitHub.com development.</p>
<h2 id="the-status-quo">The status quo<a href="#the-status-quo" aria-label="The status quo" data-anchorjs-icon="#"></a></h2>
<p>Over the years, we’ve invested significant time and effort in making local development work well out of the box. Our <a href="https://github.blog/2015-06-30-scripts-to-rule-them-all/" target="_blank" rel="noopener">scripts-to-rule-them-all approach</a> has presented a familiar interface to engineers for some time now—new hires could clone <code>github/github</code>, run setup and bootstrap scripts, and have a local instance of GitHub.com running in a half-day’s time. In most cases things just worked, and when they didn’t, our bootstrap script would open a GitHub issue connecting the new hire with internal support. Our <code>#friction</code> Slack channel—staffed by helpful, kind engineers—could debug nearly any system configuration under the sun.</p>
<div id="attachment_59287"><p></p><p id="caption-attachment-59287"><em><span>Run GitHub.com locally (eventually) with this one command!</span></em></p></div>
<p>Yet for all our efforts, local development remained brittle. Any number of seemingly innocuous changes could render a local environment useless and, worse still, require hours of valuable development time to recover. Mysterious breakage was so common and catastrophic that we’d codified an option for our bootstrap script: <code>--nuke-from-orbit</code>. When invoked, the script deletes as much as it responsibly can in an attempt to restore the local environment to a known good state.</p>
<p>And of course, this is a classic story that anyone in the software engineering profession will instantly recognize. Local development environments are fragile. And even when functioning perfectly, a single-context, bespoke local development environment felt increasingly out of step with the instant-on, access-from-anywhere world in which we now operate.</p>
<p>Collaborating on multiple branches across multiple projects was painful. We’d often find ourselves staring down a 45-minute bootstrap when a branch introduced new dependencies, shipped schema changes, or branched from a different SHA. Given how quickly our codebase changes (we’re deploying hundreds of changes per day), this was a regular source of engineering friction.</p>
<p>And we weren’t the only ones to take notice—in building Codespaces, we engaged with several best-in-class engineering organizations who had built Codespaces-like platforms to solve these same types of problems. At any significant scale, removing this type of productivity loss becomes a very clear productivity opportunity, very quickly.</p>
<div id="attachment_59286"><p></p><p id="caption-attachment-59286"><em><span>This single log message will cause any GitHub engineer to break out in a cold sweat</span></em></p></div>
<h2 id="development-infrastructure">Development infrastructure<a href="#development-infrastructure" aria-label="Development infrastructure" data-anchorjs-icon="#"></a></h2>
<p>In the infrastructure world, industry best practices have continued to position servers as a commodity. The idea is that no single server is unique, indispensable, or irreplaceable. Any piece could be taken out and replaced by a comparable piece without fanfare. If a server goes down, that’s ok! Tear it down and replace it with another one.</p>
<p>Our local development environments, however, are each unique, with their own special quirks. As a consequence, they require near constant vigilance to maintain. The next <code>git pull</code> or <code>bootstrap</code> can degrade your environment quickly, requiring an expensive context shift to a recovery effort when you’d rather be building software. There’s no convention of a warm laptop standing by.</p>
<p>But there’s a lot to be said for treating development environments as our own—they’re the context in which we spend the majority of our day! We tweak and tune our workbench in service of productivity but also as an expression of ourselves.</p>
<p>With Codespaces, we saw an opportunity to treat our dev environments much like we do infrastructure—a commodity we can churn—but still maintain the ability to curate our workbench. VS Code extensions, settings sync, and dotfiles repos bring our environment to our compute. In this context, a broken workbench is a minor inconvenience—now we can provision a new codespace at a known good state and get back to work.</p>
<h2 id="adopting-codespaces">Adopting Codespaces<a href="#adopting-codespaces" aria-label="Adopting Codespaces" data-anchorjs-icon="#"></a></h2>
<p>Migrating to Codespaces addressed the shortcomings in our existing developer environments, motivated us to push the product further, and provided leverage to improve our overall development experience.</p>
<p>And while our migration story has a happy ending, the first stages of our transition were… challenging. The GitHub.com repository is almost 13 GB on disk; simply cloning the repository takes 20 minutes. Combined with dependency setup, bootstrapping a GitHub.com codespace would take upwards of 45 minutes. And once we <em>had</em> a repository successfully mounted into a codespace, the application wouldn’t run.</p>
<p>Those 14 years of macOS-centric assumptions baked into our bootstrapping process were going to have to be undone.</p>
<p>Working through these challenges brought out the best of GitHub. Contributors came from across the company to help us revisit past decisions, question long-held assumptions, and work at the source-level to decouple GitHub development from macOS. Finally, we could (albeit very slowly) provision working GitHub.com codespaces on Linux hosts, connect from VS Code, and ship some work. Now we had to figure out how to make the thing hum.</p>
<h3 id="45-minutes-to-5-minutes">45 minutes to 5 minutes<a href="#45-minutes-to-5-minutes" aria-label="45 minutes to 5 minutes" data-anchorjs-icon="#"></a></h3>
<p>Our goal with Codespaces is to embrace a model where development environments are provisioned on-demand for the task at hand (roughly a 1:1 mapping between branches and codespaces.) To support task-based workflows, we need to get as close to instant-on as possible. 45 minutes wasn’t going to meet our task-based bar, but we could see low-hanging fruit, ripe with potential optimizations.</p>
<p>Up first: changing how Codespaces cloned github/github. Instead of performing a full clone when provisioned, Codespaces would now execute a shallow clone and then, after a codespace was created with the most recent commits, unshallow repository history in the background. Doing so reduced clone time from 20 minutes to 90 seconds.</p>
<p>Our next opportunity: caching the network of software and services that support GitHub.com, inclusive of traditional Gemfile-based dependencies as well as services written in C, Go, and a custom build of Ruby. The solution was a GitHub Action that would run nightly, clone the repository, bootstrap dependencies, and build and push a Docker image of the result. The published image was then used as the base image in github/github’s devcontainer—config-as-code for Codespaces environments. Our codespaces would now be created at 95%+ bootstrapped.</p>
<p>These two changes, along with a handful of app and service level optimizations, took GitHub.com codespace creation time from 45 minutes to five minutes. But five minutes is still quite a distance from “instant-on.” Well-known studies have shown people can sustain roughly <a href="https://www.nngroup.com/articles/powers-of-10-time-scales-in-ux/" target="_blank" rel="noopener">ten seconds</a> of wait time before falling out of flow. So while we’d made tremendous strides, we still had a way to go.</p>
<h3 id="5-minutes-to-10-seconds">5 minutes to 10 seconds<a href="#5-minutes-to-10-seconds" aria-label="5 minutes to 10 seconds" data-anchorjs-icon="#"></a></h3>
<p>While five minutes represented a significant improvement, these changes involved tradeoffs and hinted at a more general product need.</p>
<p>Our shallow clone approach—useful for quickly launching into Codespaces—still required that we pay the cost of a full clone at <em>some</em> point. Unshallowing post-create generated load with distracting side effects. Any large, complex project would face a similar class of problems during which cloning and bootstrapping created contention for available resources.</p>
<p>What if we could clone and bootstrap the repository ahead of time so that by the time an engineer asked for a codespace we’d already done most of the work?</p>
<p>Enter prebuilds: pools of codespaces, fully cloned and bootstrapped, waiting to be connected with a developer who wants to get to work. The engineering investment we’ve made in prebuilds has returned its value many times over: we can now create reliable, preconfigured codespaces, primed and ready for GitHub.com development in 10 seconds.</p>
<p>New hires can go from zero to a functioning development environment in less time than it takes to install Slack. Engineers can spin off new codespaces for parallel workstreams with no overhead. When an environment falls apart—maybe it’s too far behind, or the test data broke something—our engineers can quickly create a new environment and move on with their day.</p>
<h3 id="increased-leverage">Increased leverage<a href="#increased-leverage" aria-label="Increased leverage" data-anchorjs-icon="#"></a></h3>
<p>The switch to Codespaces solved some very real problems for us: it eliminated the fragility and single-track model of local development environments, but it also gave us a powerful new point of leverage for improving GitHub’s developer experience.</p>
<p>We now have a wedge for performing additional setup and optimization work that we’d never consider in local environments, where the cost of these optimizations (in both time and patience) is too high. For instance, with prebuilds we now prime our language server cache and gem documentation, run pending database migrations, and enable both GitHub.com and GitHub Enterprise development modes—a task that would typically require yet another loop through bootstrap and setup.</p>
<p>With Codespaces, we can upgrade every engineer’s machine specs with a single configuration change. In the early stages of our Codespaces migration, we used 8 core, 16 GB RAM VMs. Those machines were sufficient, but GitHub.com runs a network of different services and will gladly consume every core and nibble of RAM we’re willing to provide. So we moved to 32 core, 64 GB RAM VMs. By changing a single line of configuration, we upgraded every engineer’s machine.</p>
<div id="attachment_59291"><p></p><p id="caption-attachment-59291"><em><span>Instant upgrade—ship config and bypass the global supply chain bottleneck</span></em></p></div>
<p>Codespaces has also started to steal business from our internal “review lab” platform—a production-like environment where we preview changes with internal collaborators. Before Codespaces, GitHub engineers would need to commit and deploy to a review lab instance (which often required peer review) in order to share their work with colleagues. Friction. Now we ctrl+click, grab a preview URL, and send it on to a colleague. No commit, no push, no review, no deploy — just a live look at port 80 on my codespace.</p>
<h3 id="command-line">Command line<a href="#command-line" aria-label="Command line" data-anchorjs-icon="#"></a></h3>
<p>Visual Studio Code is great. It’s the primary tool GitHub.com engineers use to interface with codespaces. But asking our Vim and Emacs users to commit to a graphical editor is less great. If Codespaces was our future, we had to bring everyone along.</p>
<p>Happily, we could support our shell-based colleagues through a simple update to our prebuilt image which initializes <code>sshd</code> with our GitHub <a href="https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account" target="_blank" rel="noopener">public keys</a>, opens port 22, and forwards the port out of the codespace.</p>
<p></p>
<p>From there, GitHub engineers can run Vim, Emacs, or even ed if they so desire.</p>
<p>This has worked exceedingly well! And, much like how Docker image caching led to prebuilds, the obvious next step is taking what we’ve done for the GitHub.com codespace and making it a first-class experience for every codespace.</p>
<h2 id="reception">Reception<a href="#reception" aria-label="Reception" data-anchorjs-icon="#"></a></h2>
<p>Change is hard, doubly so when it comes to development environments. Thankfully, GitHub engineers are curious and kind—and quickly becoming Codespaces superfans.</p>
<p>
<strong>I used codespaces yesterday while my dev environment was a little broken and I finished the entire features on codespaces before my dev env was done building lol</strong></p>
<p>Codespaces are now the default development environment for GitHub.com. That <code>#friction</code> Slack channel that we mentioned earlier to help debug local development environment problems? We’re planning to archive it.</p>
<p>We’re onboarding more services and more engineers throughout GitHub every day, and we’re discovering new stories about the value Codespaces can generate along the way. But at the core of each story, you’ll discover a consistent theme that resonates with every engineer: I found a better tool, I’m more productive now, and I’m not going back.</p>

	

	

</div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 16:01:01 +0000</pubDate>
      <source>https://github.blog/2021-08-11-githubs-engineering-team-moved-codespaces/</source>
    </item>
    <item>
      <title>I&#39;ve decided not to work with Aubrey de Grey or SENS</title>
      <link>https://ldeming.posthaven.com/aubrey</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div id="post_body_1722749">
    
      <div><p>I&#39;ve decided not to work with Aubrey de Grey or SENS in any capacity moving forward.</p><p>I had one bad experience with him when I was 17 - he told me in writing that he had an ‘adventurous love life’ and that it had ‘always felt quite jarring’ not to let conversations with me stray in that direction given that ‘[he] could treat [me] as an equal on every other level’.</p><p>He sent this from his work email, and I’d known him since I was 14. Stuff like that happened sometimes, and I wrote it off as a mistake - something that might be my fault for trying to work in an industry when I was younger than average or because I had mentioned concerns about mentors doing stuff like that in a previous email. In the past few months, in part through conversations with Celine Halioua (who interned at SENS), I’ve learned it’s a serial pattern he’s enacted with women over whom he’s in a position of power. You can read about Celine&#39;s experience <a href="https://www.celinehh.com/aubrey">here</a>.</p><p>I almost left the field several times as a teenager because of stuff like this happening. I knew that sometimes there would be misunderstandings, but I didn’t expect a trusted mentor I’d known since childhood to hit on me so blatantly, and insinuate that it had been on his mind for a while. It felt wrong to voluntarily go into an industry where - I basically inferred - sexual harassment was the norm.</p><p>Sexual harassment isn&#39;t acceptable behavior in the longevity field, but Aubrey is a really bad counterexample who does a lot of outreach. So newcomers to the field - mostly students and minors - can get the wrong impression.</p><p>It feels very weird to write about this, because I have a separate deeply held belief that we shouldn’t penalize people for not fitting into social norms, or for being different in ways that we can’t understand today. I just personally have no interest in working in a culture that is okay with certain norms (for example, propositioning minors or employees), and I’m angry to realize that Aubrey inappropriately propositioned more than one woman over whom he was in a position of power, many in the community knew about it, and no one did anything.</p><p>Earlier this year, Aubrey told me he was trying to &#39;cleanup his board&#39; in response to &#39;derogatory rumors&#39;, which was actually how I found out this wasn&#39;t something only I had experienced, and that he was trying to stop his board from doing anything about it. The SENS board is aware of claims of sexual harassment and hired a firm to investigate these concerns, but also recently took a ~$25M donation which was helped by Aubrey&#39;s fundraising capabilities and reputation despite the ongoing investigation. Aubrey&#39;s position as their major fundraiser has impacted their decision to work with him despite these concerns.</p><p>Lots of people are aware of these concerns, but no one has said anything for a decade, and given the recent donation and incentives involved neither Celine nor I have confidence that SENS will take the appropriate measures to stop Aubrey from harassing more young women. It might be an open secret in the longevity community that this is a problem, but kids on the internet don’t have access to that information, and Aubrey is still mentoring minors. So, we’re making our experiences public. We wish we had known what many in the community did about him when we were entering the field. </p><br/></div>
    
  </div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 15:36:53 +0000</pubDate>
      <source>https://ldeming.posthaven.com/aubrey</source>
    </item>
    <item>
      <title>So you want to write a GUI framework</title>
      <link>https://www.cmyr.net/blog/gui-framework-ingredients.html</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
      <article>

        

        <p>Through several <a href="https://www.reddit.com/r/rust/comments/o99zj1/what_do_you_think_will_emerge_as_the_dominant/">recent discussions</a> of <a href="https://news.ycombinator.com/item?id=27782337">GUI programming in Rust</a>,
I have been left with the impression that the term ‘GUI’ means significantly
different things to different people.</p>

<p>I would like to try and clarify this point somewhat, first by describing some of
the different things that people refer to as GUI frameworks/toolkits, and then
by exploring in detail the necessary components of one of these, the classic
desktop GUI framework.</p>

<p>Although this post is not especially specific to Rust, it does have its genesis
in Rust: it is largely informed by my experience working on <a href="https://docs.rs/druid">Druid</a>, a Rust
GUI toolkit of the desktop variety.</p>

<p>Once we have a shared understanding of the problem, we will be better situated
to talk about the status of this work in Rust, which will be the topic of a
follow-up post.</p>



<h2 id="what-we-talk-about-when-we-talk-about-gui">What we talk about when we talk about GUI</h2>

<p>A GUI framework can be a lot of different things, with different use cases
and different deployment targets. A framework intended for building embedded
applications is not going to also trivially work on the desktop; a framework for
building desktop applications is not going to trivially work on the web.</p>

<p>Regardless of the specifics, there is one major dividing line to recognize, and
this is whether or not a framework is expected to <em>integrate closely into an
existing platform or environment</em>.</p>

<p>On one side of this line, then, are tools for building games, embedded
applications, and (to a lesser degree) web apps. In this world, you are
responsible for providing almost everything your applications will need, and you
will be interacting closely with the underlying hardware: accepting raw input
events, and outputting your UI to some sort of buffer or surface. (The web is
different; here the browser vendors have done that integration work for you.)</p>

<p>On the other side of this line are tools for building traditional desktop
applications. In <em>this</em> world, you must integrate tightly into a large number of
existing platform APIs, design patterns, and conventions, and it is this
integration that is the source of most of your design complexity.</p>

<h3 id="games-and-embedded-guis">Games and embedded GUIs</h3>

<p>Before we start digging into all the integrations expected of a desktop
application framework, let’s talk briefly about the first case.</p>

<p>Games and GUI for embedded applications (think of the infotainment system in the
back of a taxi, or the interface on a medical device) are different from desktop
GUIs in a number of ways, most of which can be thought of in terms
of system integration: games and embedded applications don’t have to do as much of
it. In general, a game or an embedded application is a self-contained world;
there is a single ‘window’, and the application is responsible for drawing
everything in it. The application doesn’t need to worry about menus or
sub-windows; it doesn’t need to worry about the <a href="https://raphlinus.github.io/ui/graphics/2020/09/13/compositor-is-evil.html">compositor</a>, or integrating with
the <a href="https://github.com/linebender/druid/pull/1636">platform’s IME</a> system. Although they maybe <em>should</em>, they often don’t
support <a href="https://harfbuzz.github.io/complex-scripts.html">complex scripts</a>. They can ignore rich text editing. They likely don’t need
to support <a href="https://github.com/WICG/local-font-access">font enumeration</a> or <a href="https://www.figma.com/blog/when-fonts-fall/">fallback</a>. They often ignore accessibility.</p>

<p>Of course, they do have additional challenges of their own. Embedded
applications have to think much more carefully about resource constraints, and
may need to <a href="https://lupyuen.github.io/articles/porting-druid-rust-widgets-to-pinetime-smart-watch">avoid allocation</a> altogether. When they <em>do</em> need
features like complex scripts or text input, they have to implement these
features on their own, without being able to rely on anything provided by the
system.</p>

<p>Games are similar, and additionally have their own unique performance concerns
and considerations that I am not qualified to talk about in any real detail.</p>

<p>Games and embedded are certainly interesting domains. Embedded in particular is
a place where I think Rust GUI could really make a lot of sense, for many of the
same reasons that Rust generally has a <a href="https://www.rust-lang.org/what/embedded">strong value proposition</a>
for embedded use.</p>

<p>It is unlikely, however, that a project that is intended for game or embedded
development is going to tackle the whole list of capabilities we expect in
desktop applications.</p>

<h2 id="anatomy-of-a-native-desktop-application">Anatomy of a ‘native desktop application’</h2>

<p>The principal distinguishing feature of a desktop application is its close
integration into the platform. Unlike a game or an embedded application, a
desktop application is expected to interoperate intimately with the host OS, as
well as with other software.</p>

<p>I’d like to try and go through some of the major required integration points,
and some of the possible approaches available for providing them.</p>

<h3 id="windowing">Windowing</h3>

<p>An application has to instantiate and manage windows. The API should allow for
customization of window appearance and behaviour, including things like whether
the window is resizeable, whether it has a titlebar, etc. The API should allow
for multiple windows, and it should also support modal and child-windows in a
way that respects platform conventions. This means supporting both
<a href="https://developer.apple.com/documentation/appkit/nsapplication/1428418-beginmodalsession#">application-modal</a> windows (for instance alerts that steal focus from the entire
application until dealt with) as well as <a href="https://developer.apple.com/documentation/appkit/nswindow/1419653-beginsheet">window-modal</a> windows (an alert that
steals focus from a given window until dealt with). Modal windows are used to
implement a large number of common features, including open/save dialogs (which
may be special-cased by the platform) alerts, confirmation dialogs, as well as
standard UI elements such as combo boxes and other drop-down menus (think a list
of completions for a text field).</p>

<p>The API must allow subwindows to be positioned precisely, relative to the
position of the parent window. For instance in the case of a combo box, when
showing the list of options you may wish to draw the currently selected item at
the same baseline position used when the list is closed, as in macOS:</p>



<p>Similarly, there needs to be an API that provides information about screens and
the positions of windows within them, so that a combo box can be positioned
appropriately to use available space: if the box is at the bottom of the screen
it should position the popup above itself, and otherwise below.</p>

<h4 id="tabs">Tabs</h4>

<p>You’re also going to want to support tabs. You should be able to drag a tab out
of a tab group to create a new window, as well as drag tabs between windows.
Ideally you would like use the platform’s native tabbing infrastructure, but…
that’s complicated. The browsers all roll their own implementations, and this is
probably for a good reason. You would <em>like</em> to respect the user’s preferences
around tabs (macOS let’s the user choose to open new windows as tabs,
system-wide) but that will be an additional complication. I forgive you if you
skip it, but if your framework sees much use you’re going to get someone
reporting it as a bug every month until you die, and they aren’t wrong.</p>

<div>


<video id="tabs-video" width="656" height="226" controls="">

    <source src="/assets/desktop_gui/safari_tabs.mp4" type="video/mp4"/>

    Sorry, your browser doesn&#39;t support embedded videos.
</video>

<p>Difference and appearance between &#34;native&#34; tabs (in
Safari) with custom implementations in Chrome and Firefox.</p>
</div>



<p>Closely related to windows are menus; a desktop application should respect
platform conventions around window and application menus. On Windows (the
operating system family) menus are a component of the window. On macOS, the menu
is a property of the application, which is updated to reflect the commands
available for the active window. On linux, things are slightly less clear cut.
If you’re using GTK then there are both window and application menus, although
the latter <a href="https://developer.gnome.org/ApplicationMenu/">are deprecated</a>. If you’re directly targeting x11 or
wayland, you’ll need to implement menus on your own, and you can theoretically
do whatever you want, although the easy path is Windows-style window menus.</p>

<p>Generally there are <a href="https://developer.apple.com/design/human-interface-guidelines/macos/menus/menu-bar-menus/">explicit</a> <a href="https://docs.microsoft.com/en-us/windows/win32/uxguide/cmd-menus#standard-menus">conventions</a> around what
menus you should provide, and what commands should be present in them; a
well-behaved desktop application should respect these conventions.</p>

<h4 id="painting">Painting</h4>

<p>To draw the content of your app, you need (at least) a basic 2D graphics API.
This should provide the ability to fill and stroke paths (with colors, including
transparency, as well as with radial and linear gradients), to lay out text, to
draw images, to define clip regions, and to apply transformations. Ideally your
API also provides a few more advanced features such as blend modes and blurs,
for things like drop shadows.</p>

<p>These APIs exist, in subtly different form, on the various platforms. on macOS,
there is <a href="https://developer.apple.com/documentation/coregraphics">CoreGraphics</a>, on windows <a href="https://docs.microsoft.com/en-us/windows/win32/direct2d/direct2d-portal">Direct2D</a>, and on linux there is <a href="https://www.cairographics.org">Cairo</a>. One
approach, then, is to present a common API abstraction over top of these
platform APIs, puttying over the rough edges and filling in the gaps. (This is
the approach we have currently taken, with the <a href="https://docs.rs/piet">piet</a> library.)</p>

<p>This does have its downsides. These API are different enough (especially in
trickier areas, <a href="https://www.cmyr.net/blog/piet-text-work.html">such as text</a>) that designing a good abstraction
can be challenging, and requires some jumping through hoops. Subtly different
platform behaviour can cause rendering irregularities.</p>

<p>It would be simpler to just use the same renderer everywhere. One option might
be something like <a href="https://skia.org">Skia</a>, the rendering engine used in Chrome and Firefox.
This has the advantage of portability and consistency, at the cost of binary
size and compile time costs; a Rust binary using <a href="https://github.com/rust-skia/rust-skia">skia-safe</a> crate has a
baseline size of about 17M for a release build (my methodology wasn’t great for
this, but I think it’s a reasonable baseline.)</p>

<p>Skia is still a fairly traditional software renderer, although it does now have
significant GPU support. Ultimately, though, the most exciting prospects are
those that move even more of the rendering task to the GPU.</p>

<p>An initial challange here is the diversity of APIs for GPU programming, even for
identical hardware. The same physical GPU can be interfaced with via <a href="https://developer.apple.com/metal/">Metal</a>
on Apple platforms, <a href="https://docs.microsoft.com/en-us/windows/win32/direct3d12/directx-12-programming-guide">DirectX</a> on WIndows, and <a href="https://www.vulkan.org">Vulkan</a> on many other
platforms. Making code portable across these platforms requires either duplicate
implementations, some form of <a href="https://github.com/KhronosGroup/MoltenVK">cross compilation</a> or else an
<a href="https://github.com/gfx-rs/wgpu/tree/master/wgpu-hal">abstraction layer</a>. The problem with these latter cases is that it is
genuinely hard to write an abstraction that provides adequate control of
advanced GPU features (such as the compute capabilities) across subtly different
low-level APIs.</p>

<p>Once you’ve figured out how you want to talk to the hardware, you then need to
figure out how to efficiently and correctly rasterize 2D scenes on the GPU.
This is also probably more complicated than you might initially suspect. Since
GPUs are good at drawing 3D scenes, and since 3D scenes seem “more complicated”
than 2D scenes, it may feel like a natural conclusion that GPUs should handle 2D
trivially. They do not. The rasterization techniques used in 3D are poorly
suited to 2D tasks like clipping to vector paths or antialiasing, and those that
produce the best results have the worst performance. Worse, these traditional
techniques can start to perform very badly in 2D once there are lots of blend
groups or clip regions involved, since each needs its own temporary buffer and
draw call.</p>

<p>There is some promising new
work (such as <a href="https://github.com/linebender/piet-gpu">piet-gpu</a>) that use <a href="https://anteru.net/blog/2018/intro-to-compute-shaders/">compute shaders</a> and can
draw scenes in the 2D imaging model with smoothly consistent performance. This
is an area of active research. One potential limitation is that compute shaders
are a relatively new feature, and are only available in GPUs made in the last
five-or-so years. Other renderers, including <a href="https://github.com/servo/webrender">WebRender</a> as used by Firefox,
use more traditional techniques and have wider compatibility.</p>

<p>In any case, you have options, all with various trade-offs, and none of them
clearly the winner.</p>

<h4 id="animation">Animation</h4>

<p>Oh, also: whatever approach you choose, you are going to also need to provide an
ergonomic, performant animation API. It’s worth thinking about this early; it will be
annoying to try and add it in later.</p>

<h4 id="text">Text</h4>

<p>Regardless of how you paint, you are going to need to <a href="https://gankra.github.io/blah/text-hates-you/">render text</a>.
A GUI framework should at the very least support rich text, complex scripts, text
layout (including things like line breaking, alignment, and justification,
and ideally things like line-breaking within arbitrary paths). You need to
support emoji. You also need to support <a href="https://lord.io/text-editing-hates-you-too/">text editing</a>,
including support for right-to-left and <a href="https://en.wikipedia.org/wiki/Bidirectional_text">BiDi</a>. Suffice to say that this is a
very large undertaking. Realistically, you have two options: either you bundle
<a href="https://github.com/harfbuzz/harfbuzz">HarfBuzz</a>, or you use the platform text APIs: <a href="https://developer.apple.com/documentation/coretext">CoreText</a> on macOS,
<a href="https://docs.microsoft.com/en-us/windows/win32/directwrite/direct-write-portal">DirectWrite</a> on Windows, and likely <a href="https://pango.gnome.org">Pango</a> + HarfBuzz on linux. There are a
few other alternatives, including some promising Rust projects (such as
<a href="https://github.com/yeslogic/allsorts">Allsorts</a>, <a href="https://github.com/RazrFalcon/rustybuzz">rustybuzz</a>, and <a href="https://github.com/dfrg/swash">swash</a>) but none of these are quite complete
enough to fully replace HarfBuzz or the platform text APIs just yet.</p>

<h4 id="the-compositor">The compositor</h4>

<p>2D graphics are a major part of the drawing that might be done by a desktop
application, but they are not the <em>only</em> part. There are two other common cases
worth mentioning: video, and 3D graphics. In both of these cases, we want to
be able to take advantage of available hardware: for video, the hardware
<a href="https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC_products_and_implementations#Decoding">H.264 decoder</a>, and for 3D the GPU.
What this comes down to is instructing the operating system to
embed a video or 3D view in some region of our window, and this means
interacting with the <a href="https://raphlinus.github.io/ui/graphics/2020/09/13/compositor-is-evil.html">compositor</a>. The compositor is the component of the
operating system that is responsible for taking display data from various
sources (different windows from different programs, video playback, GPU output)
and assembling it into a coherent picture of your desktop.</p>

<p>Perhaps the best way to think about why this matters to us is to think about
interactions with scrolling. If you have a scrollable view, and that view
contains a video, you would like to have the video move in sync with the view’s
other content when the view is scrolled. This is harder than it sounds. You
can’t just define a region of your window and embed a video in it; you need to
somehow tell the OS to move the video in sync with your scrolling.</p>

<h4 id="web-views">Web views</h4>

<p>Let’s not forget these: sooner or later, someone is going to want to display
some HTML (or an actual website!) within their application. We’d <em>really</em> rather
not bundle an entire browser engine to accomplish this, but making use of a
platform webview <em>also</em> implicates the compositor and overall significantly
complicates our lives. Maybe your users don’t really need that web view after
all? In any case, something to think about.</p>

<h3 id="handling-input">Handling input</h3>

<p>Once you have figured out how to manage windows and how you are going to draw
your content, you need to handle user input. We can roughly divide input into
<em>pointer</em>, <em>keyboard</em>, and <em>other</em>, where <em>other</em> is stuff like joysticks,
gamepads, and other <a href="https://en.wikipedia.org/wiki/USB_human_interface_device_class">HID devices</a>. We will ignore this last
category, except to say that this would be nice to have, but doesn’t need to be
a priority.
Finally, there are input events that originate from system accessibility
features; we will deal with these when we talk about accessibility.</p>

<p>For both pointer and keyboard events, there is a relatively easy approach, and
then there is a principled, correct approach that is significantly harder to get
right.</p>

<h4 id="pointer-input">Pointer input</h4>

<p>For pointer events, the easy approach is to present an API that sends mouse events,
and then sends trackpad events in a way that makes them look like mouse events:
ignoring multiple touches, pressure, or other features of touch gestures that
do not have obvious analogs to the mouse. The <em>hard</em> approach is to implement
some equivalent of the web’s <a href="https://developer.mozilla.org/en-US/docs/Web/API/PointerEvent">PointerEvent</a> API, where you are able to fully
represent information on multi-touch (both from a trackpad as well as a
touch-sensitive display) and stylus input events.</p>

<p>Doing pointer events the easy way is…okay, assuming you can also provide
events for common trackpad gestures like pinch-to-zoom and two-finger-scroll,
without which your framework is going to immediately frustrate many users.
And while the number of applications that need or want to do advanced gesture recognition
or which expect to handle stylus input is fairly low, they certainly
exist, and a desktop application framework that does not support these cases is
fundamentally limited.</p>

<h4 id="keyboard-input">Keyboard input</h4>

<p>The situation is worse for keyboard input, in two ways: here the hard case is
both harder to do <em>and</em> doing it the ‘easy way’ is fundamentally limiting; going
the easy route means your framework is essentially useless for much of the world’s
population.</p>

<p>The easy way, for keyboard input, is very easy: the keys of a keyboard
are generally associated with a character or string, and when the user presses a key,
you can take that string and smush it in at the cursor position in the active
text field. This works <em>reasonably</em> well for unilingual English text, and
slightly-less-well-but-at-least-sort-of
for general <a href="https://en.wikipedia.org/wiki/ISO/IEC_8859-1">Latin-1</a> languages plus scripts that behave similarly to latin, such
as Greek or Cyrillic or Turkish.
Unfortunately (but not coincidentally) a large number of programmers mostly just
type ASCII, but much of the world does not. Serving <em>these</em> users requires
integrating with the platform text input and IME system, a problem that has the unfortunate
property of being both fundamentally necessary and incredibly fiddly.</p>

<p>IME stands for <a href="https://en.wikipedia.org/wiki/Input_method">Input Method Editor</a>, and is a catch-all term for the platform
specific mechanisms that convert keyboard events into text. This process is
fairly trivial for most European languages and scripts, where at most you
may need to insert an accented vowel, but it is much more complicated for the
east-Asian languages (Chinese, Japanese, and Korean, or collectively, CJK) as
well as for various other <a href="https://harfbuzz.github.io/complex-scripts.html">complex scripts</a>.</p>

<div>
<video controls="" width="446" height="282">

    <source src="/assets/desktop_gui/ime.mp4" type="video/mp4"/>

    Sorry, your browser doesn&#39;t support embedded videos.
</video>
<p>Using a Japanese IME on macOS</p>
</div>

<p>Let’s stick to CJK, for the purpose of this example. In these scripts, keyboard
events do not correspond directly to input; instead <a href="https://github.com/linebender/druid/pull/1636">keyboard events are composed</a>
together into input text as you you type, but that text may change significantly
between keystrokes, and the changes can effect not just the current character
but also text that has previously been entered.</p>

<p>This is complicated in a number of ways. Firstly, it means that the interaction
between a given text field and the IME is <em>bidirectional</em>: the IME needs to be
able to both modify the contents of the textbox, but it also needs to be able to
query the current contents of the textbox, in order to have the appropriate
context with which to interpret events. Similarly, it needs to be notified of
changes in the cursor position or selection state; the same key-press may
produce different output based on the surrounding text. Secondly, we also need
to keep the IME up-to-date on the position of the textbox on the screen, since
the IME often presents a ‘candidate’ window of possible inputs for the active
sequence of keyboard events. Finally (and not like <em>actually</em> finally, just that
I’m three thousand words in to this and not nearly done yet) implementing IME in
a cross-platform way is significantly complicated by the differences in the
underlying platform APIs; macOS requires editable text fields to
<a href="https://developer.apple.com/documentation/appkit/nstextinputclient">implement a protocol</a>, and then lets the text field handle accepting
and applying changes from the IME, whereas the <a href="https://docs.microsoft.com/en-us/windows/win32/intl/input-method-manager-reference">Windows API</a> uses a
<a href="https://docs.microsoft.com/en-us/windows/win32/api/imm/nf-imm-immgetcontext">lock and release</a> mechanism; designing an abstraction over both
of these approaches is an additional layer of complexity.</p>

<p>There’s one additional complication related to text input: on macOS,
you need to support the <a href="https://www.hcs.harvard.edu/~jrus/site/system-bindings.html">Cocoa Text System</a>, which allows the user
to specify system-wide keybindings that can issue a variety of text editing and
navigation commands.</p>

<p>To summarize: handling input correctly is a lot of work, and if you don’t do it
your framework is basically a toy.</p>

<h3 id="accessibility">Accessibility</h3>

<p>A desktop application framework has to support native accessibility APIs, and
should ideally do this in a way that does not require special thought or work
from the application developer. Accessibility is a catchall term for a large
number of assistive technologies; the most crucial being support for
<a href="https://www.youtube.com/watch?v=Jao3s_CwdRU">screen readers</a> and assisted navigation. Screen reader support means
interoperating with platform APIs that describe the structure and contents of
your application, and <a href="https://developer.apple.com/videos/play/wwdc2021/10120/">assisted navigation</a> means providing a method of moving
between elements on the screen linearly, allowing elements to be highlighted,
described and activated in turn using a keyboard or joystick.</p>

<p>In addition to these core features, your framework should also respect the
user’s system-level preferences regarding things like text size, reduced color
contrast, and reduced animation. Related, but not accessibility, exactly: you
would like to support dark mode, as well as things like a user-chosen accent
colour.</p>

<h3 id="internationalization-and-localization">Internationalization and Localization</h3>

<p>Your framework should support internationalization. The most obvious component
of this is localization of strings, but it also includes things like mirroring
interfaces in right-to-left locales. Additionally, information like times,
dates, currency units, calendar units, names, sequences, and general
<a href="https://developer.apple.com/videos/play/wwdc2020/10160/">formatting of numerical data</a> should respect the user’s
locale. If this is not a problem you have thought about before, then it is
almost certainly more complicated than you imagine. But don’t worry:
<a href="https://www.unicode.org/reports/tr35/">there’s a standard</a>. All you need to do is implement it.</p>

<h3 id="other-common-features">Other common features</h3>

<ul>
  <li><strong>Copy/paste &amp; drag-and-drop</strong>: These overlap, although drag-and-drop is more
complicated. For copy/paste, you want to support not just text, but also other
<a href="https://docs.microsoft.com/en-us/windows/win32/dataxchg/standard-clipboard-formats">standard formats</a>, and additionally you need to
support user defined formats. For paste, you need to let the user inspect the
clipboard, see the available formats, and retrieve the data. Fun fact: on
macOS and Windows the API to retrieve data from the clipboard is synchronous,
and on x11 it is async. Have fun. For drag and drop, hopefully you can reuse
some of the work you did when you reimplemented window tabs?</li>
  <li><strong>Printing</strong>: <em>printing</em>? Who needs <em>printing</em>?? Well: your users, unfortunately.
Don’t worry, it’s probably not that hard.</li>
  <li>App resumption and window restoration: you’re going to want to remember where
the user’s windows were, and put them back when you relaunch. I hope they
didn’t unplug a monitor.</li>
  <li><strong>Assets and app packaging</strong>: You’re going to want to let the user bundle up their
application. This means doing things like generating your app’s
<a href="https://developer.apple.com/library/archive/documentation/General/Reference/InfoPlistKeyReference/Introduction/Introduction.html">manifest</a>, validating required assets like app icons, and
localization data, and making these things available at runtime per the
conventions of the target platform.</li>
  <li><strong>Async</strong> You <em>do</em> have nice ergonomic async support, don’t you?</li>
</ul>

<h4 id="and-other-less-common-features">And other less common features</h4>

<p>In addition to all of the features that are shared across most desktop
environments, there are also platform-specific features to be thought about:
some of these are stylistic things, like APIs to add
<a href="https://developer.apple.com/documentation/appkit/nsvisualeffectview">transparency or vibrancy</a> to some part of your window; or
support for adding a <a href="https://developer.apple.com/design/human-interface-guidelines/macos/extensions/menu-bar-extras/">menu bar extra</a> or working with <a href="https://docs.microsoft.com/en-us/windows/win32/shell/taskbar-extensions">task bar extensions</a>,
or <a href="https://developer.apple.com/design/human-interface-guidelines/macos/system-capabilities/quick-look/">quick look</a>, or implementing a <a href="https://docs.microsoft.com/en-us/windows/win32/shell/control-panel-applications">control panel item</a>,
or any number of other things. Your framework should at least make these things
<em>possible</em>. At the very least, you should provide opportunities for the user to
drop down and work with the platform APIs directly, so that they
have some escape hatch available for when they really need to achieve something
that you haven’t foreseen (or gotten around to yet).</p>

<h2 id="putting-it-all-together">Putting it all together</h2>

<p>That feels like a reasonable place to stop; there are certainly things I’ve
overlooked, but I hope I’ve touched on the most significant ones. Once you have
an idea of the things you need to support and implement, you can start thinking
about how to fit it all together.</p>

<h3 id="designing-cross-platform-apis">Designing cross-platform APIs</h3>

<p>One of the more subtle and interesting challenges of designing your GUI
framework is designing the API. Here, you face a very particular problem: you
are attempting to design an API that provides a common interface for a set of
underlying platform APIs that are fundamentally different.</p>

<p>A nice example is around your application’s menus. As mentioned earlier, linux
and Windows generally expect a menu bar to exist on your app’s individual
windows, whereas macOS has a single menu bar that is a component of the desktop
environment, and which becomes your application menu when your application is
active.</p>

<p>To handle this naively, you might have separate ‘application’ and ‘window’
menus, and then you might have conditional code to update one or the other
based on conditional compilation or runtime checks. This ends up being a lot of
duplicate code, however, and it will be easy to get wrong. In this particular
case, I think there is a fairly clear, fairly simple API that works on both
platforms. In your framework, you treat menus as being a property of the window:
on Windows and Linux this is actually the case, so that’s fine, and then on
macOS you set the application menu to be the menu of the currently active
window, changing it as needed when windows gain or lose active status.</p>

<p>This is a fairly clean example, and many other APIs are not so clear cut. In
general, designing these cross-platform APIs is a process of carefully reading
through and experimenting with the platform-specific APIs, and then trying to
identify the set of shared features and functionality that you can express in
the abstraction above; and when no cleanly shared set of features exist, it
means coming up with some <em>other</em> API that can at least be implemented in terms
of what is provided by the platform.</p>

<h3 id="the-seduction-of-the-web-view">The seduction of the web view</h3>

<p>All of this platform complexity, with all of its subtle design flaws, missing
documentation, and mysterious bugs, has already been worked around successfully
by a few major cross-platform GUI frameworks: the major browsers,
Chrome and FireFox. (Safari and Edge don’t need to worry about this, because
they aren’t cross-platform.)</p>

<p>The browsers have had to figure all of this out: the child windows, the text
input, the <em>accessibility</em>,
the font fallback, the compositor, the performant painting, the drag and drop…it’s
all there.</p>

<p>If you’d like to do something cross-platform, then, there is a very natural and
very understandable impulse to reach for web technologies, either by creating a
real web app that runs in the browser, or else by leaning on the browser engine
and using it to render your UI in a native window, à la <a href="https://www.electronjs.org">Electron</a>. This does
come with obvious drawbacks, particularly around performance (on various axes,
such as application size and memory consumption) as well as ‘look and feel’ (on
which we’ll expand shortly) but it sure does make life a lot simpler, and the
more time I spend working on projects in this space, the more sympathetic I
become to folks who choose the browser side of this trade-off.</p>

<h3 id="on-native-look-and-feel">On “native look and feel”</h3>

<p>Something that comes up frequently in discussions of cross-platform GUI work is
a collection of things I’ll refer to as “native look and feel”. This is vague,
and I think it’s helpful to split it in two: <em>native behaviour and convention</em>,
and <em>native appearance</em> (although these can overlap.)</p>

<p><strong>Native behaviour</strong> refers to many of the things we have already discussed, and
some other things besides. Some examples would be scroll behaviour: does your
application respect the user’s scroll preferences? Does your application have
the same acceleration curves when scrolling as the default platform scroll
views? Does your application handle standard system keyboard shortcuts, for
instance for maximizing or hiding a window? Does IME work? This extends to other
less obvious conventions, as well: does the application store user data in the
locations that are conventional to the current platform? Does it use the system
file open/save dialogs? Does it show expected menus, containing expected menu
items?</p>

<p>These things are more important on some platforms than on others. On the Mac, in
particular, getting these behavioural details correct is important: the Mac more
than other platforms is designed around <a href="https://developer.apple.com/design/human-interface-guidelines/">specific conventions</a>, and Mac
application developers have historically been diligent about respecting these.
This in turn has helped create a community of users who value these conventions
and are <a href="https://www.sketch.com/blog/2020/10/26/part-of-your-world-why-we-re-proud-to-build-a-truly-native-mac-app/">sensitive to them</a>, and breaking from them is bound to
<a href="https://daringfireball.net/linked/2020/03/20/mac-assed-mac-apps">upset this cohort</a>.
On Windows, things are slightly more relaxed; there has historically been a
greater diversity of software on Windows, and Microsoft has never been quite as
dogmatic as Apple has been about how an application should look and behave.</p>

<p><strong>Native appearance</strong> refers more to how an application looks. Do your buttons
look like native buttons? Do they have the same sizing and gradients? Do you
more generally use the controls a platform expects for a given interaction, for
instance preferring a checkbox on desktop but a toggle on mobile?</p>

<p>This is additionally complicated by the fact that ‘native appearance’ changes
between not just platforms but also OS releases, to the point where looking
‘native’ on a given machine would require runtime detection of the OS version.</p>

<p>While all of this is <em>possible</em>, it is starting to add a huge amount of
additional work, and for a modestly staffed project this can be hard to justify.
For that reason, I am personally forgiving of a project that moves away from
trying to do pixel-perfect replication of the platform’s built-in widgets, in
favour of just trying to do something tasteful and coherent, while providing the
tools necessary for the framework’s users to style things as needed.</p>

<h3 id="fin">Fin</h3>

<p>I hope this catalog has helped at least vaguely define the scope of the problem.
None of the things I have described here are impossible, but doing them all, and
doing them <em>well</em>, is quite a bit of work.</p>

<p>This last point is worth ending on: for this work to be useful, it is not enough
that it <em>exist</em>. If you would like people to use your framework, you are going
to have to make it attractive to them: providing a good API that is easy to use,
that is idiomatic in the host language, that is well documented, and that lets
them solve their actual problems.</p>


      </article>

    </div></div>]]></content:encoded>
      <pubDate>Tue, 10 Aug 2021 11:29:51 +0000</pubDate>
      <source>https://www.cmyr.net/blog/gui-framework-ingredients.html</source>
    </item>
    <item>
      <title>Show HN: KmCaster – display keyboard and mouse events on-screen</title>
      <link>https://github.com/DaveJarvis/kmcaster</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
          <article itemprop="text">
<p>Java-based on-screen display (OSD) for keyboard and mouse events.</p>
<p>This program displays keyboard and mouse events for the purpose of screencasting. While such software already exists, none meet all the following criteria:</p>
<ul>
<li>custom display size;</li>
<li>easily positioned;</li>
<li>show single events;</li>
<li>show all mouse clicks;</li>
<li>show scrolling;</li>
<li>accurate modifier key states; and</li>
<li>works with emulation software (e.g., <a href="http://sikulix.com/" rel="nofollow">Sikuli</a>).</li>
</ul>
<h2><a id="user-content-alternatives" aria-hidden="true" href="#alternatives"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Alternatives</h2>
<ul>
<li><a href="https://github.com/ctrlcctrlv/QKeysOnScreen">QKeysOnScreen</a></li>
<li><a href="https://www.thregr.org/~wavexx/software/screenkey" rel="nofollow">screenkey</a></li>
</ul>

<p>The following video compares KmCaster to <a href="https://github.com/critiqjo/key-mon">key-mon</a>:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/DaveJarvis/kmcaster/blob/master/images/kmcaster-01.gif"></a></p>

<p><a href="https://bell-sw.com/pages/downloads/#/java-14-current" rel="nofollow">OpenJDK</a> version 14.0.1 or newer.</p>
<h2><a id="user-content-linux-java-version" aria-hidden="true" href="#linux-java-version"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Linux Java Version</h2>
<p>Depending on the Linux distribution, Java 14 can be installed by issuing one of the following commands in a terminal:</p>
<div data-snippet-clipboard-copy-content="sudo apt install openjdk-14-jdk
sudo pacman -S jdk-openjdk
"><pre><code>sudo apt install openjdk-14-jdk
sudo pacman -S jdk-openjdk
</code></pre></div>
<p>Switching from earlier versions of Java to Java 14 can be accomplished by issuing one of the following commands in a terminal:</p>
<div data-snippet-clipboard-copy-content="sudo update-alternatives --config java
sudo archlinux-java set java-14-openjdk
"><pre><code>sudo update-alternatives --config java
sudo archlinux-java set java-14-openjdk
</code></pre></div>
<p>Note: on some Linux operating systems you may need to add a repository.
On Ubuntu 18.04 run the following commands before trying to install Java 15, for example (<a href="http://ubuntuhandbook.org/index.php/2020/03/install-oracle-java-14-ubuntu-18-04-20-04/" rel="nofollow">source</a>):</p>
<div data-snippet-clipboard-copy-content="sudo add-apt-repository ppa:linuxuprising/java
sudo apt install openjdk-15-jdk
"><pre><code>sudo add-apt-repository ppa:linuxuprising/java
sudo apt install openjdk-15-jdk
</code></pre></div>

<p>Download the latest Java Archive file:</p>
<p><a href="https://gitreleases.dev/gh/DaveJarvis/kmcaster/latest/kmcaster.jar" rel="nofollow">Download</a></p>

<p>After installing Java, run the program as follows:</p>

<p>To see the configuration options, run the program as follows:</p>
<div data-snippet-clipboard-copy-content="java -jar kmcaster.jar -h
"><pre>java -jar kmcaster.jar -h</pre></div>
<p>To quit the application:</p>
<ol>
<li>Click the application to give it focus.</li>
<li>Press <code>Alt+F4</code> to exit.</li>
</ol>
<h2><a id="user-content-error-messages" aria-hidden="true" href="#error-messages"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Error Messages</h2>
<p>Java version 14 is required; earlier versions will display the following
message:</p>
<blockquote>
<p>Error: A JNI error has occurred, please check your installation and try again.</p>
</blockquote>
</article>
        </div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 15:39:11 +0000</pubDate>
      <source>https://github.com/DaveJarvis/kmcaster</source>
    </item>
    <item>
      <title>Plants, Heavy Metals, and the Lingering Scars of World War I</title>
      <link>https://www.atlasobscura.com/articles/zone-rouge-plant-growth</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
<section id="article-body">

<p><span>In 1919, all along the </span>Western Front, French authorities were taking stock of the devastation of World War I. From the border with Belgium at Lille to the border with Switzerland near Strasbourg, this most brutal of wars had torn a rupture through the land: It was ripped, cratered, pitted, charred by a billion artillery shells fired over four years. “Where there are no dead,” wrote Henri Barbusse, “the earth itself is corpselike.” A Frankenstein landscape, stitched and stapled together, which harbored in its flesh millions of tons of unexploded munitions and chemical weapons enough to kill an army, all over again.</p>
<p>Economically hobbled by war, and awed by the scale of the problem facing them, the French authorities developed a triage system: They surveyed the <em>régions dévastées</em> and drew up a series of maps that charted areas believed to be devastated beyond repair. In total, more than 460 square miles were classified in this way, shaded with a red pencil and declared no‑go areas. Though over the following decades, this total was much reduced, but near Verdun, where the land was choppier, steeper, more remote, and the damage total—a “biological desert,” as the French botanist Georges H. Parent described it—much remains off-limits.</p>
<figure><figcaption>French infantry, Fort Vaux, Verdun, June 1916. <span>Underwood Archives/Getty Images</span></figcaption></figure>
<p>After a period of uncertainty, it was decided to plant a shroud of trees over the war zone—a living sarcophagus that might stabilize the soil and contain the terrors within for a generation or more. A forest of forgetfulness. Where the land was worst affected—soil stripped almost to the bedrock—they planted black pines, one of the only hardy species that could thrive there. So it remains today. They call it the Zone Rouge.</p>
<p>For a hundred years a forest grew up across the land, tall and dark and impenetrable, whose undergrowth curled and snarled into a thicket of bramble and blackthorn. In the Zone Rouge, however, there is a place where the trees never grew back: a clearing in the woods, where the oaks and hornbeams part to reveal a small round pool of what appears to be gray gravel, or tar, or ash. A swatch of ground where nothing will grow.</p>
<p>The secret to this sterile wound lies also in the decisions made after the war. At armistice, millions of unused shells lay piled up, ready to be fired. It wasn’t clear what should be done with these surplus weapons of mass destruction. At Verdun, the decision was made to recover what ammunition they could at the military camps, but to gather chemical weapons—200,000 of them—at a farm near Gremilly. Here was mustered an array of the most unpleasant hexes one man can cast upon another: mustard gas, tear gas, phosgene—whose pleasant odor of freshly mown hay belies its deadly consequences—the sneezing gas diphenyl-chloroarsine, the garlic‑scented vomiting agent diphenylcyano-arsine. Then, finally, in 1928, they dug trenches as if for a mass grave, piled in the canisters, and set them ablaze. Hence the name: la Place à Gaz, the Place of Gas.</p>
<p>The clouds of arsine gases produced during the burn poisoned the land and left it bare. It appears like tundra, or melted tarmac: waste ground of the very purest kind.</p>
<figure><figcaption>La Place à Gaz, Spincourt Forest, France, 2014. <span>Courtesy © BRGM – Daniel Hube </span></figcaption></figure>
<hr/>
<p><span>Many of the so-called heavy </span>metals—in this case used as kind of a catch-all term for cobalt, copper, iron, nickel, zinc, and others—are essential to the fundamental processes of life, but in quantity they become toxic. When plants come into contact with metal‑tainted soil, strange things can happen.</p>
<p>In the 1950s, the Russian naturalist N. G. Nesvetaylova discovered that it was possible to turn poppies different colors by the adding of various metal salts to compost: Zinc compounds produced flowers of lemon yellow, for example, whereas boron turned their leaves dark green. Copper, on the other hand, produced pale, blueish, “dove‑colored” leaves. (In this way a gardener with fairy godmother aspirations might sprinkle manganese on the soil beneath an almond tree to turn their flowers’ corollae from white to pink; aluminum sulfate over the roots of a hydrangea will turn its cotton-candy heads mauve, then indigo, then baby blue.) And there was a combinatory, witches’ brew aspect to the process: Two or more salts added together, like a tincture, and the flowers would take on unexpected new shades, wholly different from those seen when the metals were added separately.</p>
<p>Large-mouth poppies (<em>Papaver macrostomum</em>), common to the Middle East and Kashmir, develop double‑decker petals when growing in high-zinc soils, while the ladybird poppy (<em>P. commutatum</em>) of the Caucasus alter the pattern of their spots in response to copper-molybdenum. In the areas of greatest mineralization, their dark spots elongate until they meet at the center to form a cross—X marks the spot—a signpost to the contents of the underworld.</p>
<figure><figcaption>Caucasian scarlet poppy of ladybird poppy (<em>Papaver commutatum</em>) growing on waste ground in Greece. <span>FlowerPhotos/Universal Images Group via Getty Images</span></figcaption></figure>
<p>Plants growing in the vicinity of manganese might boom obscenely in size, reaching gigantic proportions with luxuriant greenery. Copper sulfate or chromite will produce dwarfs. Symptoms such as these have been used successfully by prospectors across the globe for centuries as “bioindicators” of minerals in the soil below. As explorers once scanned their environment for willow or cottonwood to find water in the desert, prospectors raked the landscape for plants displaying chlorosis, the floral equivalent of anemia, which shows up as a bleaching or fading of the leaves, except along their darker veins, which stand in dramatic silhouette.</p>
<p>Better yet, they might spot plants whose presence alone signaled valuable metals. Early Scandinavian miners, for example, were guided to their target by the <em>kobberblomst</em> (copper flower) and the <em>kisplante</em> (pyrite plant, or <em>Lychnis alpine</em>), tiny pink‑flowering campion whose delicacy belies an extreme hardiness, an ability to thrive where sometimes no other species can.</p>
<p>By the sixth century, the Imperial Chinese were already sensitive to the potential of metal‑loving plants as a means of prospecting, producing detailed manuals listing different species and their mineral familiars, and the symptomatology associated with specific metals, their instructions sounding with the prosody of incantation, of summoning spells. (“If leaves … are green, and the stalks red, much lead will be found below …”)</p>
<figure><figcaption>Nodding thread moss (<em>Pohlia nutans</em>). <span> Marina Andriichuk/Alamy</span></figcaption></figure>
<p>In such way, the expert geobotanist can glean a great deal of complex information from flora. In the Copperbelt Province of Zambia, for example, at least 27 flowers live almost exclusively in soils tainted with copper and cobalt: the thicker the pelt of these flowers, the greater the degree of mineralization. Similarly, in the Alps one might learn to predict the presence and concentration of zinc by the depth of color of the flowers of the tiny, lemon-yellow calamine violet. In Australia, two plants—a type of flowering pea (<em>Tephrosia</em>) and the paper-petaled herb <em>Polycarpaea spirostylis</em>—come together to form a map with contours: the <em>Tephrosia </em>tracing the outer edges of copper deposits, but giving way to the flag-waving <em>Polycarpaea</em> wherever the copper reaches more than 2,000 parts per million.</p>
<p>Some regions are so profoundly impacted by the presence of concentrated metal ores that even these rare “metallophytes” cannot get by. Such areas, denuded of vegetation, might appear as a sickly pasture that pockmarks otherwise lush, forested areas. Platinum has been found under bald spots in the Urals and South Africa; in Russia, boron. Folk stories spring up around such oddities: One such spot in North Carolina is known as the <a href="https://www.atlasobscura.com/places/devils-stomping-ground">“devil’s trampin ground,”</a> the source of its barrenness yet to be determined, and may predate human presence in the region. But for heavy metal contamination to be so extreme as to render a place barren dirt is rare.</p>
<p>La Place à Gaz is such a place.</p>
<figure><figcaption>La Place à Gaz, Spincourt Forest, France, 2014. <span>Courtesy © BRGM – Daniel Hube </span></figcaption></figure>
<hr/>
<p><span>Those plants that grow as </span>a wan halo around the poison ashes of la Place à Gaz: It was those I was here to see. On first sight, they seemed disarmingly familiar: the haze of what is called tufted grass and the Americans call “velvet” grass for its peach-fuzz leaves—common to marshland, verges, neglected waste ground—and, hidden beneath, like an underfur, the powdered goblet lichen <em>Cladonia fimbriata</em>. Neither are exotic species. But plants like these are specially adapted to survive in what would be otherwise be a dangerous environment. They limit their intake of the metals, preventing a build‑up to toxic levels in their bodies.</p>
<p>Their neighbor, though, a soft and feathery moss known as <em>Pohlia nutans</em> (“nodding thread moss,” after their tiny, many-headed fronds), uses a more complex strategy: Rather than close itself off to the metals in the soil, it throws open the doors, transporting metal salts upward into its limbs and stashing them away. Magpie plants of this kind are known as “hyperaccumulators,” and it’s not totally understood why they do this. It may be a form of self‑ defense: making themselves bitter herbs, to deter grazing animals. The effect, though, can be extraordinary. For example: The <em>Pycnandra acuminata</em> is a silver sylph of a tree that grows in the misty rainforests of New Caledonia. When cut with a knife, it bleeds a spectacular latex sap the color of verdigris, containing up to 26 percent nickel. In postindustrial mining regions of Wales, lichens soak up iron or copper from the rocks they grow on, turning rust-orange or turquoise in the process—the spattered paint of an artists’ studio—and rendering the metals insoluble, and therefore harmless.</p>
<p>Though metallophyte species like these have evolved naturally, finding toeholds in outcrops of metal ores and at sites like the Tantramar “copper swamp” of Canada’s New Brunswick, they are now much more likely to be found in human-impacted ones. Mine tailings, spoil heaps, slag tips, postindustrial sites of many kinds—and postconflict ones too, like la Place à Gaz. There has been an exponential growth in land despoiled by heavy metals over recent decades. Globally, more than five million such contaminated sites have been reported; more than 300,000 square miles of contaminated soil in China alone.</p>
<figure><figcaption>Powdered goblet lichen (<em>Cladonia fimbriata</em>). <span>Arterra/Universal Images Group via Getty Images</span></figcaption></figure>
<p>Knowledge and appreciation of such plants has grown in recent years, but their love of despoiled landscapes gives conservationists pause when it comes to protecting rare and unusual species. Near Swansea, in southern Wales, I once visited the former site of “Copperopolis”—an enormous complex of smelting furnaces built during the 17th and 18th centuries, which left the Lower Swansea valley a lunar landscape besmirched with lead, chromium, and copper in the years after the industry’s collapse. This scrappy wasteland, however, had recently been designated a Site of Special Scientific Interest thanks to its unusual assemblage of metallophyte plants and lichens—known as “calaminarian grassland” after the zinc ore—and brought under new environmental protections. Disturbance, though, turned out to have been very good for the area; despite the best of intentions, an early “remediation scheme,” which removed or capped metal-polluted soil, had reduced the habitat of rare plants such as the star-flowered spring sandwort, and conservationists were now considering radical, counterintuitive management methods—such as scraping up the topsoil to retoxify the ground.</p>
<p>Because of their strange and beguiling qualities, metal hyperaccumulators—of which there are known to be around 500—are of enormous scientific interest. Thanks to their thirst for otherwise toxic materials, they have great potential as tools in the recovery of highly polluted sites. By sucking heavy metals from the earth and hoarding or redistributing them, they might prepare the ground for other, more sensitive organisms. In this way, nature begins to heal over her scars.</p>
<figure><figcaption>An unexploded shell in the Zone Rouge. <span> ian alderman/Alamy</span></figcaption></figure>
<p>A field of study, phytoremediation, has grown up around hyperaccumulating plants. It seeks to harness their surreal kind of superpowers for the greater good. Other species include the brake fern, which removes arsenic from the soil and stores it in its fronds (and is being tested as a natural filter for contaminated water in Bangladesh, following a decades-long arsenic-poisoning crisis), and sunflowers, which accumulate a wide range of heavy metals and are grown on sites of former mines and smelters in Australia. It’s a slow process; the plants must grow and then be harvested—and their bodies, now containing high concentrations of the heavy metals, disposed of carefully—but it can be faster and certainly less environmentally damaging than current clean-up methods: excavation and reburial under a concrete cap.</p>
<p>Already I could see this process at work. In la Place à Gaz, the bare surface of chemical ash had clearly declined since a 2007 study by German scientists, and perhaps even since a French follow-up paper in 2016, which noted with relief “progressive revegetation of the site.” Whatever these plants were doing—and particularly the nodding thread moss—was slowly turning the chemical burn in the landscape into a habitable place to grow.</p>
</section>



</div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 08:48:38 +0000</pubDate>
      <source>https://www.atlasobscura.com/articles/zone-rouge-plant-growth</source>
    </item>
    <item>
      <title>AllStar: Continuous Security Policy Enforcement for GitHub Projects</title>
      <link>https://security.googleblog.com/2021/08/allstar-continuous-security-policy.html</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
<div>
<div id="header"><div data-version="1" id="Header1">
<div>
<div>
<p><a href="https://security.googleblog.com/">

</a></p><a href="https://security.googleblog.com/">
<h2>
            Security Blog
          </h2>
</a>
</div>
<p>
The latest news and insights from Google on security and safety on the Internet
</p>
</div>
</div></div>
</div>
</div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 13:04:00 +0000</pubDate>
      <source>https://security.googleblog.com/2021/08/allstar-continuous-security-policy.html</source>
    </item>
    <item>
      <title>Kind: A Modern Proof Language</title>
      <link>https://github.com/uwu-tech/kind</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
          <article itemprop="text">
<p>A minimal, efficient and practical proof and programming language. Under the hoods, it is basically Haskell, except purer and with dependent types. That means it can handle mathematical theorems just like Coq, Idris, Lean and Agda. On the surface, it aims to be more practical and looks more like TypeScript. Compared to other proof assistants, Kind has:</p>
<ol>
<li>
<p>The smallest core. Check <a href="https://github.com/moonad/FormCoreJS/blob/master/FormCore.js">FormCore.js</a> or <a href="https://github.com/uwu-tech/Kind/blob/master/base/Kind/Core.kind">Core.kind</a>. Both are <code>&lt; 1000-LOC</code> complete implementations!</p>
</li>
<li>
<p>Novel type-level features. Check <a href="https://github.com/uwu-tech/Kind/blob/master/blog/1-beyond-inductive-datatypes.md">this article</a> on super-inductive datatypes.</p>
</li>
<li>
<p>An accessible syntax that makes it less scary. Check <a href="https://github.com/uwu-tech/Kind/blob/master/SYNTAX.md">SYNTAX.md</a>.</p>
</li>
<li>
<p>A complete bootstrap: the language is implemented in itself. Check it <a href="https://github.com/uwu-tech/Kind/tree/master/base/Kind">here</a>.</p>
</li>
<li>
<p>Efficient real-world compilers. Check <a href="http://uwu.tech" rel="nofollow">http://uwu.tech/</a> for a list of apps. (WIP)</p>
</li>
</ol>
<h2><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/433259f5afbee51bef67116aff867390e273626cd2b96334b32fe372452b8934/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f6b696e642d6c616e67"></a>  <a href="https://t.me/formality_lang" rel="nofollow"></a></p>
<ol start="0">
<li>
<p>Choose a release. We&#39;ll use JavaScript here but ChezScheme is also <a href="https://github.com/uwu-tech/Kind/blob/master/INSTALL.md">available</a>.</p>
</li>
<li>
<p>Install Kind using <code>npm</code>:</p>
</li>
</ol>

<ol start="2">
<li>Save the file below as <code>Main.kind</code>:</li>
</ol>
<div data-snippet-clipboard-copy-content="Main: IO(Unit)
  IO {
    IO.print(&#34;Hello, world!&#34;)
  }
"><pre><code>Main: IO(Unit)
  IO {
    IO.print(&#34;Hello, world!&#34;)
  }
</code></pre></div>
<ol start="3">
<li>Type-check it:</li>
</ol>

<ol start="4">
<li>Run it:</li>
</ol>

<ol start="5">
<li>Have fun!</li>
</ol>
<h2><a id="user-content-things-you-can-do-with-kind" aria-hidden="true" href="#things-you-can-do-with-kind"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Things you can do with Kind:</h2>
<h3><a id="user-content-compile-programs-and-modules-to-several-targets" aria-hidden="true" href="#compile-programs-and-modules-to-several-targets"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Compile programs and modules to several targets.</h3>
<p>Kind has an universal compiler that targets several back-ends. Just find what you need on Kind, and compile it with <code>kind Main --lang</code>. For example, to generate a QuickSort function in JavaScript, just type <code>kind List.quicksort --js</code>. You may never write code in any other language! Available targets: <code>--js</code>, <code>--scm</code>. Several more will be available this month.</p>
<h3><a id="user-content-create-live-applications" aria-hidden="true" href="#create-live-applications"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Create live applications.</h3>
<p>Kind has an interconnected back-end that allows you to create rich, interactive applications without ever touching databases, TCP packets or messing with apis. Just add a file to <code>base/App</code> and it will be available on <a href="http://uwu.tech" rel="nofollow">http://uwu.tech/</a>. You can fork entire applications - not just the front-end, but all of it, back-end, database, and networking - in seconds.</p>
<h3><a id="user-content-prove-theorems" aria-hidden="true" href="#prove-theorems"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Prove theorems.</h3>
<p>No, theorems are not scary things mathematicians do. For programmers, they&#39;re more like unit tests, except they can involve symbols, allowing you to cover infinitely many test cases. If you like unit tests, you&#39;ll love theorems. To learn more, check <a href="https://github.com/uwu-tech/Kind/blob/master/THEOREMS.md">THEOREMS.md</a>. You can also compile Kind programs and proofs to a minuscle core language with the <code>--fmc</code> flag (example: <code>kind Nat.add.assoc --fmc</code>). Try it!</p>
<h3><a id="user-content-deploy-smart-contracts" aria-hidden="true" href="#deploy-smart-contracts"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deploy Smart-Contracts.</h3>
<p>(TODO) <em>Ethereum: we&#39;re coming for you.</em></p>
<h2><a id="user-content-examples" aria-hidden="true" href="#examples"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<h3><a id="user-content-some-programs" aria-hidden="true" href="#some-programs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Some programs</h3>
<div data-snippet-clipboard-copy-content="// A &#39;Hello, world!&#34;
Main: IO(Unit)
  IO {
    IO.print(&#34;Hello, world!&#34;)
  }
"><pre><span>// A &#39;Hello, world!&#34;</span>
Main: <span>IO</span><span>(</span><span>Unit</span><span>)</span>
  <span>IO</span><span></span> <span>{</span>
    <span>IO</span><span>.</span><span>print</span><span>(</span><span>&#34;Hello, world!&#34;</span><span>)</span>
  <span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// Quicksort (using recursion)
quicksort(list: List&lt;Nat&gt;): List&lt;Nat&gt;
  case list {
    nil:
      []
    cons:
      fst = list.head
      min = filter!((x) x &lt;? list.head, list.tail)
      max = filter!((x) x &gt;? list.head, list.tail)
      quicksort(min) ++ [fst] ++ quicksort(max)
  }
"><pre><span>// Quicksort (using recursion)</span>
<span>quicksort</span><span>(</span><span>list</span>: <span>List</span><span>&lt;</span><span>Nat</span><span>&gt;</span><span>)</span>: <span>List</span><span>&lt;</span><span>Nat</span><span>&gt;</span>
  <span>case</span> <span>list</span> <span>{</span>
    <span>nil</span>:
      <span>[</span><span>]</span>
    <span>cons</span>:
      <span>fst</span> <span>=</span> <span>list</span><span>.</span><span>head</span>
      <span>min</span> <span>=</span> <span>filter</span><span>!</span><span>(</span><span>(</span><span>x</span><span>)</span> <span>x</span> <span>&lt;</span>? <span>list</span><span>.</span><span>head</span><span>,</span> <span>list</span><span>.</span><span>tail</span><span>)</span>
      <span>max</span> <span>=</span> <span>filter</span><span>!</span><span>(</span><span>(</span><span>x</span><span>)</span> <span>x</span> <span>&gt;</span>? <span>list</span><span>.</span><span>head</span><span>,</span> <span>list</span><span>.</span><span>tail</span><span>)</span>
      <span>quicksort</span><span>(</span><span>min</span><span>)</span> <span>++</span> <span>[</span><span>fst</span><span>]</span> <span>++</span> <span>quicksort</span><span>(</span><span>max</span><span>)</span>
  <span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// List iteration (using folds)
some_text: String
  List.foldl!!(&#34;&#34;,
    (str, result) 
      str = String.to_upper(str)
      str = String.reverse(str)
      result | str,
    [&#34;cba&#34;,&#34;fed&#34;,&#34;ihg&#34;])
"><pre><span>// List iteration (using folds)</span>
some_text: <span>String</span>
  <span>List</span><span>.</span><span>foldl</span><span></span><span>!</span><span>!</span><span>(</span><span>&#34;&#34;</span><span>,</span>
    <span>(</span><span>str</span><span>,</span> <span>result</span><span>)</span> 
      <span>str</span> <span>=</span> <span>String</span><span>.</span><span>to_upper</span><span>(</span><span>str</span><span>)</span>
      <span>str</span> <span>=</span> <span>String</span><span>.</span><span>reverse</span><span>(</span><span>str</span><span>)</span>
      <span>result</span> <span>|</span> <span>str</span><span>,</span>
    <span>[</span><span>&#34;cba&#34;</span><span>,</span><span>&#34;fed&#34;</span><span>,</span><span>&#34;ihg&#34;</span><span>]</span><span>)</span></pre></div>
<div data-snippet-clipboard-copy-content="// List iteration (using fors)
some_text: String
  result = &#34;&#34;
  for str in [&#34;cba&#34;,&#34;fed&#34;,&#34;ihg&#34;] with result:
    str = String.to_upper(str)
    str = String.reverse(str)
    result | str
  result
"><pre><span>// List iteration (using fors)</span>
some_text: <span>String</span>
  <span>result</span> <span>=</span> <span>&#34;&#34;</span>
  <span>for</span> <span>str</span> <span>in</span> <span>[</span><span>&#34;cba&#34;</span><span>,</span><span>&#34;fed&#34;</span><span>,</span><span>&#34;ihg&#34;</span><span>]</span> <span>with</span> <span>result</span>:
    <span>str</span> <span>=</span> <span>String</span><span>.</span><span>to_upper</span><span>(</span><span>str</span><span>)</span>
    <span>str</span> <span>=</span> <span>String</span><span>.</span><span>reverse</span><span>(</span><span>str</span><span>)</span>
    <span>result</span> <span>|</span> <span>str</span>
  <span>result</span></pre></div>
<div data-snippet-clipboard-copy-content="// Map, Maybe, String and Nat sugars
sugars: Nat
  key  = &#34;toe&#34;
  map  = {&#34;tic&#34;: 1, &#34;tac&#34;: 2, key: 3} // Map.from_list!([{&#34;tic&#34;,1}, ...])
  map  = map{&#34;tic&#34;} &lt;- 100            // Map.set!(&#34;tic&#34;, 100, map)
  map  = map{&#34;tac&#34;} &lt;- 200            // Map.set!(&#34;tac&#34;, 200, map)
  map  = map{ key } &lt;- 300            // Map.set!(key, 300, map)
  val0 = map{&#34;tic&#34;} &lt;&gt; 0              // Maybe.default!(Map.get!(&#34;tic&#34;,map), 0)
  val1 = map{&#34;tac&#34;} &lt;&gt; 0              // Maybe.default!(Map.get!(&#34;tac&#34;,map), 0)
  val2 = map{ key } &lt;&gt; 0              // Maybe.default!(Map.get!(key, map), 0)
  val0 + val1 + val2                  // Nat.add(val0, Nat.add(val1, val2))
"><pre><span><span>//</span> Map, Maybe, String and Nat sugars</span>
sugars: Nat
  key  = <span><span>&#34;</span>toe<span>&#34;</span></span>
  map  = {<span><span>&#34;</span>tic<span>&#34;</span></span>: <span>1</span>, <span><span>&#34;</span>tac<span>&#34;</span></span>: <span>2</span>, key: <span>3</span>} <span><span>//</span> Map.from_list!([{&#34;tic&#34;,1}, ...])</span>
  map  = map{<span><span>&#34;</span>tic<span>&#34;</span></span>} &lt;- <span>100</span>            <span><span>//</span> Map.set!(&#34;tic&#34;, 100, map)</span>
  map  = map{<span><span>&#34;</span>tac<span>&#34;</span></span>} &lt;- <span>200</span>            <span><span>//</span> Map.set!(&#34;tac&#34;, 200, map)</span>
  map  = map{ key } &lt;- <span>300</span>            <span><span>//</span> Map.set!(key, 300, map)</span>
  val0 = map{<span><span>&#34;</span>tic<span>&#34;</span></span>} &lt;&gt; <span>0</span>              <span><span>//</span> Maybe.default!(Map.get!(&#34;tic&#34;,map), 0)</span>
  val1 = map{<span><span>&#34;</span>tac<span>&#34;</span></span>} &lt;&gt; <span>0</span>              <span><span>//</span> Maybe.default!(Map.get!(&#34;tac&#34;,map), 0)</span>
  val2 = map{ key } &lt;&gt; <span>0</span>              <span><span>//</span> Maybe.default!(Map.get!(key, map), 0)</span>
  val0 + val1 + val2                  <span><span>//</span> Nat.add(val0, Nat.add(val1, val2))</span></pre></div>
<div data-snippet-clipboard-copy-content="// List monadic block: returns [{1,4},{1,5},{1,6},{2,4},...,{3,6}]
my_list: List&lt;Pair&lt;Nat,Nat&gt;&gt;
  List {
    get x = [1, 2, 3]
    get y = [4, 5, 6]
    return {x, y}
  }
"><pre><span><span>//</span> List monadic block: returns [{1,4},{1,5},{1,6},{2,4},...,{3,6}]</span>
my_list: List&lt;Pair&lt;Nat,Nat&gt;&gt;
  List {
    get x = [<span>1</span>, <span>2</span>, <span>3</span>]
    get y = [<span>4</span>, <span>5</span>, <span>6</span>]
    <span>return</span> {x, y}
  }</pre></div>
<p>Check many List algorithms on <a href="https://github.com/uwu-tech/Kind/tree/master/base/List">base/List</a>!</p>
<h3><a id="user-content-some-types" aria-hidden="true" href="#some-types"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Some types</h3>
<div data-snippet-clipboard-copy-content="// A boolean
type Bool {
  true
  false
}
"><pre><span>// A boolean</span>
<span>type</span> <span>Bool</span> <span>{</span>
  <span>true</span>
  <span>false</span>
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// A natural number
type Nat {
  zero
  succ(pred: Nat)
}
"><pre><span>// A natural number</span>
<span>type</span> <span>Nat</span> <span>{</span>
  <span>zero</span>
  <span>succ</span><span>(</span><span>pred</span>: <span>Nat</span><span>)</span>
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// A polymorphic list
type List &lt;A: Type&gt; {
  nil
  cons(head: A, tail: List&lt;A&gt;)
}
"><pre><span>// A polymorphic list</span>
<span>type</span> <span>List</span> <span>&lt;</span><span>A</span>: <span>Type</span><span>&gt;</span> <span>{</span>
  <span>nil</span>
  <span>cons</span><span>(</span><span>head</span>: <span>A</span><span>,</span> <span>tail</span>: <span>List</span><span>&lt;</span><span>A</span><span>&gt;</span><span>)</span>
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// A polymorphic pair
type Pair &lt;A: Type, B: Type&gt; {
  new(fst: A, snd: B)
}
"><pre><span>// A polymorphic pair</span>
<span>type</span> <span>Pair</span> <span>&lt;</span><span>A</span>: <span>Type</span><span>,</span> <span>B</span>: <span>Type</span><span>&gt;</span> <span>{</span>
  <span>new</span><span>(</span><span>fst</span>: <span>A</span><span>,</span> <span>snd</span>: <span>B</span><span>)</span>
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// A polymorphic dependent pair
type Sigma &lt;A: Type, B: A -&gt; Type&gt; {
  new(fst: A, snd: B(fst))
}
"><pre><span>// A polymorphic dependent pair</span>
<span>type</span> <span>Sigma</span> <span>&lt;</span><span>A</span>: <span>Type</span><span>,</span> <span>B</span>: <span>A</span> <span>-</span><span>&gt;</span> <span>Type</span><span>&gt;</span> <span>{</span>
  <span>new</span><span>(</span><span>fst</span>: <span>A</span><span>,</span> <span>snd</span>: <span>B</span><span>(</span><span>fst</span><span>)</span><span>)</span>
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// A polymorphic list with a statically known size
type Vector &lt;A: Type&gt; ~ (size: Nat) {
  nil                                              ~ (size = 0) 
  cons(size: Nat, head: Nat, tail: Vector&lt;A,size&gt;) ~ (size = 1 + size)
}
"><pre><span>// A polymorphic list with a statically known size</span>
<span>type</span> <span>Vector</span> <span>&lt;</span><span>A</span>: <span>Type</span><span>&gt;</span> <span>~</span> <span>(</span><span>size</span>: <span>Nat</span><span>)</span><span></span> <span>{</span>
  <span>nil</span>                                              <span>~</span> <span>(</span><span>size</span> <span>=</span> <span>0</span><span>)</span> 
  <span>cons</span><span>(</span><span>size</span>: <span>Nat</span><span>,</span> <span>head</span>: <span>Nat</span><span>,</span> <span>tail</span>: <span>Vector</span><span>&lt;</span><span>A</span><span>,</span><span>size</span><span>&gt;</span><span>)</span> <span>~</span> <span>(</span><span>size</span> <span>=</span> <span>1</span> <span>+</span> <span>size</span><span>)</span>
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// A bounded natural number
type Fin ~ &lt;lim: Nat&gt; {
  zero&lt;N: Nat&gt;               ~ (lim = Nat.succ(N))
  succ&lt;N: Nat&gt;(pred: Fin&lt;N&gt;) ~ (lim = Nat.succ(N))
}
"><pre><span>// A bounded natural number</span>
<span>type</span> <span>Fin</span> <span>~</span> <span>&lt;</span><span>lim</span>: <span>Nat</span><span>&gt;</span> <span>{</span>
  <span>zero</span><span>&lt;</span><span>N</span>: <span>Nat</span><span>&gt;</span>               <span>~</span> <span>(</span><span>lim</span> <span>=</span> <span>Nat</span><span>.</span><span>succ</span><span>(</span><span>N</span><span>)</span><span>)</span>
  <span>succ</span><span>&lt;</span><span>N</span>: <span>Nat</span><span>&gt;</span><span>(</span><span>pred</span>: <span>Fin</span><span>&lt;</span><span>N</span><span>&gt;</span><span>)</span> <span>~</span> <span>(</span><span>lim</span> <span>=</span> <span>Nat</span><span>.</span><span>succ</span><span>(</span><span>N</span><span>)</span><span>)</span>
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// The type used in equality proofs
type Equal &lt;A: Type, a: A&gt; ~ (b: A) {
  refl ~ (b = a)
}
"><pre><span>// The type used in equality proofs</span>
<span>type</span> <span>Equal</span> <span>&lt;</span><span>A</span>: <span>Type</span><span>,</span> <span>a</span>: <span>A</span><span>&gt;</span> <span>~</span> <span>(</span><span>b</span>: <span>A</span><span>)</span> <span>{</span>
  <span>refl</span> <span>~</span> <span>(</span><span>b</span> <span>=</span> <span>a</span><span>)</span>
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// A burrito
type Monad &lt;M: Type -&gt; Type&gt; {
  new(
    bind: &lt;A: Type, B: Type&gt; M&lt;A&gt; -&gt; (A -&gt; M&lt;B&gt;) -&gt; M&lt;B&gt;
    pure: &lt;A: Type&gt; A -&gt; M&lt;A&gt;
  )
}
"><pre><span>// A burrito</span>
<span>type</span> <span>Monad</span> <span>&lt;</span><span>M</span>: <span>Type</span> <span>-</span><span>&gt;</span> <span>Type</span><span>&gt;</span> <span>{</span>
  <span>new</span><span>(</span>
    <span>bind</span>: <span>&lt;</span><span>A</span>: <span>Type</span><span>,</span> <span>B</span>: <span>Type</span><span>&gt;</span> M<span>&lt;</span><span>A</span><span>&gt;</span> -<span>&gt;</span> <span>(</span><span>A</span> <span>-</span><span>&gt;</span> <span>M</span><span>&lt;</span><span>B</span><span>&gt;</span>) -<span>&gt;</span> <span>M</span><span>&lt;</span><span>B</span><span>&gt;</span>
    pure: <span>&lt;</span><span>A</span>: <span>Type</span><span>&gt;</span> A -<span>&gt;</span> <span>M</span><span>&lt;</span><span>A</span><span>&gt;</span>
  )
<span>}</span></pre></div>
<div data-snippet-clipboard-copy-content="// Some game entity
type Entity {
  player(
    name: String
    pos: V3
    health: Nat
    items: List&lt;Item&gt;
    sprite: Image
  )
  wall(
    hitbox: Pair&lt;V3, V3&gt;
    collision: Entity -&gt; Entity
    sprite: Image
  )
}
"><pre><span>// Some game entity</span>
<span>type</span> <span>Entity</span> <span>{</span>
  <span>player</span><span>(</span>
    <span>name</span>: <span>String</span>
    <span>pos</span>: <span>V3</span>
    health: <span>Nat</span>
    <span>items</span>: <span>List</span><span>&lt;</span><span>Item</span><span>&gt;</span>
    <span>sprite</span>: <span>Image</span>
  <span>)</span>
  <span>wall</span><span>(</span>
    <span>hitbox</span>: <span>Pair</span><span>&lt;</span><span>V3</span><span>,</span> <span>V3</span><span>&gt;</span>
    <span>collision</span>: <span>Entity</span> <span>-</span><span>&gt;</span> <span>Entity</span>
    <span>sprite</span>: <span>Image</span>
  <span>)</span>
<span>}</span></pre></div>
<p>Check all core types on <a href="https://github.com/uwu-tech/Kind/tree/master/base">base</a>!</p>
<h3><a id="user-content-some-proofs" aria-hidden="true" href="#some-proofs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Some proofs</h3>
<div data-snippet-clipboard-copy-content="// Proof that `a == a + 0`
Nat.add.zero(a: Nat): a == Nat.add(a, 0)
  case a {
    zero: refl
    succ: apply(Nat.succ, Nat.add.zero(a.pred))
  }!
"><pre><span>// Proof that `a == a + 0`</span>
<span>Nat</span><span>.</span><span>add</span><span>.</span><span>zero</span><span>(</span><span>a</span>: <span>Nat</span><span>)</span>: <span>a</span> <span>==</span> <span>Nat</span><span>.</span><span>add</span><span>(</span><span>a</span><span>,</span> <span>0</span><span>)</span>
  <span>case</span> <span>a</span> <span>{</span>
    zero: <span>refl</span>
    succ: <span>apply</span><span>(</span><span>Nat</span><span>.</span><span>succ</span><span>,</span> <span>Nat</span><span>.</span><span>add</span><span>.</span><span>zero</span><span>(</span><span>a</span><span>.</span><span>pred</span><span>)</span><span>)</span>
  <span>}</span><span>!</span></pre></div>
<div data-snippet-clipboard-copy-content="// Proof that `1 + (a + b) == a + (1 + b)`
Nat.add.succ(a: Nat, b: Nat): Nat.succ(a + b) == (a + Nat.succ(b))
  case a {
    zero: refl
    succ: apply(Nat.succ, Nat.add.succ(a.pred, b))
  }!
"><pre><span>// Proof that `1 + (a + b) == a + (1 + b)`</span>
<span>Nat</span><span>.</span><span>add</span><span>.</span><span>succ</span><span>(</span><span>a</span>: <span>Nat</span><span>,</span> <span>b</span>: <span>Nat</span><span>)</span>: <span>Nat</span><span>.</span><span>succ</span><span>(</span><span>a</span> <span>+</span> <span>b</span><span>)</span> <span>==</span> <span>(</span><span>a</span> <span>+</span> <span>Nat</span><span>.</span><span>succ</span><span>(</span><span>b</span><span>)</span><span>)</span>
  <span>case</span> <span>a</span> <span>{</span>
    zero: <span>refl</span>
    succ: <span>apply</span><span>(</span><span>Nat</span><span>.</span><span>succ</span><span>,</span> <span>Nat</span><span>.</span><span>add</span><span>.</span><span>succ</span><span>(</span><span>a</span><span>.</span><span>pred</span><span>,</span> <span>b</span><span>)</span><span>)</span>
  <span>}</span><span>!</span></pre></div>
<div data-snippet-clipboard-copy-content="// Proof that addition is commutative
Nat.add.comm(a: Nat, b: Nat): (a + b) == (b + a)
  case a {
    zero:
      Nat.add.zero(b)
    succ: 
      p0 = Nat.add.succ(b, a.pred)
      p1 = Nat.add.comm(b, a.pred)
      p0 :: rewrite X in Nat.succ(X) == _ with p1
  }!
"><pre><span>// Proof that addition is commutative</span>
<span>Nat</span><span>.</span><span>add</span><span>.</span><span>comm</span><span>(</span><span>a</span>: <span>Nat</span><span>,</span> <span>b</span>: <span>Nat</span><span>)</span>: <span>(</span><span>a</span> <span>+</span> <span>b</span><span>)</span> <span>==</span> <span>(</span><span>b</span> <span>+</span> <span>a</span><span>)</span>
  <span>case</span> <span>a</span> <span>{</span>
    zero:
      <span>Nat</span><span>.</span><span>add</span><span>.</span><span>zero</span><span>(</span><span>b</span><span>)</span>
    succ: 
      <span>p0</span> <span>=</span> <span>Nat</span><span>.</span><span>add</span><span>.</span><span>succ</span><span>(</span><span>b</span><span>,</span> <span>a</span><span>.</span><span>pred</span><span>)</span>
      <span>p1</span> <span>=</span> <span>Nat</span><span>.</span><span>add</span><span>.</span><span>comm</span><span>(</span><span>b</span><span>,</span> <span>a</span><span>.</span><span>pred</span><span>)</span>
      p0 :: <span>rewrite</span> <span>X</span> <span>in</span> <span>Nat</span><span>.</span><span>succ</span><span>(</span><span>X</span><span>)</span> <span>==</span> <span>_</span> <span>with</span> <span>p1</span>
  <span>}</span><span>!</span></pre></div>
<p>Check some Nat proofs on <a href="https://github.com/uwu-tech/Kind/tree/master/base/Nat/add">base/Nat/add</a>!</p>
<h3><a id="user-content-a-web-app" aria-hidden="true" href="#a-web-app"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>A web app</h3>
<div data-snippet-clipboard-copy-content="// Render function
App.Hello.draw: App.Draw&lt;App.Hello.State&gt;
  (state)
  &lt;div style={&#34;border&#34;: &#34;1px solid black&#34;}&gt;
    &lt;div style={&#34;font-weight&#34;: &#34;bold&#34;}&gt;&#34;Hello, world!&#34;&lt;/div&gt;
    &lt;div&gt;&#34;Clicks: &#34; | Nat.show(state@local)&lt;/div&gt;
    &lt;div&gt;&#34;Visits: &#34; | Nat.show(state@global)&lt;/div&gt;
  &lt;/div&gt;

// Event handler
App.Hello.when: App.When&lt;App.Hello.State&gt;
  (event, state)
  case event {
    init: IO {
      App.watch!(App.room_zero)
      App.new_post!(App.room_zero, App.empty_post)
    }
    mouse_down: IO {
      App.set_local!(state@local + 1)
    }
  } default App.pass!
"><pre><span>// Render function</span>
<span>App</span><span>.</span><span>Hello</span><span>.</span><span>draw</span>: <span>App</span><span>.</span><span>Draw</span><span>&lt;</span><span>App</span><span>.</span><span>Hello</span><span>.</span><span>State</span><span>&gt;</span>
  <span>(</span><span>state</span><span>)</span>
  <span>&lt;</span><span>div</span> <span>style</span><span>=</span><span>{</span><span>&#34;border&#34;</span>: <span>&#34;1px solid black&#34;</span><span>}</span><span>&gt;</span>
    <span>&lt;</span><span>div</span> <span>style</span><span>=</span><span>{</span><span>&#34;font-weight&#34;</span>: <span>&#34;bold&#34;</span><span>}</span><span>&gt;</span>&#34;Hello, world!&#34;<span>&lt;</span><span>/</span><span>div</span><span>&gt;</span>
    <span>&lt;</span><span>div</span><span>&gt;</span><span>&#34;Clicks: &#34;</span> <span>|</span> <span>Nat</span><span>.</span><span>show</span><span>(</span><span>state</span>@<span>local</span><span>)</span><span>&lt;</span><span>/</span>div&gt;
    <span>&lt;</span><span>div</span><span>&gt;</span>&#34;Visits: &#34; | Nat.show(state@global)<span>&lt;</span><span>/</span><span>div</span><span>&gt;</span>
  <span>&lt;</span><span>/</span>div&gt;

<span>// Event handler</span>
<span>App</span><span>.</span><span>Hello</span><span>.</span><span>when</span>: <span>App</span><span>.</span><span>When</span><span>&lt;</span><span>App</span><span>.</span><span>Hello</span><span>.</span><span>State</span><span>&gt;</span>
  <span>(</span><span>event</span><span>,</span> <span>state</span><span>)</span>
  <span>case</span> <span>event</span> <span>{</span>
    init: <span>IO</span> <span>{</span>
      <span>App</span><span>.</span><span>watch</span><span>!</span><span>(</span><span>App</span><span>.</span><span>room_zero</span><span>)</span>
      <span>App</span><span>.</span><span>new_post</span><span>!</span><span>(</span><span>App</span><span>.</span><span>room_zero</span><span>,</span> <span>App</span><span>.</span><span>empty_post</span><span>)</span>
    <span>}</span>
    mouse_down: <span>IO</span> <span>{</span>
      <span>App</span><span>.</span><span>set_local</span><span>!</span><span>(</span><span>state</span>@<span>local</span> <span>+</span> <span>1</span><span>)</span>
    <span>}</span>
  <span>}</span> <span>default</span> <span>App</span><span>.</span><span>pass</span><span>!</span></pre></div>
<p>Source: <a href="https://github.com/uwu-tech/Kind/blob/master/base/App/Hello.kind">base/App/Hello.kind</a></p>
<p>Live: <a href="http://uwu.tech/App.Hello" rel="nofollow">http://uwu.tech/App.Hello</a></p>
</article>
        </div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 12:47:34 +0000</pubDate>
      <source>https://github.com/uwu-tech/kind</source>
    </item>
    <item>
      <title>TikTok overtakes Facebook as most downloaded app</title>
      <link>https://asia.nikkei.com/Business/Technology/TikTok-overtakes-Facebook-as-world-s-most-downloaded-app</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><p>TOKYO -- A global survey of downloads in 2020 shows TikTok, a video-sharing app developed in China, on top of the list of social media providers for the first time since the study was first conducted in 2018.</p><p>As concern for personal privacy grows, Telegram, a messaging app that can delete posts, also ranked high during a year when social media use has been driven up by the COVID-19 pandemic. </p><p>ByteDance launched the international version of TikTok in 2017, and has since overtaken Facebook, WhatsApp, Instagram and Facebook Messenger -- all of which are Facebook owned -- in downloads, even in the U.S.</p><p>&#34;I enjoy videos by artists who aren&#39;t performing live anymore because of the pandemic,&#34; said Nina, 37, of Portland in the U.S.</p><div><p><span></span><span><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon--cross" href="#icon--cross"></use><image src="https://www.ft.com/__origami/service/image/v2/images/raw/https%3A%2F%2Fasia.nikkei.com%2Fassets%2Fimages%2Ficon--cross.2d18c509.svg?format=png&amp;source=nar-cms&amp;tint=%23ffffff"></image></svg></span></p></div><p>Some believe that personal information shared with TikTok is not secure. In 2020, former President Donald Trump called on the company to sell off its U.S. operations or be banned. The app&#39;s popularity nevertheless grew during the pandemic, when it became the leading download in Europe, South America and the U.S.</p><p>Joe Biden, Trump&#39;s successor, withdrew the presidential executive order, but uncertainties remain elsewhere. While The Financial Times reported on Sunday that ByteDance has revived plans to go public in the coming months, a spokesperson told Nikkei Asia on Monday that the article was &#34;inaccurate,&#34; insisting the company has no current plans for a stock market listing.</p><p>China&#39;s Likee, a TikTok competitor, creates short videos that many companies use for marketing, and it ranked eighth in the latest global download league.</p><p>At the start of 2021, WhatsApp announced that it would share messaging data with Facebook relating to interactions between users and companies. Although WhatsApp promised to protect information about communications between friends and family, some users moved over to other apps.</p><p>Bucking that trend, Telegram, a messaging app originally developed in Russia but now based in Germany, moved up to seventh place. Users can adjust the settings to delete messages automatically after a specified period. The app was a particular hit with protesters in Hong Kong and Thailand who wanted to operate under the state radar. </p><p>Users have previously been mostly guided by the convenience and ease of use of free social media, but privacy has become more of a concern lately. &#34;Companies&#39; approach to handling data will become a deciding factor in consumer choices,&#34; Shinichi Yamaguchi, an associate professor at the Center for Global Communications, told Nikkei.</p><p>Discord, a voice calling app that moved up to seventh place, has benefited from people&#39;s need to isolate during the pandemic. The app is popular among gamers for chatting when online, and has been funded by Sony Group. Social media services have primed the pump for greater outside investment and corporate involvement. </p><p>Domestic apps dominate the China market where many from overseas are closed out. Three in China&#39;s top ten are for short video posting, including Douyin, the predecessor of TikTok, which ByteDance still provides in China.</p><p>Douyin is popular for music, dancing and general entertainment content. Douyin Volcano Edition -- also from ByteDance -- provides videos from everyday life, including people falling down in the street and other mishaps. </p><p>Tencent&#39;s WeShow focuses on video games, and includes celebrities playing games live.</p><div><p><span></span><span><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon--cross" href="#icon--cross"></use><image src="https://www.ft.com/__origami/service/image/v2/images/raw/https%3A%2F%2Fasia.nikkei.com%2Fassets%2Fimages%2Ficon--cross.2d18c509.svg?format=png&amp;source=nar-cms&amp;tint=%23ffffff"></image></svg></span></p></div><p>Among newcomers, Little Red Book (RED, Xiaohongshu) was fifth most downloaded. It combines social networking with e-commerce, mainly cosmetics, fashion, dietary supplements and consumer goods. In China, word-of-mouth is important, especially among women. &#34;I read reviews of cosmetics and other products every day,&#34; said a female user in Beijing.</p><p>Online networking is particularly popular among Generation Z, or Zoomers, born in the second half of the 1990s. Soul, the number ten app, uses artificial intelligence to analyze users&#39; personalities through psychological tests. It then matches them to others with similar profiles. A major difference from traditional matchmaking services is that people are not selected based on a picture of their face. Consumers use the app for matchmaking or simply to find new friends.</p><p>Short videos have gained popularity in other parts of Asia. Snack Video, an app from China&#39;s Kuaishou, was the sixth most downloaded in Asia-Pacific. Its main feature is live commerce -- a combination of video broadcasting and online shopping. Companies have used it to grow sales during the COVID-19 pandemic.</p><p>Line, which is especially popular in Thailand, was pushed out of the global rankings by strong competition from new players. In Japan, however, it moved from number two to the top spot. Line is working to make itself a super app by enhancing its payment settlement function.</p><p>During the pandemic, Osaka prefectural authorities used Line for vaccine reservations, and it is widely used by other local governments in Japan to disseminate information, making it increasingly a part of the infrastructure of daily. However, a subcontractor in China was discovered to be able to view Japanese user data, making security more of a concern. </p><p>The U.S. matchmaking app Pairs was originally developed in Japan, where it rose to eighth place. With the pandemic ongoing, it offers a remote dating function that was added in April 2020 to enable people to maintain romantic links from home. &#34;I started using the service because I couldn&#39;t have any real encounters with people during the pandemic,&#34; said one male office worker in his twenties.</p><p>&#34;The total viewing time for TikTok in the U.S. and U.K. is longer than that for YouTube, and short videos will continue to attract attention,&#34; Chuzen Kin, marketing manager at App Annie, an app market intelligence company, told Nikkei as he reviewed trends. &#34;In terms of content, music and comedies are becoming more popular.&#34; Vocal social media is also on the up, with Clubhouse taking off in Japan and the U.S. in early 2021.</p></div></div>]]></content:encoded>
      <pubDate>Tue, 10 Aug 2021 19:21:14 +0000</pubDate>
      <source>https://asia.nikkei.com/Business/Technology/TikTok-overtakes-Facebook-as-world-s-most-downloaded-app</source>
    </item>
    <item>
      <title>How Sweden became the Silicon Valley of Europe</title>
      <link>https://www.reuters.com/business/finance/how-sweden-became-silicon-valley-europe-2021-08-11/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_reuters_com_business_finance_how-sweden-became-silicon-valley-europe-2021-08-11_/image.jpg" /> 
<div id="readability-page-1" class="page"><div><p data-testid="paragraph-0">STOCKHOLM, Aug 11 (Reuters) - As Klarna&#39;s billionaire founder Sebastian Siemiatkowski prepares to stage one of the biggest-ever European fintech company listings, a feast of capitalism, he credits an unlikely backer for his runaway success: the Swedish welfare state.</p><p data-testid="paragraph-1">In particular, the 39-year-old pinpoints a late-1990s government policy to put a computer in every home.</p><p data-testid="paragraph-2">&#34;Computers were inaccessible for low-income families such as mine, but when the reform came into play, my mother bought us a computer the very next day,&#34; he told Reuters.</p><p data-testid="paragraph-3">Siemiatkowski began coding on that computer when he was 16. Fast-forward more than two decades, and his payments firm Klarna is valued at $46 billion and plans to go public. It hasn&#39;t given details, though many bankers predict it will list in New York early next year.</p><p data-testid="paragraph-4">Sweden&#39;s home computer drive, and concurrent early investment in internet connectivity, help explain why its capital Stockholm has become such rich soil for startups, birthing and incubating the likes of Spotify, Skype and Klarna, even though it has some of the highest tax rates in the world.</p><p data-testid="paragraph-5">That&#39;s the view of Siemiatkowski and several tech CEOs and venture capitalists interviewed by Reuters.</p><p data-testid="paragraph-6">In the three years the scheme ran, 1998-2001, 850,000 home computers were purchased through it, reaching almost a quarter of the country&#39;s then-four million households, who didn&#39;t have to pay for the machines and thus included many people who were otherwise unable to afford them.</p><p data-testid="paragraph-7">In 2005, when Klarna was founded, there were 28 broadband subscriptions per 100 people in Sweden, compared with 17 in the United States - where dial-up was still far more common - and a global average of 3.7, according to data from the World Bank.</p><p data-testid="paragraph-8">Spotify allowed users to stream music when Apple&#39;s <a href="https://www.reuters.com/companies/AAPL.O" target="_blank">(AAPL.O)</a> iTunes was still download-based, which gave the Swedish company the upper-hand when streaming became the norm around the world.</p><p data-testid="paragraph-9">&#34;That could only happen in a country where broadband was the standard much earlier, while in other markets the connection was too slow,&#34; Siemiatkowski said.</p><p data-testid="paragraph-10">&#34;That allowed our society to be a couple of years ahead.&#34;</p><p data-testid="paragraph-11">Some executives and campaigners say the Scandinavian nation demonstrates that a deep social safety net, often viewed as counter to entrepreneurial spirit, can foster innovation. It&#39;s an outcome that might not have been envisaged by the architects of Sweden&#39;s welfare state in the 1950s.</p><p data-testid="paragraph-12">Childcare is, for the most part, free. A range of income insurance funds can protect you if your business fails or you lose your job, guaranteeing up to 80% of your previous salary for the first 300 days of unemployment.</p><p data-testid="paragraph-13">&#34;The social safety net we have in Sweden allows us to be less vulnerable to taking risks,&#34; said Gohar Avagyan, the 31-year-old co-founder of Vaam, a video messaging service used for sales pitches and customer communication.</p><p data-testid="paragraph-14">STARTUP RATE VS SILICON VALLEY</p><p data-testid="paragraph-15">Although overall investments are larger in the bigger European economies of Britain and France and their longstanding finance hubs, Sweden punches above its weight in some regards.</p><p data-testid="paragraph-16">It has the third highest startup rate in the world, behind Turkey and Spain, with 20 startups per 1000 employees and the highest three year survival rate for startups anywhere, at 74%, according to a 2018 study by OECD economists.</p><p data-testid="paragraph-17">Stockholm is second only to Silicon Valley in terms of unicorns - startups valued at above $1 billion - per capita, at around 0.8 per 100,000 inhabitants, according to Sarah Guemouri at venture capital firm Atomico.</p><p data-testid="paragraph-18">Silicon Valley - San Francisco and the Bay Area - boasts 1.4 unicorns per 100,000, said Guemouri, co-author of a 2020 report on European tech companies.</p><p data-testid="paragraph-19">No one can say for sure if the boom will last, though, in a country where capital gains are taxed at 30 percent and income tax can be as high as 60 percent.</p><p data-testid="paragraph-20">In 2016, Spotify said it was considering moving its headquarters out of the country, arguing high taxes made it difficult to attract overseas talent, though it hasn&#39;t done so.</p><p data-testid="paragraph-21">Yusuf Ozdalga, partner at venture capital firm QED Investors, said access to funding and administrative or legal tasks connected with founding a company could also prove tough to navigate for non-Swedish speakers.</p><p data-testid="paragraph-22">He contrasted that to Amsterdam, capital of the Netherlands, where the government adopted English as an official language in April to make life easier for international companies.</p><p data-testid="paragraph-23">&#39;INTERESTING DILEMMA&#39; FOR VC</p><p data-testid="paragraph-24">Jeppe Zink, partner at London-based venture capital firm Northzone, said a third of all the exit value from fintech companies in Europe - the amount received by investors when they cash out - came from Sweden alone.</p><p data-testid="paragraph-25">Government policy had contributed to this trend, he added.</p><p data-testid="paragraph-26">&#34;Its an interesting dilemma for us venture capitalists as we&#39;re not used to regulation creating markets, in fact we are inherently nervous about regulation.&#34;</p><p data-testid="paragraph-27">Sweden&#39;s digital minister Anders Ygeman said that social regulation could make it &#34;possible to fail&#34; and then &#34;be up and running again&#34; for innovators.</p><p data-testid="paragraph-28">Peter Carlsson, CEO of startup Northvolt, which makes Lithium-ion batteries for electric vehicles and is valued at $11.75 billion, said that ultimately success bred success.</p><p data-testid="paragraph-29">&#34;You&#39;re really creating ripple effects when you&#39;re seeing the success of somebody else and I think that&#39;s perhaps the most important thing in order to create local ecosystems.&#34;</p><p><span>Reporting by Supantha Mukherjee and Colm Fulton in Stockholm; Editing by Pravin Char</span></p><p>Our Standards: <a href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank">The Thomson Reuters Trust Principles.</a></p></div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 08:16:36 +0000</pubDate>
      <source>https://www.reuters.com/business/finance/how-sweden-became-silicon-valley-europe-2021-08-11/</source>
    </item>
    <item>
      <title>Netflix Intensifies ‘VPN’ Ban and Targets Residential IP-Addresses Too</title>
      <link>https://torrentfreak.com/netflix-intensifies-vpn-ban-and-targets-residential-ip-addresses-too-210811/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___torrentfreak_com_netflix-intensifies-vpn-ban-and-targets-residential-ip-addresses-too-210811_/image.jpg" /> 
<div id="readability-page-1" class="page"><div>
 <p>

<span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to TorrentFreak." href="https://torrentfreak.com"><span property="name">Home</span></a><meta property="position" content="1"/></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Anti-Piracy category archives." href="https://torrentfreak.com/category/anti-piracy/"><span property="name">Anti-Piracy</span></a><meta property="position" content="2"/></span> &gt; <span></span>
</p>
<p>
<span> </span>
Netflix has stepped up its efforts to ban VPN and proxy users from bypassing geographical restrictions. The streaming service is now blocking residential IP addresses too, since some unblocking tools use these to bypass restrictions. This isn&#39;t without collateral damage as many regular Internet users without a VPN now report &#34;missing content&#34; on Netflix.
</p>
</div><div>
<p>Six years ago, Netflix <a href="https://torrentfreak.com/netflix-cracks-down-on-vpn-and-proxy-pirates-150103/">started blocking</a> customers who tried to access its service over a commercial VPN or proxy service. </p>
<p>These changes came after copyright holders <a href="https://torrentfreak.com/copyright-holders-want-netflix-ban-vpn-users-140917/">repeatedly complained</a> that ‘pirates’ were bypassing Netflix’s geographical restrictions. </p>
<p>The VPN ban caused a lot of frustration for legitimate VPN users, many of whom had no intention of breaking any rules. At the same time, the VPN ‘pirates’ found workarounds by picking services that actively bypass Netflix’s restrictions. </p>
<h2>Bypassing Restrictions</h2>
<p>There are various ways VPN services have managed to circumvent these blocking efforts. Most keep the technical details private, but it’s commonly known that some are using residential IP-addresses as proxies, to make it look like VPN users are regular ISP subscribers. </p>
<p>This cat and mouse game has caused quite a bit of frustration at Netflix headquarters and, over the past few days, the company appears to have intensified its blocking measures. </p>
<p>There is a flurry of complaints on social media from users whose VPN services were suddenly ‘blocked’ by Netflix. Previously, these people couldn’t play any content while using a VPN. That changed last year. Now, VPN users can still see Netflix originals while <a href="https://torrentfreak.com/netflix-is-less-annoying-to-vpn-users-now-but-some-titles-are-hidden-200618/">other content is hidden and blocked</a>.</p>
<p>People who try to access blocked titles directly through a saved URL will see Netflix’s <a href="https://torrentfreak.com/images/netflix-pardon.jpg">dreaded proxy/VPN error message</a> instead. </p>
<h2>Netflix Bans Residential IP-Addresses</h2>
<p>Netflix doesn’t explain which IP addresses are blocked and why, but the most recent efforts are much broader than before. This issue was brought to our attention by <a href="https://torrentfreak.com/best-vpn-anonymous-no-logging/#wevpn">WeVPN</a>, which noticed that the updated geo-fencing system is blocking its residential IP addresses. </p>
<p>These IP addresses are assigned to common consumer ISPs such as AT&amp;T, Comcast, Verizon. While it makes sense for Netflix to put an end to these workarounds, there appears to be some collateral damage.</p>
<p>“The collateral damage is that you have hundreds of thousands of legitimate residential Netflix subscribers blocked from accessing Netflix’s local country full catalog from their home,” a WeVPN spokesperson informs us.</p>
<p>While we are unable to verify how many people are facing issues, it is clear that the measures are spilling over to regular subscribers.</p>
<h2>Complaints Start Pouring In</h2>
<p>TorrentFreak reached out to Netflix for a comment but the company didn’t immediately reply. However, a quick glance on social media shows a disturbing number of Netflix subscribers who are “missing” content, which is exactly what would happen when an IP-address is flagged. </p>
<p>“Hi! I noticed that my account is displaying nothing but Netflix originals and a handful of non-Netflix original content on my TV, but on my phone, it displays everything as usual/normal,” Reddit user <a href="https://www.reddit.com/r/netflix/comments/p26fr6/netflix_glitch/">literarydone observed</a>.</p>
<center></center>
<p>“Idk whats happened but Netflix suddenly stopped showing tv shows that I was watching when my laptop is connected to the internet over wifi. it shows the same tv shows when my laptop is connected to the internet over mobile data hotspot,” <a href="https://www.reddit.com/r/netflix/comments/p1trnt/help_netflix_shows_not_showing_over_but_showing/">another person wrote</a>, with a commenter reporting the same problem. </p>
<center></center>
<p>Over the past 24 hours alone, there were <a href="https://www.reddit.com/r/netflix/comments/p1tiv2/some_titles_suddenly_not_available_in_my_area/">multiple</a> reports <a href="https://www.reddit.com/r/netflix/comments/p1ureb/netflix_switched_all_my_continue_watchings_to/">from people</a> who <a href="https://www.reddit.com/r/netflix/comments/p1h3jp/shows_keep_disappearing_and_reappearing_out_of/">are</a> suffering “<a href="https://www.reddit.com/r/netflix/comments/p0ub9h/netflix_missing_content_fixed/">missing title</a>” issues. None of these appear to use a VPN.</p>
<p>The common theme is that Netflix only shows Netflix originals on their IP address, which is expected when it’s flagged as a VPN or proxy. One Redditor managed to get a new IP-address from his ISP, which immediately resolved the problem.</p>
<h2>“Contact Your ISP…”</h2>
<p>While Netflix hasn’t released an official comment on the situation, the company is aware of the problems. </p>
<p>One user who complained <a href="https://twitter.com/raymond_clum/status/1425333580352393218">on Twitter</a>, got the advice to contact their ISP to see if their IP address is associated with proxy or VPN use. This is a peculiar suggestion, as the blocking is taking place on Netflix’s end.</p>
<center></center>
<p>We don’t know how widespread the problem is but based on the number of complaints we have seen so far it’s certainly not an isolated issue. That begs the question if the VPN banning measures are worth the collateral damage. </p>
<p>Netflix has all the rights to take action against people who bypass their restrictions, but when this harms paying customers who don’t use a VPN, it might not be the best solution.</p>
<p>Meanwhile, VPNs are taking countermeasures to make sure that their customers can access Netflix without restrictions. </p>
<p>WeVPN told us that the company is experimenting with a solution, which appears to function for now. <a href="https://torrentfreak.com/best-vpn-anonymous-no-logging/#cyberghost">CyberGhost</a> and <a href="https://torrentfreak.com/best-vpn-anonymous-no-logging/#pia">Private Internet Access</a>, which were also affected by Netflix’s new blockades, say they managed to route around it within a day. </p>
</div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 15:32:21 +0000</pubDate>
      <source>https://torrentfreak.com/netflix-intensifies-vpn-ban-and-targets-residential-ip-addresses-too-210811/</source>
    </item>
    <item>
      <title>PG&amp;E power line suspected in Dixie fire was set to be buried underground</title>
      <link>https://www.latimes.com/california/story/2021-08-10/pge-power-line-dixie-fire-scheduled-to-be-buried-underground</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
                                                <div><div data-subscriber-content=""><p>After Pacific Gas &amp; Electric equipment sparked a massive fire <a href="https://www.latimes.com/local/california/la-me-camp-fire-deathtrap-20181230-story.html">that burned much of Paradise</a>, Calif., and killed 86 people in 2018, the utility vowed a safety campaign aimed at preventing similar disasters.</p><p>PG&amp;E said it would bury some power lines snaking through Northern California forest land, significantly reducing the risk of wildfires caused when winds damage equipment. Among the power lines set to be buried was a 10-mile stretch that may have started this year’s destructive Dixie fire, now the second largest in California history.<br/></p><div data-with-aside="true">
 <p>For the record:</p>
 <p><span>9:19 a.m. Aug. 11, 2021</span><span>A previous version of this story stated incorrectly that the U.S. Forest Service had approved the line burial project last July, and the California Department of Transportation granted a permit in October. Those approvals were for work on a different section of the line. </span></p>
</div><p>The situation underscores the rising scrutiny PG&amp;E is facing this summer as a string of huge fires across Northern California have raged amid hot, dry conditions. PG&amp;E power lines might have been responsible for at least three of those fires, according to documents the utility has filed with state regulators and a federal court. Legal liability from the devastation in Paradise and other wildfires <a href="https://www.latimes.com/environment/story/2020-06-17/pge-bankruptcy-new-pge-looks-like-old-pge">pushed PG&amp;E into bankruptcy</a> and brought vows that the utility would fix its power grid. But the new fires have brought new outrage and demonstrated that  there is still much work to do.</p><p>The Dixie fire <a href="https://www.latimes.com/california/story/2021-08-09/containment-of-massive-dixie-fire-weeks-away-officials-say" data-autoplayable-video="true">has now burned more than 480,000 acres in four counties,</a> destroying more than 400 homes and commercial buildings</p>

<p>The U.S. Forest Service approved the line burial project last July, and the California Department of Transportation granted a permit in October.</p><p>The project remains in progress, with no estimated completion date, said James Noonan, spokesman for PG&amp;E.</p><p>“This project will require CalTrans and other governmental agency permits, FERC review, and other land/environmental dependencies,” Noonan wrote in an email (FERC is the Federal Energy Regulatory Commission). “The final completion date for this project is dependent upon the timely fulfilment of these various requirements.”</p>
<p>PG&amp;E’s Wildfire Risk Governance Steering Committee had approved the work in January after considering public safety power shutoff decisions, ingress and egress issues, and tree-fall risk along the power line, which was considered moderate, Noonan said.</p><p>“We are taking steps every day to improve the safety and reliability of our electric system,” Noonan wrote. “This includes working with customers and communities to manage trees and other vegetation located near power lines that could cause a wildfire or power outage.”</p><p>On July 13, a Douglas fir fell onto the line, and two fuses were blown, according to documents PG&amp;E filed in federal court. In the filings, the utility <a href="https://www.latimes.com/california/story/2021-08-07/california-wildfire-injures-4-firefighters-as-crews-scramble-to-get-the-upper-hand">described a series of mishaps and delays</a> that resulted in an employee not reaching the site until about 10 hours later, by which time a 600- to 800-square-foot fire had ignited.</p>

<p>Before the fire, there were no issues with the equipment on the span of line that had been identified but not fixed, nor were there trees  that had been targeted for trimming or removal on which the work hadn’t yet been performed, Noonan said. A vegetation management inspection took place Jan. 14 but did not flag the tree that’s believed to have fallen on the line as needing work, he said.</p><p>PG&amp;E had also inspected the two poles between which the tree was found leaning May 13 and found nothing that required corrective action, Noonan said.</p><p>Nine days after the Dixie fire started, PG&amp;E equipment might have ignited the Fly fire nearly 30 miles to the northeast — disturbances were recorded on a circuit around the same time the fire broke out, and a tree was later found resting on a conductor, the utility said in a report filed with the California Public Utilities Commission. That fire grew to more than 4,300 acres before merging with the Dixie fire, which on Sunday leapfrogged the 2018 Mendocino Complex fire <a href="https://www.latimes.com/california/story/2021-08-08/dixie-fire-now-second-largest-wildfire-california-history" data-autoplayable-video="true">to become the second largest in California history.</a></p><p>District attorneys in two counties — Butte and Plumas — are investigating PG&amp;E for potential criminal liability in the fire.</p>
<p>“It’s literally torn our county in two,” said Plumas County Dist. Atty. David Hollister, who has partnered with the office of Butte County Dist. Atty. Mike Ramsey and the California Department of Forestry and Fire Protection to conduct the investigation. “We’ve lost most of the northern part of our county.”</p><p>Investigators have visited the origin sites of both the Dixie and Fly fires to gather PG&amp;E equipment and trees that might have fallen into the lines for a forensic examination, Ramsey said. They are examining whether adequate vegetation management was done there and whether equipment was maintained, as well as the amount of time it took the utility to realize there was a problem with the line after a disturbance was recorded, he said.</p><p>“As we all know, that canyon where the Camp fire started is extraordinarily dry,” Ramsey said. “We know; they should know.”</p><p>The Dixie fire started in the same canyon, he noted.</p>
<p>Ramsey’s office previously secured a deal in which PG&amp;E pleaded guilty to 84 counts of manslaughter and one count of reckless arson in connection with the Camp fire, which destroyed the town of Paradise. The utility received the maximum fine of $3.5 million, which Ramsey called “woefully underwhelming.”</p><p>“But more important was the fact that they were held responsible for the first time for killing folks,” he said, noting that PG&amp;E had been previously prosecuted for regulatory violations when its equipment caused deaths.</p><p>Although no deaths have yet been reported in the Dixie fire, prosecutors are exploring other avenues as well, they said, noting that Section 452 of the California Penal Code sets forth crimes concerning reckless burning.</p><p>But with the fire just 21% contained and authorities still scrambling to make sure everyone is safe, it remains early in the investigation.</p>

<p>Tehama County Dist. Atty. Matthew Rogers said his office is not investigating PG&amp;E at this time. Lassen County Dist. Atty. Susan Rios said she may ask to join the investigation later if the county sustains losses attributable to the fire but noted that there hasn’t been a damage assessment conducted there yet.</p><p>“They’re just trying to save lives and save towns,” said Rios, whose own home remained under an evacuation warning Monday.</p><p>The events come as the latest blow to the beleaguered utility, which is also <a href="https://www.latimes.com/california/story/2021-07-29/shasta-county-d-a-considers-pg-e-criminally-liable-for-deadly-zogg-fire-no-charges-filed-yet">facing a criminal investigation for its role in sparking last year’s Zogg fire.</a> That fire killed four people, destroyed more than 200 homes and burned 56,000 acres.</p>
<p>In addition, a federal judge last week ordered PG&amp;E to explain its role in potentially igniting the Fly fire, and to provide more information about the Dixie fire, including drone video taken the day it ignited.</p><p>A drone seen flying over the fire in the hours after it started is rumored to have been operated by PG&amp;E or one of its contractors, Ramsey said. The sighting grounded firefighting aircraft for the evening, he said.</p><p>“The air assets had it pretty well blocked in with retardant at less than one or two acres,” Ramsey said. “The drone showed up, and those air assets had to be taken out, and that night it burned through the retardant lines. And now we have the monster that we have.”</p><p>Records show that all the drones authorized to fly on PG&amp;E’s behalf on July 13 in Butte or Plumas counties had completed their flights by about 12:30 p.m., Noonan said.</p>
<p>U.S. District Judge William Alsup, who is overseeing the utility’s criminal probation stemming from <a href="https://www.latimes.com/local/la-xpm-2011-aug-30-la-me-0831-san-bruno-20110831-story.html">an explosion of one of its Bay Area gas lines that killed eight people in 2010,</a> also ordered PG&amp;E to provide a list by Aug. 16 of all fires its equipment has started  this season.</p><p>The utility has already disclosed that its equipment might have ignited a third fire: the Bader fire, which  burned a quarter-acre in Magalia on July 14. One stem of a two-stemmed black oak was found leaning on a power line that had snapped, according to court documents.</p><p>And in April, Sonoma County Dist. Atty. Jill Ravitch <a href="https://www.latimes.com/world-nation/story/2021-04-06/californias-pacific-gas-electric-charged-in-2019-wildfire">charged PG&amp;E with five felony and 28 misdemeanor counts over its role in the Kincade fire,</a> which badly injured six firefighters in 2019.</p><p>PG&amp;E is an investor-owned utility that is overseen by the California Public Utilities Commission. It is required to make certain disclosures as a result of its criminal probation, as well as a 2018 law passed by the state Legislature that requires utilities to submit annual plans to mitigate the risk of wildfire in their service territories and file quarterly updates on their progress.</p>
<p>PG&amp;E has consistently failed to meet targets it has  committed to in those disclosures for things like vegetation management and equipment inspections, advocates say.</p><p>“The challenge is that every time we see a third-party inspection report, either from the Public Utilities Commission or the federal court monitor, we see that over and over again, PG&amp;E has failed to trim the vegetation in the highest fire risk zones like they were supposed to, that they failed to inspect all of their transmission towers and equipment they were supposed to,” said Mark Toney, executive director of the Utility Reform Network, a San Francisco-based consumer advocacy group. “It’s very concerning that we have so much money being spent and not the kind of results that people expect and deserve.”</p><p>The California Public Advocates Office, a consumer watchdog agency, identified so many deficiencies in PG&amp;E’s wildfire mitigation plan this year that in June it submitted a response urging the PUC to issue a finding that the utility was no longer in good standing for the remainder of 2021.</p><p>Two years ago, the Legislature adopted a bill that includes a provision laying out a six-step process that will result in PG&amp;E being converted from a private corporation to a quasi-public entity called Golden State Power if it is unable to meet safety standards. PG&amp;E is currently at step one.</p>
<p>“The higher the PUC moves them up, the greater the chance that PG&amp;E will cease to exist as we know it,” Toney said. “I wouldn’t be surprised to see the PUC looking at moving PG&amp;E up that ladder, given the more recent events, particularly after Cal Fire issues its report.”</p><p>Public utilities differ from investor-owned utilities mainly in that they are not-for-profit so they’re beholden only to customers, said Barry Moline, executive director of the California Municipal Utilities Assn., a trade group for public utilities.</p><p>At the same time, he said, it’s important to note that just because PG&amp;E’s equipment might have sparked a wildfire doesn’t mean the utility is necessarily at fault.</p><p>“While I think everybody is eager to jump on PG&amp;E, I think that it’s really difficult to understand the exact circumstances without an investigation and trying to figure out what happened,” he said. “I believe that PG&amp;E is highly focused on maintaining their system and doing what they can to minimize any ignition of a fire.”</p>
<p>Should it be unable to get a handle on wildfire mitigation, PG&amp;E could also lose the confidence of its investors, in addition to potentially losing the franchise to provide power, Toney said.</p><p>“If they get further downgrades in their credit, and end up at junk bond status, it’s always possible they could end up in bankruptcy again,” he said. “That would be a terrible outcome for everyone involved, quite frankly.”</p></div></div>
                                            </div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 14:20:34 +0000</pubDate>
      <source>https://www.latimes.com/california/story/2021-08-10/pge-power-line-dixie-fire-scheduled-to-be-buried-underground</source>
    </item>
    <item>
      <title>Regulators should treat stablecoins like banks</title>
      <link>https://www.economist.com/leaders/2021/08/07/why-regulators-should-treat-stablecoins-like-banks</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_economist_com_leaders_2021_08_07_why-regulators-should-treat-stablecoins-like-banks/image.jpg" /> 
<div id="readability-page-1" class="page"><div><header><h2 data-test-id="Article Description" itemprop="description">Cryptocurrencies are not yet a threat to the financial system, but the dangers are growing</h2></header><div data-test-id="Lead Image"><div itemscope="" itemprop="image" itemtype="https://schema.org/ImageObject"><meta itemprop="url" content="https://www.economist.com/img/b/1280/720/90/sites/default/files/images/2021/08/articles/main/20210807_ldd010.jpg"/><p></p></div></div><hr/></div><div itemprop="text"><p data-caps="initial"><span data-caps="initial">T</span><small>WELVE YEARS</small> after bitcoin was born, governments are still struggling to cope with cryptocurrencies. Britain has banned Binance, a crypto exchange and the European Union’s regulators want transactions to be more traceable. On August 3rd Gary Gensler, the head of America’s Securities and Exchange Commission, said cryptocurrency markets were “rife with fraud, scams and abuse” and called on Congress to give his agency new regulatory powers. The price of bitcoin, the biggest cryptocurrency, gyrates with regulators’ every word.</p><div><figure><figcaption>Listen to this story</figcaption><audio controls="" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/008%20Leaders%20-%20Regulating%20cryptocurrencies-f37ea4863833581743d0dfb7b28880e4.mp3" title="Why regulators should treat stablecoins like banks" controlslist="nodownload"><p>Your browser does not support the &lt;audio&gt; element.</p></audio></figure><p>Enjoy more audio and podcasts on<!-- --> <a id="audio-ios-cta" href="https://apps.apple.com/app/apple-store/id1239397626?pt=344884&amp;ct=article%20audio%20player&amp;mt=8" target="_blank" rel="noreferrer">iOS</a> <!-- -->or<!-- --> <a id="audio-android-cta" href="https://play.google.com/store/apps/details?id=com.economist.lamarr&amp;referrer=utm_source%3Darticle%2520audio%2520player" target="_blank" rel="noreferrer">Android</a>.</p></div><p>Governments have an obligation to fight the deception, tax evasion and money laundering that plagues the crypto world. Police seizures of bitcoin suggest that they are becoming more zealous. The harder issue they must grapple with is whether cryptocurrencies threaten the financial system. Were bitcoin to collapse, <a href="https://www.economist.com/finance-and-economics/2021/08/02/what-if-bitcoin-went-to-zero">our crypto “stress test” suggests</a> that its holders would lose hundreds of billions of dollars but that the fallout would be manageable. Yet there is another danger posed by “stablecoins”, a special type of cryptocurrency that pegs its value to conventional money.</p><p>Pledges of stability often lead to financial crises. Because banks offer deposits that are redeemable on demand and superficially riskless, but which are backed by longer-term, less liquid and riskier assets, they are vulnerable to runs. Stablecoins are similar. The biggest, Tether, has issued $62bn-worth of tokens which it says are redeemable for a dollar apiece. But of the assets backing the tokens in March only about 5% were cash or Treasury bills, according to Tether’s public disclosures. It says it will update the figures soon and that it is “fully backed by reserves”.</p><div><figure><div itemscope="" itemprop="image" itemtype="https://schema.org/ImageObject" data-slim="1"><meta itemprop="url" content="https://www.economist.com/img/b/608/468/90/sites/default/files/images/print-edition/20210807_LDC140.png"/><p></p></div></figure><p>Most of the assets were riskier—about half of them commercial paper. Stablecoins’ growth from a value of $14bn in August 2020 to over $100bn today has given them a big financial footprint. Extrapolating Tether’s disclosures implies that it owns over $30bn-worth of commercial paper, which probably makes it the asset class’s seventh-largest investor, not far off funds run by Vanguard and BlackRock, according to JPMorgan Chase. With estimated leverage of 383-to-1, Tether would be unable to honour all its tokens after losses of just 0.26%—a safety cushion that regulators would never allow at a bank.</p><p>Few stablecoins say much about their balance-sheets. Tether’s disclosures of the breakdown of its assets are puny and fall far below the standards expected of a bank. In February Tether was among the defendants who agreed to an $18.5m fine with New York’s attorney-general, which said that in 2017 Tether had misled the market about its <small>US</small> dollar backing and that it had not accurately disclosed the transfer of $625m of its assets to Bitfinex, an online trading platform. Tether says the funds were repaid and that it has a “total commitment to transparency”.</p><p>No wonder Mr Gensler calls cryptocurrencies a Wild West. Some policymakers have compared stablecoins to the period of “free banking”, when privately issued banknotes of uncertain backing and worth circulated in America’s economy in the 19th century. A more useful comparison is with money-market funds, which were created in the 1970s to circumvent rules limiting the interest banks could pay depositors. After promising to maintain the value of their shares at a dollar, money-market funds blew up in 2008 in the global financial crisis. American taxpayers stepped in to forestall a fire sale of their assets and a crash in the market for commercial paper, on which the real economy depends. A collapse of stablecoins could look similar.</p><p>Regulators must act quickly to subject stablecoins to bank-like rules for transparency, liquidity and capital. Those failing to comply should be cut off from the financial system, to stop people drifting into an unregulated crypto-ecosystem. Policymakers are right to sound the alarm, but if stablecoins continue to grow, governments will need to move faster to contain the risks.</p></div><p>It may be tempting to ban stablecoins, especially if central banks launch their own digital currencies—much as private banknotes were replaced with government monopolies on physical cash. Yet it is possible that regulated private-sector stablecoins will eventually bring benefits, such as making cross-border payments easier, or allowing self-executing “smart contracts”. Regulators should allow experiments whose goal is not merely to evade financial rules. But first they must prevent the repackaging of risks with which the world is all too familiar. <span data-ornament="ufinish">■</span></p><p data-test-id="Footnote">This article appeared in the Leaders section of the print edition under the headline &#34;Unstablecoins&#34;</p></div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 15:02:34 +0000</pubDate>
      <source>https://www.economist.com/leaders/2021/08/07/why-regulators-should-treat-stablecoins-like-banks</source>
    </item>
    <item>
      <title>Copy-Protection for Vinyl in the 1970s</title>
      <link>https://www.currybet.net/cbet_blog/2008/01/copy-protection-for-vinyl-in-t.php</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_currybet_net_cbet_blog_2008_01_copy-protection-for-vinyl-in-t_php/image.jpg" /> 
<div id="readability-page-1" class="page"><div>

                                    <div>
                                        <p>When I was thinking about an <a href="https://www.currybet.net/cbet_blog/2007/12/drm_icons.php">iconography for DRM</a>, I began to think back to the old days of recording vinyl onto my portable tape deck using a <a href="https://en.wikipedia.org/wiki/DIN_connector">5-pin DIN plug</a> to connect the two machines, the advent of tape-to-tape decks, and being <a href="http://www.last.fm/music/This+Poison/_/Poised+Over+The+Pause+Button">poised over the pause button</a> to record tracks from <a href="http://www.bbc.co.uk/radio1/johnpeel/index.shtml">John Peel</a> sessions. It was all so simple and blissfully analogue in those days, without a hint of copy protection in place.</p>

<p>Well, that isn&#39;t <em>strictly</em> true.</p>

<p>I dimly recalled that there <em>was</em> a form of copy-restriction placed on some vinyl albums. A high-pitched frequency was pressed into the album, which was inaudible to the human ear during playback, but which would destabilise the process of recording the album to cassette 
tape.</p>

<p>The reason I remember it more than anything was that a family friend had built some sort of decoder to eliminate the frequency, and was hoping, should it become widely adopted, to become rich on the back of selling little gizmos that would strip this primitive analogue rights management from recordings.</p>

<p>Unfortunately, copy-protection for vinyl appears to be one of those technological developments that seems to have slipped down the internet&#39;s collective <a href="https://en.wikipedia.org/wiki/Memory_hole">memory hole</a>. I couldn&#39;t find very much about it on the web at all, and of the stuff I did find, some of it seemed wrong or difficult to verify.</p>



<p>It may be that this was more common in the USA than the UK, but take this claim in  
Ram Samudrala&#39;s 1998 paper &#39;<a href="https://www.ram.org/ramblings/philosophy/fmp/creativity_ownership.html">Creativity and ownership: where is the balance?</a>&#39; that:</p>

<blockquote>&#39;When recording artists started objecting to this (Elvis Costello released his album Almost Blue with a sticker indicating that it didn&#39;t contain a spoiler signal), the industry gave up this effort.&#39;</blockquote>

<p>As far as I&#39;m aware the album <em>did</em> come with a sticker on the front in the UK, but it was one that was defensive of Costello&#39;s move into &#39;genre&#39; music, rather than a protest against anti-piracy measures:</p>

<blockquote>&#39;Warning: This album contains country &amp; western music, and may be harmful to narrow minds.&#39;</blockquote> 

<p>
</p>

<p>Then there is this 1999 article by Barry Fox from the New Scientist - &#34;<a href="https://www.newscientist.com/article/mg16422175.100-the-pirates-tale.html">The pirate&#39;s tale</a>&#34;. This puts the origination of this tactic down to The Beatles:</p>


<blockquote>&#39;It didn&#39;t take the recording industry long to see the danger [<em>of the compact cassette</em>]. When the Beatles founded the company Apple, they hired a colourful Greek character called &#34;Magic Alex&#34; Mardas. Together they hatched a plan to put a &#34;spoiler&#34; signal on the Beatles&#39;s next album, Sergeant Pepper. The record would play normally, but anyone who tried to copy it onto a blank cassette would find their recording ruined.
</blockquote>

<p>
</p>

<p>This is almost certainly wrong in attributing the development to The Beatles. &#34;Magic Alex&#34; Mardas was not the revolutionary engineer he portrayed himself to be. In 1995, <a href="https://www.injustice.org/nemo/newsfile/im951028.html">in an article about what became of The Beatle&#39;s hangers-on</a>, The Independent described him as:</p>

<blockquote>&#39;The Greek TV repairman, set up in Apple Electronics by the Beatles, who promised to build an artificial sun, a telephone you told who to call, wallpaper loudspeakers, a house which hovered supported by an invisible beam, and even a flying saucer. Not one invention was made, and the recording studio he built proved unusable and was demolished. Allen Klein fired him, and he has since disappeared.&#39;</blockquote>





<p>Additionally, Sergeant Pepper was released by <em>Parlophone</em> in 1967, not Apple. The Beatle&#39;s first album release for Apple was the double-vinyl set that became commonly known as &#34;The White Album&#34;.</p>

<p>It may be that this story is due to confusion with the fact that Sergeant Pepper <em>does</em> in fact contain a very high frequency signal on it - but only for a very short duration. At the end of side 2 on the original vinyl release, the long drawn out chord that closes &#34;A Day In The Life&#34; leads into a short snippet of nonsense noise that repeats infinitely in a locked-groove.</p>

<p>Just prior to this, a very high pitch frequency is included, designed to attract the attention of any dogs who were around when the album was being played. However, <a href="https://savetherobot.wordpress.com/2007/06/03/the-story-you-tell-yourself/">it was included as a joke for a couple of seconds</a>, and not as a serious attempt to prevent home recording of the album.</p>


<p>Where Barry Fox <em>wasn&#39;t</em> wrong was in his assessment that the system didn&#39;t work very well. Repeated plays of vinyl dampen the ability to reproduce high frequencies, and it seems that often the spoiler signal was either audible during regular playback, or didn&#39;t have sufficient impact upon recordings. Either way, after a few plays it was destined to disappear due to regular wear and tear on the record&#39;s groove.</p>



<p>In the end, the best information I found about this 1970s development with vinyl copy-protection was included as incidental information on this Google Answers thread - &#39;<a href="https://answers.google.com/answers/threadview?id=56500">Did Recording Industry Intend Compact Discs (CDs) to Have Copy Protection?</a>&#39;</p>

<p>It seems a shame that the history is not more widely available on the internet.</p><p>You see it looks like in the 1970s the music industry tried to stop people copying their content, didn&#39;t do it very well, made themselves unpopular with both the consumer and the artists they are meant to represent, and eventually had to recant. What is it they say about those who fail to learn from history?</p>
                                        
</div> <!-- end div entry-body -->

</div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 12:18:53 +0000</pubDate>
      <source>https://www.currybet.net/cbet_blog/2008/01/copy-protection-for-vinyl-in-t.php</source>
    </item>
    <item>
      <title>The Raspberry Pi as a Stratum-1 NTP Server (2012)</title>
      <link>https://www.satsignal.eu/ntp/Raspberry-Pi-NTP.html</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div width="100%"><tbody><tr><td>



</td><td></td><!--msnavigation--><td>

<p><SPAN size="2">As an experiment, I purchased one of the low-cost
credit-card-size <a href="http://www.raspberrypi.org/" target="_blank">Raspberry
Pi</a> computers, and have configured it to run NTP (Network Time Protocol). 
I have also used this board with a GPS receiver with pulse per second (PPS) output to make a
stratum-1 NTP server, but as I know little of Linux, it has taken some time to
achieve this aim!  There are some helpful Linux commands scattered throughout
this page.  These notes are almost as much for my own records for
the next time I need to visit this project, but I hope they may be helpful to
others.</SPAN></p>

<p><SPAN size="2">If you want to get started quickly, with the best results for
minimum fuss, please see my <a href="https://www.satsignal.eu/ntp/Raspberry-Pi-quickstart.html">Raspberry Pi NTP
quick-starter</a> page.  Please also see <a href="https://www.satsignal.eu/ntp/Raspberry-Pi-quickstart.html">that
page</a> for issues with the Jessie release of Linux, and with the newer
Raspberry Pi model 3.</SPAN></p>

<p><SPAN size="2">I start by describing how to get the Raspberry Pi running with
just a LAN connection - no display, keyboard or mouse - a so-called <i>headless</i>
operation.  I then describe how to configure NTP for your environment, and
adding a GPS/PPS receiver to convert your box into a stratum-1 NTP server
including the operating system updates needed.  Next, I note a couple of
problems I had with the first GPS receiver I tried, and how I cured those with a
different GPS receiver to produce a stratum-1 NTP server consuming about 4
watts.  The easiest approach with good performance is described <a href="#user-mode">here</a>.  </SPAN></p>

<p><SPAN size="2">Since starting this page there have been two developments
which make the process somewhat easier - a program has been developed which
allows the use of an unmodified operating system by working in <a href="#user-mode">user-mode</a>
rather than kernel-mode PPS, and a module is now available which plugs directly
onto the 26-pin GPIO header of the Raspberry Pi, so <a href="#no-soldering">no
soldering is involved</a>.  My thanks to Folkert van Heusden and Anthony
Stirk for these developments.</SPAN></p>

<ul>
  <li><SPAN size="2">No soldering required - <a href="http://ava.upuaut.net/store/index.php?route=product/product&amp;path=59_60&amp;product_id=95">NTPI
    Raspberry Pi GPS addon board</a> - uses GPIO 18</SPAN></li>
  <li><SPAN size="2">rpi_gpio_ntp (no longer available, was: http://vanheusden.com/time/rpi_gpio_ntp/)
    - user-mode PPS support (if Kernel-mode PPS support unavailable, e.g. early
    Raspbian)</SPAN></li>
</ul>

<p><SPAN size="2"> Note that the Adafruit GPS Hat uses GPIO 4, physical pin 7,
so you would need to change the commands given in this document.  Later additions have included remote <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#ntp">monitoring
of the NTP server</a> performance, and more general monitoring of the Raspberry
Pi using the standard <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#snmp">SNMP functions</a>, with an additional
<a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#cpu-temp">CPU
temperature</a> monitoring add-on.  My main <a href="https://www.satsignal.eu/raspberry-pi/index.html">Raspberry
Pi page</a> may also be of interest.</SPAN></p>

<p><SPAN size="2">Note that good performance is dependant on the GPS unit having
a clear view of the sky, particularly the southern part of the sky if you are in
the northern hemisphere.  With older GPS receivers this required an outdoor
antenna, but more modern units such as those mentioned here may well work
indoors providing that some sky is visible, perhaps on the top floor of the
building (as I am).  If you have an &#34;RF-proof&#34; roof (lead-lined,
perhaps?!) or certain windows with a lining to stop incoming heat, or wall construction
including metal, you may still need an outdoor antenna, and almost certainly if
you live in a basement!  Whilst weather will normally have only a small
effect in the signal - e.g. heavy rain - it&#39;s possible that a layer of snow
could attenuate the signal enough to stop the GPS receiving enough signal. 
Monitoring performance may help you spot these problems. (Thanks to Joe,
HB9DRT for the information on snow - I&#39;ve only seen that problem once here
during an exceptionally cold winter).</SPAN></p>

<p><SPAN size="2">If changes in ambient temperature are reducing the precision
you expected, look at <a href="https://www.satsignal.eu/ntp/Raspberry-Pi-ntpheat.html">ntp-heat</a>.</SPAN></p>

<h2>Introduction</h2>

<p><SPAN size="2">The Raspberry Pi is a credit-card size computer,
available from distributors across the world.  I bought an attractive blue
case and 5 V, 2 A power-supply from <a href="https://www.modmypi.com/" target="_blank">ModMyPi</a>. 
You can see the Ethernet lead on the left, and the 4 GB SD card with the
operating system on the right, together with the micro-USB power lead. 
There is a model-B (shown and used below), and a lower-spec model-A which may
become available at some time in the future.  I&#39;m using the 512 MB model-B,
introduced in Autumn 2012.</SPAN></p>

<p></p>

<h2><a name="results">What results can I expect?</a></h2>

<p><SPAN size="2">Shown below are the offset results with the Raspberry Pi in
three configurations: with WAN-only connections syncing to the Internet (as you
might find it a typical home situation), with
LAN connections to a local stratum-1 server, and acting as a
stratum-1 server itself with two different small GPS/PPS receivers as the reference clock.  Any
glitches in the live data are likely to be the result of me rebooting, making configuration
changes, or the GPS signal being less than normal.  The normal NTP configuration is listed <a href="#ntp-conf" target="_blank">here</a>. 
As expected, syncing from the LAN produces better results than from the Internet
(WAN), and making the device into a stratum-1 server results in even lower
offsets.</SPAN></p>

<p><SPAN size="2">Zero offset corresponds to the middle line of the graph, as the
utility I use is incapable of plotting negative values.  I therefore add
half the Y-axis range to the actual values before plotting.  <i>Note: </i>these
graphs are <i> not</i> all to the same vertical scale!</SPAN></p>

<h4>Offset using Internet servers alone - millisecond scale</h4>

<p><SPAN size="2"> The resulting performance is good, but it will depend on both the loading of the link between me and
the ISP, and the general load on the ISP&#39;s network and the general Internet. 
Offsets are reported to be with about +/-5 milliseconds (and therefore off-scale
once on the graph below).  The four-line </SPAN><SPAN face="Courier New" size="2">ntp.conf</SPAN><SPAN size="2">
in use at the time is shown below the graph.</SPAN></p>

<table>
  <tbody><tr>
    <td></td>
    <td><SPAN size="2">
      Raspberry Pi #1</SPAN></td>
  </tr>
</tbody></table>

<blockquote>

<pre># Drift file to remember clock rate across restarts
driftfile /var/lib/ntp/ntp.drift
# Servers
pool uk.pool.ntp.org iburst</pre>

</blockquote>

<h4> </h4>

<p><SPAN size="2">Changing to a tight coupling to a local stratum-1 server on the LAN produces much better
results, with timekeeping in the order of 30 microseconds.  I&#39;ve added a
second graph with a +/- 500 microsecond scale to show any larger excursions.</SPAN></p>

<table>
  <tbody><tr>
    <td></td>
    <td><SPAN size="2">Raspberry Pi #1</SPAN></td>
  </tr>
  <tr>
    <td></td>
    <td><SPAN size="2">Same data but on a</SPAN></td>
  </tr>
</tbody></table>


<blockquote>


<pre># Drift file to remember clock rate across restarts
driftfile /var/lib/ntp/ntp.drift
# Servers
server 192.168.0.3 minpoll 5 maxpoll 5 iburst prefer
server 192.168.0.2 minpoll 5 maxpoll 5 iburst
server 192.168.0.7 minpoll 5 maxpoll 5 iburst
pool uk.pool.ntp.org minpoll 10 iburst</pre>

</blockquote>


<h4> </h4>

<p><SPAN size="2">Much better results are obtained using a Trimble Resolution SMT
GPS module, with its PPS pin connected to the GPIO 24 pin for a kernel-mode
&#34;ATOM&#34; ref-clock.  This unit is a &#34;timing&#34; GPS, with 15
ns specified accuracy for the PPS signal.  Each second pulse on the GPIO
pin causes an interrupt in which the CPU clock is noted, and then used by NTP to
make fine adjustments to the software clock speed.  The
transients of a few microseconds amplitude lasting for about an hour may be due to sudden
ambient temperature changes affecting the crystal used by the card&#39;s clock
generator.  Not shown on the graph, but the offset due to a CPU-heavy task (recompiling NTP from source,
taking about 25 minutes) resulted in a 20 s positive excursion,
followed by a 10 s negative excursion as temperatures cooled and NTP
recovered.</SPAN></p>

<table>
  <tbody><tr>
    <td></td>
    <td><SPAN size="2">Raspberry Pi #1</SPAN></td>
  </tr>
</tbody></table>


<h4> </h4>

<p><SPAN size="2">When using a u-blox MEO-6M GPS
module, with its PPS pin connected to the GPIO 24 pin for a kernel-mode
&#34;ATOM&#34; ref-clock, similar results are obtained.  The transient in the middle of the graph is
when a second device was connected to the 5 V line from the USB.  This unit
is a &#34;navigation&#34; GPS, where the PPS is specified about 100 ns, rather
than a &#34;timing&#34; GPS - but
the difference between the two units is most-often masked by the other variations in the
system.</SPAN></p>

<table>
  <tbody><tr>
    <td></td>
    <td><SPAN size="2">Raspberry Pi #1</SPAN><p><SPAN size="2">
      Kernel PPS sync</SPAN></p>
    </td>
  </tr>
</tbody></table>


<p><SPAN size="2">Later, it was noticed that the offset was varying
periodically, and this is an unexpected result.  Below is a sample from 2012 Dec 19-21, with the less stable
period staring at the end of 2012-Dec-19 and finishing at the end of Dec 20. 
More detailed examination of the loopstats data shows an actual period of just over 100
seconds, and it&#39;s being aliased by the 5-minute sampling of MRTG.</SPAN></p>

<table>
  <tbody><tr>
    <td></td>
    <td><SPAN size="2">Raspberry Pi #1</SPAN>
      <p><SPAN size="2">
      Kernel PPS sync</SPAN></p>
    </td>
  </tr>
</tbody></table>


<p><SPAN size="2">To test, I had changed the GPS receiver from U-blox 6M to Adafruit MTK3339
navigation GPS module, and later changed again from the navigation module to
a timing GPS module based on the <a href="http://www.u-blox.com/de/lea-6t.html">U-blox
LEA-6T</a>, to see whether the oscillation was affected.  These changes
made no difference to either the magnitude and the period of the oscillation,
and the amplitude of the oscillation was considerably greater than what would be
expected even from a &#34;navigation&#34; GPS receiver.  So my earlier
theories about navigation versus timing GPS modules, USB loading, and serial I/O
loading were incorrect.  This problem was eventually cured by a firmware
update on Raspberry Pi #1, from version 337601 to version 346337.</SPAN></p>


<h4>
<a name="current">
Current performance - click a graph to get to the performance page for each computer</a></h4>

<p><SPAN size="2">I added a second Raspberry Pi computer and now have both
connected to the two different GPS/PPS receivers mentioned above, but with the
antennas for those receivers in a similar indoor location.  Below is a
comparison of the performance.  Raspberry Pi #1 is located in an unheated
room with a north-facing wall.  Raspberry Pi #3 is also in the office, but situated
a little nearer to a radiator, providing the daily transients.  In mid-November
2013 I moved to a new kernel which was locally compiled with an option to
improve NTP performance.</SPAN></p>

<table>
  <tbody><tr>
    <td><a href="https://www.satsignal.eu/mrtg/performance_raspi-1.php"></a></td>
    <td><SPAN size="2">Raspberry Pi #1</SPAN></td>
  </tr>
<!--
  <tr>
    <td valign="middle" align="center"><a href="../mrtg/performance_raspi-2.php"></a></td>
    <td valign="middle" align="center"><font size="2">Raspberry Pi #2<br>
 512 MB, Linux/3.6.11<br>
      <br>
      Kernel-mode PPS sync<br>
      <a href="http://www.adafruit.com/products/746">
      Adafruit MTK3339</a><br>
      navigation GPS receiver<br>
      in office environment</font></td>
  </tr>
-->
  <tr>
    <td><a href="https://www.satsignal.eu/mrtg/performance_raspi-3.php"></a></td>
    <td><SPAN size="2">Raspberry Pi #3</SPAN></td>
  </tr>
  <tr>
    <td><a href="https://www.satsignal.eu/mrtg/performance_raspi-4.php"></a></td>
    <td><SPAN size="2">Raspberry Pi #4</SPAN></td>
  </tr>
  <tr>
    <td><a href="https://www.satsignal.eu/mrtg/performance_raspi-5.php"></a></td>
    <td><SPAN size="2">Raspberry Pi #5</SPAN></td>
  </tr>
</tbody></table>


<SPAN size="2"><a href="https://www.satsignal.eu/mrtg/performance_raspi-1.php"><br/>
</a>and here&#39;s a slightly different way of looking at the value of the offset,
plotting the absolute value of the offset, red for positive offsets and blue for
negative offsets.  Transient events such as restarts have been removed.<br/>
</SPAN>

<table>
  <tbody><tr>
    <td><a href="https://www.satsignal.eu/mrtg/performance_ntp-pn.php"></a></td>
    <td><SPAN size="2">Raspberry Pi #1</SPAN></td>
  </tr>
<!--
  <tr>
    <td valign="middle" align="center"><a href="../mrtg/performance_ntp-pn.php"></a></td>
    <td valign="middle" align="center"><font size="2">Raspberry Pi #2<br>
 512 MB,
      Linux/3.2.27+<br>
      <br>
      Kernel-mode PPS sync<br>
      <a href="http://www.adafruit.com/products/746">
      Adafruit MTK3339
      navigation</a><br>
      GPS receiver
      in office environment</font></td>
  </tr>
-->
  <tr>
    <td><a href="https://www.satsignal.eu/mrtg/performance_ntp-pn.php"></a></td>
    <td><SPAN size="2">Raspberry Pi #3</SPAN></td>
  </tr>
  <tr>
    <td><a href="https://www.satsignal.eu/mrtg/performance_ntp-pn.php"></a></td>
    <td><SPAN size="2">Raspberry Pi #4</SPAN></td>
  </tr>
  <tr>
    <td><a href="https://www.satsignal.eu/mrtg/performance_ntp-pn.php"></a></td>
    <td><SPAN size="2">Raspberry Pi #5</SPAN></td>
  </tr>
</tbody></table>



<h2>Creating an SD card with the OS</h2>
<p><SPAN size="2">You can buy an SD card with the Linux OS installed and ready
to go.  As I knew I would need to make modifications to the OS I did buy a
ready-programmed SD card just in case, but I made my own by
following the steps here: <a href="http://www.raspberrypi.org/downloads" target="_blank">http://www.raspberrypi.org/downloads</a></SPAN></p>
<ol>
  <li><SPAN size="2">Download an OS image for the SD card - <a href="http://downloads.raspberrypi.org/images/raspbian/2012-09-18-wheezy-raspbian/2012-09-18-wheezy-raspbian.zip" target="_blank">2012-09-18-wheezy-raspbian.zip<br/>
    </a> </SPAN></li>
  <li><SPAN size="2">Unzip the contents of the Zip archive to a .IMG file</SPAN></li>
  <li><SPAN size="2">Download the SD card writer program: <a href="http://sourceforge.net/projects/win32diskimager/" target="_blank">Win32DiskImager<br/>
    </a> </SPAN></li>
  <li><SPAN size="2">Use the Disk Imager to write the OS image to the SD card</SPAN></li>
</ol>
<p><SPAN size="2">I then plugged in the SD card to the Raspberry Pi, connected
it to the network, and applied power...</SPAN></p>

<h2><a name="no-soldering">The no-soldering way</a></h2>
<p><SPAN size="2">Many folk have asked about adding a GPS to
the Raspberry Pi without needing soldering, and now that has become a reality thanks to the <a href="http://ava.upuaut.net/store/index.php?route=product/product&amp;path=59_60&amp;product_id=95" target="_blank">NTPI
GPS Addon Board</a> produced by Nevis Computers Ltd in the UK.  I used the <a href="http://vanheusden.com/time/rpi_gpio_ntp/" target="_blank">rpi_gpio_ntp
program</a> was developed by Folkert van Heusden and <a href="http://www.febo.com/pipermail/time-nuts/2013-June/077484.html">announced</a>
in the Time-Nuts mailing list, which allows user-mode working with the PPS
signal, thus not requiring a special version of the operating system. 
Current versions of Raspbian no longer need this.</SPAN></p>
<p><SPAN size="2">This is what you get in the box for the NTPI Raspberry Pi GPS
Addon Board (with the puck antenna option):</SPAN></p>
<ul>
  <li><SPAN size="2">A card with the timing quality GPS device and SMA connector
    which plugs into the 26-pin GPIO connector on the Pi.</SPAN></li>
  <li><SPAN size="2">An <i> optional</i> magnetic puck GPS antenna, or you can use your own
    antenna with an SMA connector.</SPAN></li>
  <li><SPAN size="2">You may also need: A CR2032 battery for backup, which fits on to the underside
    of the card.</SPAN></li>
</ul>
<p></p>
<p><SPAN size="2">To get this working with a Raspberry Pi, these are the steps I
took.  I&#39;ve included the steps to add MRTG monitoring and remote file
access from Windows systems, but you may omit those if you don&#39;t need them.</SPAN></p>
<ul>
  <li><SPAN size="2">Set up my router to reserve an IP address for the Raspberry
    Pi</SPAN></li>
  <li><SPAN size="2">Altered the name of the device - <a href="http://simonthepiman.com/how_to_rename_my_raspberry_pi.php">how
    to</a>.</SPAN></li>
  <li><SPAN size="2">Configured NTP to use the <a href="#pool">pool directive</a>,
    allow <a href="#remote-monitoring">remote monitoring</a>, and <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#ntp">log
    statistics</a>.</SPAN></li>
  <li><SPAN size="2"><a href="#compile-ntp">Updated NTP</a> to the current development
    version (I used an FTP copy from another Pi).</SPAN></li>
  <li><SPAN size="2"><a href="#serial" target="_blank">Setup serial I/O</a> on the
    Raspberry Pi.</SPAN></li>
  <li><SPAN size="2">Installed and configured gpsd and its utilities.</SPAN></li>
  <li><SPAN size="2">Configured NTP to use the shared memory for coarse time
    (driver type 28.0).</SPAN></li>
  <li><SPAN size="2">Installed the PPS tools (sudo apt-get install pps-tools)</SPAN></li>
  <li><SPAN size="2">Installed the rpi_gpio_ntp
    program from Folkert van Heusden (only for very early Raspbian
    versions).</SPAN></li>
  <li><SPAN size="2">Configured NTP to use the <a href="#user-mode">shared
    memory for PPS</a> (driver type 28.1).</SPAN></li>
  <li><SPAN size="2">Installed <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#snmp">SNMP</a> and configured
    various MRTG data collectors (optional).</SPAN></li>
  <li><SPAN size="2">Configured SNMP to monitor the <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#cpu-temp" target="_blank"> CPU
    temperature</a> (optional).</SPAN></li>
  <li><SPAN size="2">Installed <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#samba">SAMBA</a>, so that I could
    access the NTP statistics from the PC (optional).</SPAN></li>
  <li><SPAN size="2">Set the PC for wireless networking
    Wi-Fi USB dongle (optional).</SPAN></li>
</ul>
<table>
  <tbody><tr>
    <td><SPAN size="2">Installed onto the Raspberry Pi</SPAN>
      <p><SPAN size="2">Note that this board uses </SPAN></p>
    </td>
    <td><SPAN size="2"></SPAN></td>
  </tr>
</tbody></table>
<p><SPAN size="2">How well does this user-mode work, you may ask.  Well
it&#39;s pretty good, although not <i>quite</i> as good as kernel-mode, but more
than adequate for the precision offered by the Raspberry Pi.  Perhaps the
best that the Pi could do otherwise would be syncing over a LAN connection to a
stratum-1 NTP server.  The offset plot comparison below shows the Pi synced
to a stratum-1 server but over Wi-Fi, and then the performance after installing
the NTPI GPS Addon board.  An ideal plot would be a straight line at the
500 microsecond level on this graph (I have to add an offset as MRTG can&#39;t plot
negative numbers).  The 6-hour averaged jitter reported by NTP dropped from
100-150 microseconds to under 4 microseconds.  The improvement in going
from Wi-Fi sync to PPS sync is obvious!</SPAN></p>
<blockquote>
  <p><SPAN size="2"></SPAN></p>
</blockquote>
<h2><a name="easy">The easy way for kernel-mode PPS</a></h2>
<h3>Updating to the latest Raspbian Linux</h3>
<p>The version of Linux you are running is displayed when you log in, but you
can also use the command:</p>
<p><SPAN color="#000066" face="Courier New">  $ uname -a</SPAN></p>
<p>to show what version is running.  I started with:</p>
<p>To update my software, I ran the commands:</p>
<p><SPAN color="#000066" face="Courier New">  $	sudo apt-get update</SPAN></p>
<p>and after rebooting I ended with:</p>
<h3>Configuring Linux for PPS on the GPIO port</h3>
<p>Thanks to e-mails from Olav Andrarde and Timo Kokkonen I have discovered that
PPS support has now been added to the Linux available for the Raspberry Pi,
although you do need to add a couple of lines to the configuration to enable it. 
As Raspbian in continually evolving, the exact method for making these changes
also evolves, so depending on when you downloaded Raspbian you should follow
either the <a href="#2014-11">November 2014</a> or the <a href="#2015-02">February
2015</a> sections below.  I&#39;ve put the more recent information first,
although I normally prefer things to be listed in chronological order like in a
diary!</p>
<h4> <a name="2015-02"> Raspbian versions around February 2015</a></h4>
<p>After updating another Raspberry Pi to the most recent Raspbian - for a new
Raspberry Pi 2 which I had purchased - I discovered that things had changed once
again.  These details from my <a href="https://www.satsignal.eu/ntp/Raspberry-Pi-quickstart.html">quick-start
page</a>.  Linux versions: 3.18.6+  3.18.7-v7</p>
<p><SPAN color="#000066" face="Courier New">  $ sudo nano /boot/config.txt
- Add </SPAN><SPAN color="#400000" face="Courier New">dtoverlay=pps-gpio,gpiopin=18</SPAN>
on a new line.</p>
<p><SPAN face="Courier New"><SPAN color="#000066">  $ sudo nano
/etc/modules</SPAN> </SPAN> Add <SPAN color="#000066" face="Courier New">pps-gpio</SPAN>
on a new line.</p>
<p>Now proceed to <a href="#pps-check">check that PPS is working</a>.</p>
<h4> <a name="2014-11"> Raspbian versions around November 2014</a></h4>
<p>There are two steps required to enable PPS on the GPIO port, one to tell the
kernel to include the support, and one to actually cause the support module to
be loaded.  First, tell the kernel that GPIO pin 18 is to be used. 
This enables the PPS-GPIO support in the kernel.  Add this text to the
/boot/cmdline.txt file:</p>
<p><SPAN color="#000066" face="Courier New">  $ sudo nano /boot/cmdline.txt</SPAN></p>
<p>Next, you need to tell the kernel to load the module which provides this
support.  Quite why this can&#39;t be automatic I don&#39;t know!  Add the
module name to the end of the file /etc/modules.</p>
<p><SPAN color="#000066" face="Courier New">  $ sudo nano /etc/modules</SPAN></p>
<h3><a name="pps-check">Checking that PPS is working</a></h3>
<p>To check that the module is loaded, you can use the lsmod command, for
example:</p>
<p><SPAN color="#000066" face="Courier New">  $ lsmod | grep pps</SPAN></p>
<p>The output should be similar to:</p>
<p>  pps_gpio 2529 1</p>
<p>You should now be able to run the ppstest command and see the transitions
once per second, for example:</p>
<p><SPAN color="#000066" face="Courier New">$ sudo ppstest /dev/pps0</SPAN> # press Ctrl-C to cancel.. </p>
<p>Note that there is no value given for the &#34;clear&#34; time.  Gary
E Miller reports that the pps-gpio driver only looks for one edge, the positive
going edge.  If you are using a different GPS device from those mentioned
here you may need to a 3.3 volt output inverter in the PPS line from the GPS.</p>
<h3> Hauke Lampe&#39;s approach</h3>
<p><SPAN size="2">Historical information only: Since I first wrote this page, Hauke Lampe has made
available a pre-configured Raspberry Pi OS image <a href="http://ntpi.openchaos.org/downloads/">here</a>. 
His <a href="http://ntpi.openchaos.org/pps_pi/">write-up</a> is based on his version of a serial GPS as I describe here, but you need do
none of the building and configuration work I mention here.  My Raspberry
Pi #3 is running from this OS image, and I&#39;m developing the GPS installation at
the moment.  The comments and changes so far:</SPAN></p>
<ul>
  <li><SPAN size="2">Altered the IP address for the Ethernet port to a fixed
    (static) address - <a href="http://www.raspberryshake.com/raspberry-pistatic-ip-address/">how
    to</a>.</SPAN><SPAN face="Courier New" size="2">192.168.0.51 
    raspberry-pi-1</SPAN><SPAN size="2"><SPAN face="Courier New"> </SPAN>
    to the file (with the address and name of your RPi, of course).  On
    Windows-8, note that Windows Defender may try and replace an edited hosts
    file with the default one, thus removing your changes!  On Windows, the
    file to be edited is \Windows\system32\drivers\etc\hosts, and you may need
    Administrator level access to edit that file.</SPAN></li>
  <li><SPAN size="2">Altered the name of the device - <a href="http://simonthepiman.com/how_to_rename_my_raspberry_pi.php">how
    to</a>.</SPAN></li>
  <li><SPAN size="2">Configured NTP to use the <a href="#pool">pool directive</a>,
    allow <a href="#remote-monitoring">remote monitoring</a>, and <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#ntp">log
    statistics</a>.</SPAN></li>
  <li><SPAN size="2">Installed <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#snmp">SNMP</a> and configured
    various MRTG data collectors.</SPAN></li>
  <li><SPAN size="2">Installed <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#samba">SAMBA</a>, so that I could
    access the NTP statistics from the PC.</SPAN></li>
  <li><SPAN size="2">Set the PC for wireless networking with an old NetGear
    Wi-Fi USB dongle (later replaced with an Edimax unit).</SPAN></li>
</ul>
<p><SPAN size="2">Quite a few of the &#34;standard&#34; Raspberry Pi utilities
are not present in this OS image, and will need to be downloaded separately with
apt-get.  Here&#39;s what it looks like at the moment.  The patch antenna
is attached to the white <a href="https://www.modmypi.com/" target="_blank">ModMyPi</a>
case with double-sided sticky tape, and the receiver board with a single nut and
bolt.  The leads were also from ModMyPi, and the 150 mm length is not
required here!  The Wi-Fi dongle is on the left.  Performance graphs
are <a href="https://www.satsignal.eu/mrtg/performance_raspi-3.php">here</a>.</SPAN></p>
<p><a name="neo-6m"></a></p>

<h2><a name="headless">Running the Raspberry Pi as a headless system</a></h2>
<p><SPAN size="2">As my trial application is for an NTP server, I don&#39;t need a
display on the Raspberry Pi, or for that matter a keyboard and mouse permanently connected.  All my
interaction will be via a terminal, and even that will be emulated via program
running on a remote PC, with connectivity through the Ethernet port on the
Raspberry Pi.  Such operation is commonly called a <i>headless server</i>. 
A power supply and case were the only items I added to the as-supplied Raspberry
Pi.  There is some advice on running headless here: <a href="http://glynrob.com/hardware/raspberry-pi-headless/" target="_blank">http://glynrob.com/hardware/raspberry-pi-headless/</a></SPAN></p>
<p><SPAN size="2">You need a terminal program which will work in SSH mode - I
used <a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html" target="_blank">PuTTY</a>
on a Windows XP PC.</SPAN></p>
<p><SPAN size="2">I watched on my router which runs DD-WRT to see the IP address
which had been assigned to the Pi, and I then used that address to run
PUTTY in SSH mode.  I got a connection right away with the default user
name and password.  You can save the settings for the Pi from within PUTTY
(I called mine &#34;RaspberryPi&#34;), and make a Windows shortcut to connect
to your Pi with values such as:</SPAN></p>
<blockquote>
  <pre>Target:   C:\Tools\PuTTY\PUTTY.EXE -load RaspberryPi
Start in: C:\Tools\PuTTY</pre>
</blockquote>
<p><SPAN size="2">Your path and saved settings name will be different.</SPAN></p>
<p><SPAN size="2">Linux command to disconnect and logout:</SPAN><SPAN face="Courier New">
<SPAN size="2">logout</SPAN></SPAN></p>
<p><SPAN size="2">By the way, by adding an X-windows server
program such as <a href="http://sourceforge.net/projects/xming/" target="_blank"> Xming</a> to your PC you can see the graphics from the Raspberry Pi
as well, should you wish, but that&#39;s <i> not</i> required for the operations described
here - you can do everything directly from the command-line.</SPAN></p>
<h2>Updating the OS</h2>
<p><SPAN size="2">I followed the advice <a href="http://glynrob.com/hardware/raspberry-pi-headless/" target="_blank">here</a>
to update the OS, although I don&#39;t think that it was really necessary as the OS
downloaded was only a month old.  To run the commands requires privileged
access, achieved here by prefixing the command with <SPAN face="Courier New">sudo</SPAN>. 
These commands take considerable time to run, and require Internet access from
your Pi.  Allow 30 - 45 minutes.</SPAN></p>
<ol>
  <li>
    <pre>$ sudo apt-get update</pre>
  </li>
  <li>
    <pre>$ sudo apt-get dist-upgrade</pre>
  </li>
</ol>
<p><SPAN size="2">You are likely to need to restart the OS after making these changes:</SPAN></p>
<ol>
  <li>
    <pre>$ sudo reboot</pre>
  </li>
</ol>
<p><SPAN size="2">If you want to determine what upgrades are pending, try:</SPAN></p>
<ol>
  <li><pre>$ sudo apt-get --just-print upgrade</pre>
  </li>
</ol>
<p><SPAN size="2">and redirect the output to a text file.  Thanks to Graham
in County Durham, UK, for that tip.</SPAN></p>
<h2>Setting the time zone</h2>
<p><SPAN size="2">You may want to check that the Raspberry Pi is configured to
give you the time in your local time zone:  Use the command:</SPAN></p>
<blockquote>
<pre>$ sudo dpkg-reconfigure tzdata</pre>
</blockquote>
<p><SPAN size="2">and select the appropriate region and town.  For the UK,
I selected Europe/London.  The procedure will reflect the timezone data and
current date and time in both the local time and UTC when it completes.</SPAN></p>
<h2><a name="Wi-Fi">Configuring Wi-Fi</a></h2>
<p><SPAN size="2">Although there is a graphical interface with which to
configure the Wi-Fi, it didn&#39;t seem to work for me.  In any case, if you
are operating headless, you may not even have access to the GUI.  The
network adapter I used was the tiny <a target="_blank" href="http://www.edimax.com/edimax/merchandise/merchandise_detail/data/edimax/global/wireless_adapters_n150/ew-7811un"> Edimax EW-7811UN 150Mbps</a> Wireless Nano USB Adapter
unit, which I got from Amazon (fair price and good delivery).</SPAN></p>
<p><SPAN size="2">There appear to be three steps:</SPAN></p>
<ol>
  <li><SPAN size="2">Convert the Wi-Fi password into a hex string (this may be
    optional):</SPAN></li>
  <li><SPAN size="2">Edit the /etc/wpa_supplicant/wpa_supplicant.conf file</SPAN></li>
  <li><SPAN size="2">Edit the /etc/network/interfaces file </SPAN></li>
</ol>
<p><SPAN size="2">You can then use the <SPAN face="Courier New">$ sudo ifdown
wlan0</SPAN> and <SPAN face="Courier New">$ sudo ifup wlan0</SPAN> commands to
restart the wireless networking.  I would also advise a reboot to ensure
that everything has gone as expected.</SPAN></p>
<h2>How to edit the NTP configuration</h2>
<p><SPAN size="2">The NTP configuration file lives in the /etc directory, so you
can change to that directory to edit the file.  Because the file is a
system file, you need to <i> sudo</i> command to allow to save the edited version of the
file, but first, I made a copy (<i>cp</i>) of the supplied file just in case I messed
up and had to revert to the working NTP configuration.  I used the <i> vi</i>
editor which is supplied with the OS, and there are instructions for vi <a href="http://www.cs.colostate.edu/helpdocs/vi.html" target="_blank">here</a>,
and also <a href="https://www.guru99.com/the-vi-editor.html" target="_blank">at
Guru99.com</a> as part of a <a href="https://www.guru99.com/unix-linux-tutorial.html" target="_blank">Linux/Unix
tutorial for beginners</a>. 
The <i> nano</i> editor which I discovered later, and which is supplied with the Raspberry Pi, is a much better choice - much
easier to use!</SPAN></p>
<ol>
  <li>
    <pre><SPAN face="Courier New" size="2">$ cd /etc</SPAN></pre>
  </li>
  <li>
    <pre><SPAN face="Courier New" size="2">$ sudo cp ntp.conf ntp-original.conf</SPAN></pre>
  </li>
  <li>
    <pre><SPAN face="Courier New" size="2">$ sudo nano ntp.conf
 </SPAN></pre>
  </li>
</ol>
<h2>Configuring NTP to allow for <a name="remote-monitoring"> remote monitoring</a></h2>
<p><SPAN size="2">I wanted to be able to monitor NTP from another PC on my LAN,
rather than adding the MRTG monitoring program to the Pi, however there are
lines which restrict access to the NTP running on the Pi in its default
installation.  These lines in the ntp.conf file start with the keyword
&#34;restrict&#34;.  I removed these restrictions by commenting out these
lines - which is achieved by adding a hash character at the start of the
line.  For example:</SPAN></p>
<blockquote>
  <pre>Replace:
    restrict -4 default kod notrap nomodify nopeer noquery

With:
    # restrict -4 default kod notrap nomodify nopeer noquery</pre>
</blockquote>
<p><SPAN size="2">After changing ntp.conf, you need to restart the ntp daemon:</SPAN></p>
<ol>
  <li>
    <pre><SPAN face="Courier New" size="2">$ sudo /etc/init.d/ntp restart</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">If you were concerned about security on your LAN you might
wish to be more selective in making changes to the restrictions.</SPAN></p>
<h2><a name="pool">Configuring NTP for best results from the local LAN</a></h2>
<p><SPAN size="2">While the default ntp.conf contents will work correctly for
most purposes, they do not take advantage of the new NTP POOL directive to
specify pool servers.  I also altered the generic &#34;debian&#34; pool
to the more local &#34;UK&#34; pool.  I therefore changed the server
pool. lines to a single pool directive:</SPAN></p>
<blockquote>
  <pre>Replace:
    server 0.debian.pool.ntp.org iburst
    server 1.debian.pool.ntp.org iburst
    server 2.debian.pool.ntp.org iburst
    server 3.debian.pool.ntp.org iburst</pre>
  <pre>With:
    pool uk.pool.ntp.org iburst</pre>
</blockquote>
<p><SPAN size="2">My own LAN has three stratum-1 NTP servers, one on FreeBSD and
two running Windows, so I added those
before the pool servers.  Of course, this is specific to my LAN.  For
the minimum offset, I made NTP poll the local servers at 32 (2^^5) second
intervals, and did not allow that to drift upwards towards the 1024 seconds
maximum interval that NTP would reach left to its own devices.  I did this
with the minpoll and maxpoll qualifiers.  However, I did not want to force
the Internet servers to be interrogated that often (it is considered at best
impolite, and at worst could get you blocked from a server), so I therefore made
the minimum polling interval for the Internet servers 1024 seconds
(2^^10).  I removed things which had been commented out to simplify the file,
and make it easier to understand.  Hence my ntp.conf ended up as follows:</SPAN></p>
<blockquote>
  <pre># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help

# Drift file to remember clock rate across restarts
driftfile /var/lib/ntp/ntp.drift

# Servers
server 192.168.0.3 minpoll 5 maxpoll 5 iburst prefer
server 192.168.0.2 minpoll 5 maxpoll 5 iburst
server 192.168.0.7 minpoll 5 maxpoll 5 iburst
pool uk.pool.ntp.org minpoll 10 iburst</pre>
</blockquote>
<p><SPAN size="2">Remember to restart NTP after making the changes.  From a
Windows monitoring PC with <a href="https://www.satsignal.eu/ntp/setup.html" target="_blank">NTP installed</a>
I now see:</SPAN></p>
<blockquote>
<pre>C:\&gt;ntpq -p raspi
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
*pixie           .PPS.            1 u   26   32  377    0.467   -0.010   0.023
+feenix          .PPS.            1 u    4   32  377    0.603   -0.226   0.039
+stamsund        .PPS.            1 u   31   32  377    0.586    0.004   0.040
-dns0.rmplc.co.u 193.62.22.74     2 u  707 1024  377   21.915    3.490   2.295
-dns1.rmplc.co.u 193.62.22.74     2 u  177 1024  377   23.736    4.609   3.621
-dawn.rt.uk.eu.o 193.79.237.14    2 u  277 1024  377   20.508    3.150   3.065
-lyla.preshweb.c 129.215.42.240   3 u  544 1024  377   28.082    5.937   3.944

C:\&gt;ntpq -c rv raspi
associd=0 status=0615 leap_none, sync_ntp, 1 event, clock_sync,
version=&#34;ntpd 4.2.6p5@1.2349-o Fri May 18 20:30:57 UTC 2012 (1)&#34;,
processor=&#34;armv6l&#34;, system=&#34;Linux/3.2.27+&#34;, leap=00, stratum=2,
precision=-20, rootdelay=0.467, rootdisp=2.387, refid=192.168.0.3,
reftime=d4365966.98133154  Sat, Oct 27 2012 14:00:22.594,
clock=d4365984.f4e4138b  Sat, Oct 27 2012 14:00:52.956, peer=49569, tc=5,
mintc=3, offset=-0.010, frequency=-43.888, sys_jitter=0.023,
clk_jitter=0.015, clk_wander=0.008</pre>
</blockquote>

<h2><a name="trimble">Configuring for a serial over USB GPS device</a></h2>
<p><SPAN size="2">One approach towards getting NTP to see
the serial part of the GPS receiver output stream (for the coarse part of the
time, the seconds) is to install
the gpsd driver, and this does allow some checking of the basic
connectivity.  The GPS I started using was a Trimble Resolution SMT,
for which I managed to get both an evaluation board and an <a href="http://partiallystapled.com/"> interface board</a> which
converted the serial output into both RS-232 and USB.  I used the serial
over USB rather than the RS-232 for the Raspberry Pi.</SPAN></p>
<table>
  <tbody><tr>
    <td></td>
    <td><SPAN size="2">This is the GPS I used.  It
      is an evaluation board for a timing GPS surface-mount receiver, the
      Trimble Resolution SMT.  This is slightly unusual in having a TSIP-format
      output rather than the standard NMEA format, but the Linux gpsd can
      recognise and accept that format.  The output is on an 8-pin header with a non-standard
      pin spacing!  It&#39;s sensitive enough that I could use a magnetic mount
      GPS puck in my upper floor computer room.</SPAN></td>
  </tr>
  <tr>
    <td></td>
    <td><SPAN size="2">Just as I acquired the board,
      there was an offer on the time-nuts mailing list for a <a href="http://partiallystapled.com/">
      ready-made interface</a>.  This has a matching 8-pin connector, and
      provides a PPS output, serial output at RS-232 levels (not used here), and has a built-in serial to USB converter!  Ideal!</SPAN>
      <p><SPAN size="2">Do be careful in using other GPS units that you don&#39;t
      exceed +3.3 V on the PPS signal fed to the Raspberry Pi, as a 5 V signal
      level will <b> damage</b> the device.  I used a resistive divider
      (not shown) to reduce the level to a nominal 3.2 V.</SPAN></p>
      <p><SPAN size="2">I soldered a 3k9 * + 6k8 resistive divider to the PPS
      header, and then soldered a twin lead fed to a 0.1-inch header I happened to have lying around. 
      This I connected to pins
      GND and GPIO-24 on the 26-pin <a href="http://elinux.org/RPi_Low-level_peripherals"> Raspberry Pi GPIO
      header</a>.</SPAN></p>
    </td>
  </tr>
</tbody></table>
<p><SPAN size="2">First, I connected the device to the lower USB port, and then
checked what was seen on the USB ports.</SPAN></p>
<ol>
  <li>
    <pre>$ sudo lsusb
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 002: ID 0424:9512 Standard Microsystems Corp.
Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp.
Bus 001 Device 005: ID 04d8:00df Microchip Technology, Inc.</pre>
  </li>
</ol>
<p><SPAN size="2">It seems that my GPS is appearing as 005 in that list, but
what will it be named?  To check this, you need to look through one of the
Linux log files:</SPAN></p>
<ol>
  <li>
    <pre>$ more /var/log/syslog</pre>
  </li>
</ol>
<p><SPAN size="2">and in my case about the time I plugged in the device there
was a reference to: <SPAN face="Courier New">ttyACM0:</SPAN>, and I recognised
tty as a serial port (TeleType from long ago!).  If you are using a real
serial device it will appear as ttyAMA0.</SPAN></p>
<p><SPAN size="2"> The next steps are to
install the gpsd software, and start the gpsd service pointing to the device
name just discovered:</SPAN></p>
<ol>
  <li>
    <pre><SPAN face="Courier New" size="2">$ sudo apt-get install gpsd gpsd-clients python-gps</SPAN></pre>
  </li>
  <li>
    <pre><SPAN face="Courier New" size="2">From one report I had received, if you get errors with the above step you may need to run an update to apt-get:
$ sudo apt-get update
and possibly then:
$ sudo apt-get upgrade
which may upgrade the entire OS to the current version.</SPAN></pre>
  </li>
  <li>
    <pre><SPAN face="Courier New" size="2">$ sudo gpsd /dev/ttyACM0 -n -F /var/run/gpsd.sock</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">At this point, you should be able to see a text-mode output
from your GPS receiver by running the command &#34;<SPAN face="Courier New">cgps
-s</SPAN>&#34;, something like the following.</SPAN></p>
<ol>
  <li>
    <pre><SPAN face="Courier New" size="2">$ cgps -s</SPAN></pre>
  </li>
</ol>
<p></p>
<p><SPAN size="2">Note that as this is a timing-mode GPS, it will prefer
satellites which have a higher elevation, as these are less likely to have
multi-path effects or reflections.  However, for the level of accuracy for
which we are aiming (microseconds, not nanoseconds), this refinement is not
essential, and I could see no significant difference between a
&#34;timing&#34; GPS and a &#34;position&#34; GPS on the microsecond level. 
Note that you will need to make <i>gpsd</i> start automatically at boot time,
and to tell the configuration tool what device to use, and add the &#34;-n&#34;
option for working with NTP. See the note <a href="#autostart">later in this
document</a>.</SPAN></p>
<h2>Telling NTP the seconds from the GPS</h2>
<p><SPAN size="2">Now that gpsd is working, we can edit the NTP configuration to
add a type 28 reference clock which will make NTP look at the shared memory
created by gpsd.  This can be done for both the coarse time (seconds) and
the fine time (PPS edge) with a 28.0 and a 28.1 driver, although I only use the
28.0 driver here as the Raspberry Pi supports PPS via a kernel-mode driver (more
later).  The first step is
to get the seconds alone, and be aware that this will <i>not</i> be better than
Internet time alone due to the offset of the serial/USB data from the true
second, and because of the variability and drift in this offset.  We will
need to add a connection later between the PPS signal and one of the Raspberry Pi&#39;s I/O
pins to generate a PPS interrupt.  Here is my modified ntp.conf file. 
I&#39;ve used 0.000 for the time1 modifier to start with, so that we can determine
an approximate value for the delay of the serial data from the GPS after ntp is
up and running.  I changed the refid for the type 28 driver to &#34;SHM&#34;
to indicate more clearly that the data is coming from the SHared Memory provided
by gpsd.</SPAN></p>
<p><SPAN size="2">Note that I have marked more than one server as
&#34;prefer&#34;.  This is because if the first preferred server goes
offline, it appears that NTP will no longer accept the PPS data as valid (is
that wise?), so a second preferred server is configured to cover that
possibility.  In my case, it happens because 192.168.0.3 sometimes has an
NTP update, causing its NTP to go offline for some seconds, and hence causes a
glitch in the connected servers.  Having more than one preferred server
should prevent that.</SPAN></p>
<blockquote>
  <pre># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help
 
# Drift file to remember clock rate across restarts
driftfile /var/lib/ntp/ntp.drift</pre>
  <pre># Server from shared memory provided by gpsd
server 127.127.28.0 minpoll 4 maxpoll 4 prefer
fudge 127.127.28.0 time1 0.000 refid SHM stratum 15</pre>
  <pre># Local LAN servers
server 192.168.0.3 minpoll 5 maxpoll 5 iburst prefer
server 192.168.0.2 minpoll 5 maxpoll 5 iburst prefer
server 192.168.0.7 minpoll 5 maxpoll 5 iburst prefer</pre>
  <pre># UK pool servers
pool uk.pool.ntp.org minpoll 10 iburst</pre>
</blockquote>
<p><SPAN size="2">Note that when using a PPS source you <i>must</i> have one
other server marked &#34;prefer&#34;.  In the example above I have added
prefer to the shared memory driver (type 28) so that the combination of PPS and
GPSD would provide the correct time even with no Internet servers.  Looking at the output from ntpq -p after some time we might
see:</SPAN></p>
<blockquote>
  <pre>C:\&gt;ntpq -p raspi
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
xSHM(0)          .SHM.           15 l   15   16  377    0.000  -353.23   1.277
*pixie           .PPS.            1 u   25   32  377    0.484   -0.016   0.105
+feenix          .PPS.            1 u   31   32  377    0.592   -0.120   0.044
+stamsund        .PPS.            1 u   16   32  377    0.546   -0.037   0.083
xns0.luns.net.uk 157.44.176.4     2 u 1656 1024  156   31.904    3.702   5.455
xtime.videxio.ne 131.188.3.223    2 u  45m 1024   74   31.765    8.590   2.796
xlyla.preshweb.c 129.215.42.240   3 u  510 1024  377   25.568    4.793   5.990
-dawn.rt.uk.eu.o 193.67.79.202    2 u  492 1024  367   20.308    2.408   2.903</pre>
</blockquote>
<p><SPAN size="2">and while the SHM driver is present and connected (reach =
377), it has been rejected by NTP (the &#34;x&#34; in the first column),
perhaps because its offset was consistently too great compared to the other
servers.  That&#39;s the purpose of the time1 modifier in the &#34;fudge&#34;
command.  We can see that the SHM output is some 350 milliseconds later, so
we can use that value for time1 to bring the GPS output approximately into line
with UTC, as shown in the edited /etc/ntp.conf below.  (The time values in
the ntpq -p display are all in milliseconds).</SPAN></p>
<p><SPAN size="2"><i>Hint:</i> if at this point the reach field for the SHM
device stays at zero, likely the gpsd wasn&#39;t started with the &#34;-n&#34;
option.  You can make the gpsd always start at system boot time with that -n
option as described <a href="#autostart">later in this note</a>.</SPAN></p>
<blockquote>
  <pre># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help
 
# Drift file to remember clock rate across restarts
driftfile /var/lib/ntp/ntp.drift</pre>
  <pre># Server from shared memory provided by gpsd
server 127.127.28.0 minpoll 4 maxpoll 4
fudge 127.127.28.0 time1 +0.350 refid SHM stratum 15</pre>
  <pre># Local LAN servers
server 192.168.0.3 minpoll 5 maxpoll 5 iburst prefer
server 192.168.0.2 minpoll 5 maxpoll 5 iburst
server 192.168.0.7 minpoll 5 maxpoll 5 iburst</pre>
  <pre># UK pool servers
pool uk.pool.ntp.org minpoll 10 iburst</pre>
</blockquote>
<p><SPAN size="2">The output from ntpq -p then shows the offset for the SHM
driver to be much nearer to zero, and this /might/ be good enough for you if you
are out in the field with no other reference.  But we can do better, and
the next step is to use the precise PPS signal from the GPS to improve the
accuracy down to the microsecond level.</SPAN></p>
<blockquote>
  <pre>C:\&gt;ntpq -p raspi
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
-SHM(0)          .SHM.           15 l    1   16   17    0.000    1.766   0.943
*pixie           .PPS.            1 u   13   32    3    0.421   -0.325   0.194
+feenix          .PPS.            1 u   13   32    3    0.528   -0.644   0.969
+stamsund        .PPS.            1 u   11   32    3    0.409   -0.336   0.145
-dns0.rmplc.co.u 195.66.241.2     2 u   35 1024    1   22.872    3.604   5.037
-mail1.itdojo.or 10.10.120.2      2 u   34 1024    1   38.472    3.324   7.084
 ntp.fundamental 193.62.22.82     2 u   33 1024    1   30.980    2.450   3.837
 82.113.154.206  193.62.22.82     2 u   32 1024    1   19.219    0.683   5.880</pre>
</blockquote>
  <p>Note: if you are working stand-alone, without any Internet servers, you may
  need an extra &#34;flag1 1&#34; in the fudge for the type 28
  ref-clock.  Please see the notes <a href="https://www.satsignal.eu/ntp/Raspberry-Pi-quickstart.html#stand-alone">here</a>
  for further information.  Thanks to Whitham D. Reeve Anchorage, Alaska USA
  for the testing.
 </p>
<h4><a name="GPS-time">But my time is 16 seconds out!</a></h4>
<p><SPAN size="2">I did notice with the GPS unit that I have that it doesn&#39;t
have battery backup, so when it first starts it has to download quite a lot of data
from the GPS satellites before it has full lock.  While the PPS signal
is acquired quite quickly, it takes a few minutes for the receiver to determine
the number of seconds offset between GPS-time GPST) and the usual UTC.  As I
write, that GPST-UTC offset is 16 seconds - the offset is because recent
leap-seconds are not applied to GPS time - <a href="http://en.wikipedia.org/wiki/Global_Positioning_System#Timekeeping_and_leap_seconds">more
information</a>.  The implications of this are different
depending on what other servers you have configured in your ntp.conf file</SPAN></p>
<ul>
  <li><SPAN size="2">If you have some Internet or LAN servers, ntp is clever
    enough to ignore the obvious &#34;bad chimer&#34;, and may simply display a
    large offset for the GPS in the <SPAN face="Courier New"> ntpq -p</SPAN> output when starting
    up.  After
    a few minutes, the offset will revert to the correct value.  The delay
    is not a problem in this case.</SPAN></li>
  <li><SPAN size="2">If you have no other source than the GPS, then you should
    probably wait a few minutes before assuming that even the coarse seconds
    part of the time is correct.  I haven&#39;t checked how long it will take
    NTP to step the clock by the 16 seconds needed after the GPS starts sending
    UTC rather than GPS time.  If your GPS does this, consider adding some
    sort of battery backup so that the GPS-UTC offset is stored while the unit
    is down.</SPAN></li>
</ul>
<p><SPAN size="2">Quite why I saw this issue while using <SPAN face="Courier New">gpsd</SPAN>
is uncertain.  Since writing the above I have been in contact with the
author of <SPAN face="Courier New">gpsd</SPAN> who tells me that protection is
incorporated into the <SPAN face="Courier New">gpsd</SPAN> software whereby it
will not pass on the time to its shared memory until the output from the GPS
receiver has a (GPST-UTC) value in excess of 10 seconds.  So I should never
have seen the 16 seconds faster value at all.</SPAN></p>
<p><SPAN size="2">Please note that this problem is likely peculiar to my
particular GPS receiver - an
evaluation board with no battery backup.  Just be aware of this problem in
case it bites you!  It doesn&#39;t happen with the u-blox pure serial GPS
receiver I describe <a href="#serial">later</a>, as this board has battery
backup.  &#34;Your mileage may vary&#34;, as they say!</SPAN></p>
<h2><a name="pps">PPS - Pulse Per Second for improved precision</a></h2>
<p><SPAN size="2">The next step was to get the PPS working.  This requires
updating the Linux kernel for the Raspberry Pi, and while you can do that
yourself, there is a ready-made kernel image and support modules available on
the Web.  Much of the information below is based on David K&#39;s Web page:</SPAN></p>
<p><SPAN size="2">  </SPAN><a href="https://github.com/davidk/adafruit-raspberrypi-linux-pps"><SPAN size="2">https://github.com/davidk/adafruit-raspberrypi-linux-pps</SPAN></a></p>
<p><SPAN size="2">You can check the version of the kernel you are running at the moment by:</SPAN></p>
<ol>
  <li>
<pre><SPAN size="2" face="Courier New">$ uname -a
Linux raspberrypi 3.2.27+ #250 PREEMPT Thu Oct 18 19:03:02 BST 2012 armv6l GNU/Linux</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">First you need to get the updated kernel image and
modules.  At the time of writing, these were available for the current
version of the OS from chrisprt as mentioned <a href="http://www.raspberrypi.org/phpBB3/viewtopic.php?f=41&amp;t=1970&amp;start=125">here</a>:</SPAN></p>
<p><SPAN size="2">Kernel image: <a href="https://docs.google.com/open?id=0BznvtPCGqrd3ZElKZHEtUDRpUEU">https://docs.google.com/open?id=0BznvtPCGqrd3ZElKZHEtUDRpUEU</a></SPAN></p>
<p><SPAN size="2">These come down as Zip files and, as I wasn&#39;t sure about
downloading these from Google Docs directly on the Raspberry Pi, I downloaded
them to a local Windows FTP server first, and then installed an FTP client on the
Raspberry Pi to drag the Zip files across in FTP image mode - i.e. binary files.</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New"># Installing an FTP client:
$ sudo apt-get install ftp</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">I could then use standard FTP command to drag the files from
my local FTP server to the Raspberry Pi.  I created a directory named pps
below the home user directory for the files, and then unzipped the archives I
had copied:</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ mkdir pps</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ cd pps</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New"># FTP get 3.2.27-pps-g965b922-dirty.zip in binary (image) mode.
# FTP get kernel-pps-gpio24.zip in binary (image) mode
(substitute your own commands here).</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ unzip kernel-pps-gpio24.zip</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ unzip 3.2.27-pps-g965b922-dirty.zip</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">In the pps/kernel-pps-gpio24 directory you will find a file
kernel-pps-gpio24.img.  This must be renamed and moved to the /boot/ directory, while we first
take a safety copy of the original kernel image.</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo mv /boot/kernel.img /boot/kernel.img.orig</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo cp kernel-pps-gpio24.img /boot/kernel.img</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">Now we need to move the module files into the area where the
new kernel expects to find them.  I found the command on the Web page
either confusing or wrong, as I ended up with the wrong structure to start
with.  What it appears to need is:</SPAN></p>
<blockquote>
  <pre>/lib/modules/3.2.27+
/lib/modules/3.2.27+/kernel
/lib/modules/3.2.27+/modules.*</pre>
  <pre>/lib/modules/3.2.27-cutdown+
/lib/modules/3.2.27-cutdown+/kernel
/lib/modules/3.2.27-cutdown+/modules.*</pre>
  <pre>/lib/modules/3.2.27-pps-g965b922-dirty
/lib/modules/3.2.27-pps-g965b922-dirty/kernel/
/lib/modules/3.2.27-pps-g965b922-dirty/modules.*</pre>
</blockquote>
<p><SPAN size="2">You will find both the kernel directory and the modules files
in the unzipped 3.2.27-pps-g965b922-dirty directory, so the following command
may work correctly for you.  I made a mess of this having followed the Web
page verbatim, and not having made allowances for the differences in the file
name.  Assuming you are now in the pps directory, move the required files to
the /lib/modules directory, and add the pps-gpio module to the module list:</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo mv 3.2.27-pps-g965b922-dirty /lib/modules/3.2.27-pps-g965b922-dirty</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2"><SPAN face="Courier New">$ echo &#34;pps-gpio&#34; | sudo tee -a /etc/modules
</SPAN>(<SPAN face="Courier New">Command corrected, thanks Matthew Huxtable!  Alternatively edit 
 /etc/modules using the nano editor to add the pps-gpio at the end.
$ sudo nano /etc/modules</SPAN></SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo reboot  </SPAN></pre>
  </li>
</ol>
    <p><SPAN size="2">You will see the changed kernel name at the next login,
    and you can check with the uname -a command as before:</SPAN>
  </p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ uname -a
Linux raspberrypi 3.2.27-pps-g965b922-dirty #1 PREEMPT Sat Sep 22 16:30:50 EDT 2012 armv6l GNU/Linux
</SPAN> </pre>
  </li>
</ol>
<h4>An aside - what is a module in Linux?

 </h4>
<p><SPAN size="2">You may be used to the idea of device drivers for Windows -
those .SYS files - but what are &#34;modules&#34; in Linux and how do they
relate to device drivers?  I asked that question on the time-nuts list, and
got this reply from Michael Tharp:</SPAN>

 </p>
<blockquote>
  <p><SPAN size="2">&#34;Linux modules are the same, although Linux modules almost always need to be
  compiled against the specific kernel version while Windows drivers are typically only bound to which release you&#39;re running. 
  That is the reason you have to compile the kernel, rather than just plop down a driver
  downloaded from the internet. </SPAN>

 </p>
  <p><SPAN size="2">&#34;That said, the reason your PPS driver is a module is that it makes it easier to tweak options. 
  Almost all modules that are part of the main kernel source (which PPS is, for a year or so)
  can be compiled in rather than as a separate module, but you can pass options to a module as you load it while you cannot do that with a
  built-in.  It also makes it possible to tweak the source, recompile just that module, and test it on the fly rather than recompiling the entire
  kernel and rebooting.&#34;</SPAN>

 </p>
</blockquote>
<p><SPAN size="2">Many thanks, Michael.</SPAN></p>
<h2>Checking the PPS is working</h2>
<p><SPAN size="2">To check that you are running the new kernel and that the <SPAN face="Courier New">pps-gpio</SPAN>
module is loaded, then install the <SPAN face="Courier New">pps-tools</SPAN> and
run it to see the changes on pin 24 (assuming you have a 3.3 V PPS signal
connected.  <b>Warning:</b> do <i>not</i> connect a 5 V signal to the GPIO pins!</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ uname -a
Linux raspberrypi 3.2.27-pps-g965b922-dirty #1 PREEMPT Sat Sep 22 16:30:50 EDT 2012 armv6l GNU/Linux</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ dmesg | grep pps
[ 0.000000] Linux version 3.2.27-pps-g965b922-dirty (root@bt) (gcc version 4.
6.2 (Ubuntu/Linaro 4.6.2-14ubuntu2~ppa1) ) #1 PREEMPT Sat Sep 22 16:30:50 EDT 20
12
[ 1.866364] usb usb1: Manufacturer: Linux 3.2.27-pps-g965b922-dirty dwc_otg_h
cd
[ 12.797224] pps_core: LinuxPPS API ver. 1 registered
[ 12.803850] pps_core: Software ver. 5.3.6 - Copyright 2005-2007 Rodolfo Giome
tti &lt;giometti@linux.it&gt;
[ 12.824858] pps pps0: new PPS source pps-gpio.-1
[ 12.832182] pps pps0: Registered IRQ 194 as PPS source
[ 133.043038] pps_ldisc: PPS line discipline registered
[ 133.044841] pps pps1: new PPS source acm0
[ 133.044879] pps pps1: source &#34;/dev/ttyACM0&#34; added</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo aptitude install pps-tools # may take some time</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo ppstest /dev/pps0 # press Ctrl-C to cancel.. 
trying PPS source &#34;/dev/pps0&#34; 
found PPS source &#34;/dev/pps0&#34; 
ok, found 1 source(s), now start fetching data... 
source 0 - assert 1351501153.999956346, sequence: 47481 - clear 0.000000000, sequence: 0 
source 0 - assert 1351501154.999954601, sequence: 47482 - clear 0.000000000, sequence: 0 
source 0 - assert 1351501155.999951856, sequence: 47483 - clear 0.000000000, sequence: 0 
^C</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">The &#34;clear&#34; entries showing as zero is correct for
this driver implementation.  Note that if you don&#39;t have a PPS signal
connected to GPIO pin 24 the last three lines from the dmesg output may be
missing.  In the output above, the PPS source was only registered some 133
seconds after startup, possibly the length of time it took the GPS to
lock.  On a second system with no PPS connected the last three lines were
missing.</SPAN></p>
<h2><a name="compile-ntp">Getting an NTP with PPS (&#34;ATOM&#34;) support</a></h2>
<p><SPAN size="2">Unfortunately, the version of NTP supplied with the Raspberry
Pi Linux does not support PPS.  Likely it has been compiled to minimise its
memory and disk footprint.  These are the steps to download, compile and
install NTP (with help from <a href="http://www.raspberrypi.org/phpBB3/viewtopic.php?f=41&amp;t=1970&amp;start=25">jbeal&#39;s
posting</a>).  You can choose between a release and a development version
as shown in step 4 below. You could also use a copy of the development tarball on
your own local FTP server.  So from logging in, here are the steps. 
The lines below are shown for development version ntp-dev-4.2.7p397, but you
will need to alter the version number to suit the version you wish to compile. 
The two time-consuming steps (configure and make) appear to be CPU limited
rather than SD-card I/O access limited. 
You can see which version I am currently running <a href="https://www.satsignal.eu/mrtg/performance_ntp.php#versions">here</a>.</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ mkdir ntp                  # make a convenient working directory, if you don&#39;t already have one</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ cd ntp                     # enter that directory</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo apt-get install libcap-dev	# once-off, required to prevent later file not found error
$ sudo apt-get install libssl-dev	# once-off, you may not need this, but reports suggest you might to build keygen</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New"># Get the desired tarball, current or development - use one of the following:
$ wget http://archive.ntp.org/ntp4/ntp-4.2/ntp-4.2.8p10.tar.gz					# release
$ wget http://archive.ntp.org/ntp4/ntp-dev/ntp-dev-4.2.7p397.tar.gz			# development
 
(May redirect to:  https://www.eecis.udel.edu/~ntp/ntp_spool/ntp4/ntp-4.2/ntp-4.2.8p10.tar.gz)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ tar xvfz ntp-4.2.8p10.tar.gz</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ cd ntp-dev-4.2.8p10</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ ./configure --enable-linuxcaps	# takes 11-15 minutes

# If your PPS doesn&#39;t work and you get a &#34;clock type 22 invalid&#34; message, be sure to install pps-tools
# first, and clear out the directory:  cd ~/ntp, rm -r ntp-dev-4.2.7p397, and start again from step 5.

# It seems that the --enable-linuxcaps flag may not be required on other Linux variants,
# or on the RPi with later versions of Linux with PPS and pps-tools installed.
# It is required for the more recent Raspbian Jessie (later 2015).</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ make					# takes 18-25 minutes
(use &#34;make -j5&#34; for faster execution on the four-core Raspberry Pi 2/3.)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New"># This removes the original NTP and installs the new.
# Step may not be needed - see below.
# Recommend: omit this step.
$ sudo apt-get remove ntp    # get rid of previously existing install of ntpd</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo make install          # puts ntp* in /usr/local/bin/ntp*, takes 30-60 seconds</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">It is not entirely clear to me whether step 9 above is
required.  It does not appear to be when updating from 4.2.7p304 to
4.2.7p321, for example.  I am <i>not</i> using step 9.</SPAN></p>
<p><SPAN size="2"> Once you have a new set of NTP
binaries, you first need to stop NTP, use super-user mode to copy the binaries
to their final directory, and then restart NTP.  Once restarted, a simple
check that it&#39;s working correctly.  I recommend these steps,
although there are alternatives.  See the note below about step 2.</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo /etc/init.d/ntp stop</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo cp /usr/local/bin/ntp* /usr/bin/  &amp;&amp; sudo cp /usr/local/sbin/ntp* /usr/sbin/</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo /etc/init.d/ntp start</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ ntpq -crv -pn		# optional step to check for version and basic function</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2"><i>Note:</i>  on some more recent versions of Raspbian steps 1
and 3 may require:</SPAN>
 </p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo service ntp stop</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo cp /usr/local/bin/ntp* /usr/bin/  &amp;&amp; sudo cp /usr/local/sbin/ntp* /usr/sbin/</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo service ntp start</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2"><i>Note:</i> that on some systems the binary ntp* files will
be written to a mixture of <SPAN face="Courier New">/usr/local/bin</SPAN> and <SPAN face="Courier New">/usr/local/sbin</SPAN>,
according to the paths defined in sntp/loc.  I have been told that the
&#34;sbin&#34; is for system files (i.e. ones not usually run by users such as
servers and daemons, and the &#34;bin&#34; is for files usually executed by
users).  For Debian, for just the ntp* files, this is:</SPAN>
 </p>
<blockquote>
  <pre># Debian installations and man page suffixes
MDOC
ntp-keygen,sbin,8
ntp-wait,sbin,8
ntpd,sbin,8
ntpdate,sbin,8
ntpdc,bin,1
ntpdsim,sbin,8
ntpq,bin,1
ntpsnmpd,sbin,8
ntptime,sbin,8
ntptrace,bin,1</pre>
</blockquote>
<p><SPAN size="2">so you may need to check both directories to get the most
recent files.  Check with &#34;ls -l&#34; which shows the file date.</SPAN>
 </p>
<p><SPAN size="2">A confession: I did alter one system to
point the NTP start-up to the directory I preferred, rather than leaving it
pointing to an old version.  I suspect that in my own personal use, only
the ntpd and ntpq executables matter. </SPAN></p>
    <h3>Updating multiple Raspberry Pi cards
 </h3>
<p><SPAN size="2">If, like me, you have multiple Raspberry Pi cards, you will
not want to waste almost an hour compiling and updating NTP on each card. 
Fortunately, my experience so far using the development versions of NTP
(4.2.7p...) suggests that simply copying the binaries from one Pi to another
works as expected.  This may be luck, or it may be because the OS
differences between Linux/3.2.27+ and Linux/3.6.11+ are not that great.  If
you have access to an FTP server (I used a Windows PC running IIS) you may be
able to use commands such as those below to save a compiled version from one Pi
and load it onto another.  You may need to use  <SPAN face="Courier New">sudo
apt-get install ftp</SPAN>  if FTP is not already available.  Step 5 is required once.  Step
7 is required for each new version you save. 
Replace &#34;368&#34; in the steps below with the version number you have just
compiled.</SPAN>
 </p>
<h4>To save the newly compiled versions:
 </h4>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ ftp &lt;server-address-or IP&gt;</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">(Login as Anonymous or known user)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">bin  (forces binary mode)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">mkdir RaspberryPi  (step only needed once)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">cd RaspberryPi</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">mkdir 397-safe  (step needed once per new version)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">cd 397-safe</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">prompt   (may disable prompting for steps 10 and 13)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">lcd /usr/local/bin</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">mput ntp*</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">(respond Y to the prompt for all the files)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">lcd /usr/local/sbin</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">mput ntp*</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">ls -l  (to check that all eight files are there and have the date you expect)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">quit</SPAN>
 </pre>
  </li>
</ol>
<h4>To load a new version onto another Raspberry Pi:
 </h4>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ cd /usr/local/bin</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo ftp &lt;server-address-or IP&gt;	# sudo allows writing to system directories</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">(Login as Anonymous or known user)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">bin  (forces binary mode)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">cd RaspberryPi/397-safe</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">ls -l  (to check that the files there are correct and have the date you expect)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">mget ntp*</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">(respond Y to the prompt for all the files)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">quit</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo /etc/init.d/ntp stop</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo cp /usr/local/bin/ntp* /usr/sbin/  &amp;&amp;  sudo cp /usr/local/bin/ntp* /usr/bin/</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo /etc/init.d/ntp start</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ ntpq -crv -pn  # to check the NTP version, and that it is still working</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">See the discussion above about the combined commands in
    step 11.  An alternative to steps 1 to 9 might be, if you are brave:</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">sudo wget -P /usr/local/bin -N ftp://&lt;ftp-server-address&gt;/RaspberryPi/397-safe/ntp*
</SPAN> </pre>
  </li>
</ol>
    <h4>Making updating other cards even easier - use a fixed directory name:</h4>
<p><SPAN size="2">After storing the working version on your FTP server, copy it
on the FTP server to a directory with a fixed name such as:</SPAN></p>
<blockquote>
  <pre>/RaspberryPi/ntp/</pre>
</blockquote>
<p><SPAN size="2">You can then write a script for updating other Raspberry Pi
cards something like this:</SPAN></p>
<blockquote>
  <pre>$ nano update-ntp</pre>
</blockquote>
<p><SPAN size="2">with the following contents:</SPAN></p>
<blockquote>
    <pre>sudo wget --no-passive-ftp -P /usr/local/bin -N ftp://&lt;ftp-server&gt;/RaspberryPi/ntp/ntp*
sudo /etc/init.d/ntp stop
sleep 1
sudo cp /usr/local/bin/ntp* /usr/sbin/  &amp;&amp;  sudo cp /usr/local/bin/ntp* /usr/bin/
sleep 1
sudo /etc/init.d/ntp start
sleep 4
ntpq -crv -pn</pre>
</blockquote>
    <p><SPAN size="2">replacing &lt;ftp-server&gt; with the name or IP address
    of your own FTP server.  With a Microsoft FTP server, I found that I
    needed to add --no-passive-ftp  after the wget command, as shown above.  Remember to make the script executable:</SPAN></p>
<blockquote>
    <pre>$ chmod +x update-ntp</pre>
</blockquote>
    <p><SPAN size="2">and run it from your local directory:</SPAN></p>
<blockquote>
    <pre>$ ./update-ntp</pre>
</blockquote>
<p>You can tell another Raspberry Pi to run the update by using the SSH command
thus:</p>
<blockquote>
  <pre>$ ssh pi@the-other-raspi &#34;./update-ntp&#34;</pre>
</blockquote>
<p>You will need to enter the password for the user &#34;pi&#34;, although
this can be avoided (I am told) by using public key based authentication, if that fits with your security model. 
Once you have managed to copy your key	to the second machine (man ssh-copy-id) you need no password either. 
I&#39;m afraid I don&#39;t know how to do that, though.</p>
<h2><a name="ntp-conf">Updating to the final NTP configuration file</a></h2>
<p><SPAN size="2">To get NTP to use the PPS data which is now available to it,
the timestamps of the transitions on the GPIO pin, we need to add another
refclock (server) line to the ntp.conf file.  The server we use is a type
22 server called the ATOM refclock, and we can give it a reference ID of
&#34;PPS&#34;.  I also changed the reference ID of the serial data to
&#34;GPS&#34;.  Note that with a type 22 clock you <i>must</i> have one
other server marked as &#34;prefer&#34;.</SPAN></p>
<blockquote>
  <pre># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help

# Drift file to remember clock rate across restarts
driftfile /var/lib/ntp/ntp.drift

# coarse time ref-clock, not really needed here as we have LAN &amp; WAN servers
server 127.127.28.0  minpoll 4 maxpoll 4
fudge 127.127.28.0 time1 +0.350 refid GPS  stratum 15

# Kernel-mode PPS ref-clock for the precise seconds
server 127.127.22.0 minpoll 4 maxpoll 4
fudge 127.127.22.0  refid PPS

# LAN servers
server 192.168.0.3  minpoll 5 maxpoll 5 iburst prefer
server 192.168.0.2  minpoll 5 maxpoll 5 iburst
server 192.168.0.7  minpoll 5 maxpoll 5 iburst

# WAN servers, &#34;pool&#34; will expand the number of servers to suit
pool uk.pool.ntp.org  minpoll 10  iburst</pre>
</blockquote>
<p><i>Note:</i> when using the ATOM (type 22) refclock, one of the other servers
<i>must</i> be marked as prefer.  This is because the type 22 clock only
supplies the timing <i>within</i> the second, and another server is required to
determine the <i>current</i> second.
 </p>
    <h2>Checking that NTP is seeing the PPS data</h2>
    <p><SPAN size="2">When you have restarted NTP with the new binaries, you
    should see a new line in the output from an ntpq -p command, and the word
    &#34;kern&#34; should be present in the output of an ntpq -c rv command:</SPAN></p>
<blockquote>
    <pre>C:\&gt;ntpq -p raspi
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 SHM(0)          .GPS.           15 l   13   16  377    0.000   33.837   3.510
<b>o</b>PPS(0)          .PPS.            0 l   12   16  377    0.000    0.002   0.002
*pixie           .PPS.            1 u   18   32  377    0.498   -0.030   0.025
+feenix          .PPS.            1 u    5   32  377    0.619   -0.078   0.035
+stamsund        .PPS.            1 u   29   32  377    0.614   -0.017   0.051
 uk.pool.ntp.org .POOL.          16 p    - 1024    0    0.000    0.000   0.002
-ntp.uk.syrahost 192.93.2.20      2 u  405 1024  377   30.031    8.487   0.274
-ntp2.exa-networ 195.66.241.10    2 u  217 1024  377   26.263    3.167   1.277
-resntp-a-vip.lo 182.7.208.171    3 u   49 1024  377   17.854    2.828   1.460
-time.shf.uk.as4 91.208.177.20    3 u   75 1024  377   18.825    0.680   1.974

C:\&gt;ntpq -c rv raspi
associd=0 status=011d leap_none, sync_pps, 1 event, <b>kern</b>,
version=&#34;ntpd 4.2.7p314@1.2483 Mon Oct 29 15:30:42 UTC 2012 (3)&#34;,
processor=&#34;armv6l&#34;, system=&#34;Linux/3.2.27-pps-g965b922-dirty&#34;, leap=00,
stratum=1, precision=-19, rootdelay=0.000, rootdisp=1.180, refid=PPS,
reftime=d439fe8b.16dba50f  Tue, Oct 30 2012  7:21:47.089,
clock=d439fe97.4d7ac44d  Tue, Oct 30 2012  7:21:59.302, peer=63905, tc=4,
mintc=3, offset=0.001547, frequency=-45.081, sys_jitter=0.001907,
clk_jitter=0.000, clk_wander=0.000
</pre>
</blockquote>
<h2><a name="nohz">Enhanced PPS performance</a></h2>
<p>You may find that by adding the directive &#34;nohz=off&#34; to the end of
your /boot/cmdline.txt that jitter is decreased by up to 50%.  Try it and
see for yourself.  Some systems it helps, and others it does not,
depending, I suspect, on which OS and firmware versions you have.  As 
guide, you may get to average jitter in the range:</p>
<blockquote>
  <table>
    <tbody><tr>
      <th><SPAN size="2">Model</SPAN></th>
      <th><SPAN size="2">Averaged jitter</SPAN></th>
    </tr>
    <tr>
      <td><SPAN size="2">Raspberry Pi B </SPAN></td>
      <td><SPAN size="2">3.9 us, but may be better</SPAN></td>
    </tr>
    <tr>
      <td><SPAN size="2">Raspberry Pi B+</SPAN></td>
      <td><SPAN size="2">4.2 s</SPAN></td>
    </tr>
    <tr>
      <td><SPAN size="2">Raspberry Pi 2 B</SPAN></td>
      <td><SPAN size="2">2 s</SPAN></td>
    </tr>
    <tr>
      <td><SPAN size="2">Raspberry Pi 3 B    </SPAN></td>
      <td><SPAN size="2">1 s</SPAN></td>
    </tr>
  </tbody></table>
</blockquote>
<p>Thanks to the folks <a href="https://github.com/raspberrypi/linux/issues/750#issuecomment-239135234">here</a>
for their help.</p>
<h2><a name="autostart">
Getting it all to auto-start</a></h2>
<p><SPAN size="2">With the replaced version of NTP we lose the automatic running
of <SPAN face="Courier New">ntpd</SPAN> at startup which was present in the
original install.  We also need to start <SPAN face="Courier New">gpsd</SPAN>
so that the coarse time is available to NTP.</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New"># To configure gpsd to auto-start, try: 
$ sudo dpkg-reconfigure gpsd</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">This seems to work as expected, and allows the <SPAN face="Courier New">gpsd</SPAN>
to automatically start up.  To check that, I rebooted, and logged in, and
could run <SPAN face="Courier New">cgps -s</SPAN> right away.  However, NTP
won&#39;t see the time from the GPS until <i>after</i>  <SPAN face="Courier New"> cgps -s</SPAN> is run. 
This is fixed as follows (thanks to A Carver):  by default, <SPAN face="Courier New">gpsd</SPAN>
won&#39;t connect to the GPS receiver until there is client software such as <SPAN face="Courier New">cgps</SPAN>
with requires it.  This allows for some power-saving in the GPS
receiver.  To circumvent this, the gpsd needs to be started with the
&#34;-n&#34; option.  These options are set in the directory
/etc/default, so you need to edit the file /etc/default/gpsd to change the
line: GPSD_OPTIONS=&#34;&#34; to GPSD_OPTIONS=&#34;-n&#34;.  Method A
is to do this through dpkg-reconfigure gpsd, method B is to edit the file
directly.</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo nano /etc/default/gpsd</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">NTP will auto-start after a reboot with either Method A or
Method B above.</SPAN></p>
<p><SPAN size="2">There is a cut-out area in the ModMyPi case which will allow
connections to the GPIO connector such as ribbon cable which you can easily
break out and file down to a neat edge.  The sharp-eyed amongst you will
notice that three wires are shown - this is because the surplus header had three
wires connected, but I only used two - ground (blue) and GPIO 24 (red). 
The black lead would be GPIO 23 but it is unused and not connected.</SPAN></p>
<p></p>
<h2>GPS Receiver Issues</h2>
<p><SPAN size="2">There are two issues which are now becoming apparent with the
particular GPS receiver I have and the Raspberry Pi.</SPAN></p>
<ol>
  <li><SPAN size="2">It seems that after a power-down reboot, the GPS receiver
    isn&#39;t correctly detected, and need a disconnect/reconnect of its USB
    connector before it is seen by <SPAN face="Courier New">gpsd</SPAN>. 
    The pulse-per-second signal still comes<SPAN face="Courier New"> </SPAN>through,
    though, so the precise seconds are working, but the coarse seconds are
    not.  This is clearly unacceptable as you would not get the correct
    time unless you also had an Internet NTP server available.</SPAN></li>
  <li><SPAN size="2">During the booting of the GPS receiver (if it does not have
    a battery) it <i>may</i> output GPS time (which is 16 seconds adrift from
    UTC at the time of writing).  Although <SPAN face="Courier New">gpsd </SPAN>is
    supposed to catch this situation, it could mean that the Raspberry Pi will
    not know what the correct time is for some time after booting, and even then
    it could take 10-20 minutes to be sure that the correct time was actually
    being sent by the GPS and for NTP to make the 16 seconds step correction
    required.</SPAN></li>
</ol>
<p><SPAN size="2">If your receiver has this 16-second ambiguity, be sure you have a source
of coarse time available such as an Internet NTP server.</SPAN></p>
<h2><a name="serial">Adding a 3.3 V Serial GPS receiver instead</a></h2>
<p><SPAN size="2">Because of the problem (1) above, I bought a serial GPS
receiver from China.  By following the instructions <a href="http://www.irrational.net/2012/04/19/using-the-raspberry-pis-serial-port/">here</a>
I was able to make the serial port available on the Raspberry Pi independent of
its use as a login or boot-up terminal port. </SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">(make a safety coppy of the file we are about to edit)
$ sudo cp /boot/cmdline.txt /boot/cmdline_backup.txt</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo nano /boot/cmdline.txt
(remove the parameter including the string &#34;ttyAMA0&#34;:
console=ttyAMA0,115200
In Raspbian Wheezy there are two parameters:
 console=ttyAMA0,115200 kgdboc=ttyAMA0,115200)</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo nano /etc/inittab
(Comment out the line like &#34;2:23:respawn:/sbin/getty -L ttyAMA0 115200 vt100&#34;
 by putting a hash (#) at the start of the line.  Note that the line was not 
 2:23 on my version of Linux, so be sure to look for the actual line with ttyAMA0.
 It was the last line of the file, as it happens).</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">(You need to reboot to bring these changes into effect)
$ sudo reboot</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">(Minicom then works - to exit minicom, Ctrl-A, release, Q or X.)
$ minicom -b 9600 -o -D /dev/ttyAMA0</SPAN></pre>
  </li>
  <li>
    <pre><SPAN size="2" face="Courier New">(If minicom gives command not found, you need to install it:)
$ sudo apt-get install minicom</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">Note that the Raspberry Pi requires 3.3 V signals on the serial
pins - <b><i>not</i></b> RS-232 level which will damage the device.  You
should now be able to configure gpsd to talk to ttyAMA0 (using <SPAN face="Courier New">sudo
dpkg-reconfigure gpsd</SPAN>), and the <SPAN face="Courier New">cgps -s</SPAN>
command should work as above.</SPAN></p>
<h3><a name="adafruit"></a><a name="u-blox">3.3 V GPS receiver</a></h3>
<p><SPAN size="2">I found a 3.3 V I/O very compact GPS receiver board <a href="http://cgi.ebay.co.uk/ws/eBayISAPI.dll?ViewItem&amp;item=251164946899">here</a>
described as: &#34;GPS Receiver u-blox NEO-6M Module with Antenna USART TTL &amp; IIC Interface&#34;. 
It includes the patch antenna and cost less than 20.  Such modules seem to be a
standard for flight control for model aircraft and similar applications.  It does require one minor modification, which is to solder a wire
to pin 3 of the NEO-6M module to extract the PPS signal.  This unit
was later replaced by a <a href="http://www.adafruit.com/products/746" target="_blank">Adafruit
    Ultimate GPS Breakout</a> which requires no additional soldering, and has a
built-in antenna.</SPAN></p>

<p><SPAN size="2">One the left is the receiver undergoing initial tests.  You can
see the patch antenna on the left raised on a block of foam so that it can
&#34;see&#34; past the clutter of the receiver, the receiver itself on the blue PCB mounted vertically in the
breadboard, and you might just see the orange-pink wire at the top-right corner
of the receiver which has been carefully soldered to pin 3 of the NEO-6M to get
the PPS signal.  The wire is anchored in the top-right mounting hole for
strain relief.  You don&#39;t need surface-mount tools to make this connection,
but you <i>do</i> need a very fine soldering tip and considerable care! 
Like the TX/RX signals from this module, the PPS signal is at 3.3 V level and
therefore ideal for feeding the Raspberry Pi.  The board will accept either
3.3 V or 5 V power while retaining 3.3V I/O levels, and I&#39;m using 5 V to power the
board to reduce the load on the 3.3 V regulator on the Raspberry Pi board. 
The U-blox device is specified at less than 50 mA supply current.</SPAN></p>
<p><SPAN size="2">For the final installation, the Adafruit module was used as
its single package was more convenient.  Not all the leads are connected as
there is only one place to connect ground on the module (so the blue is left
open), and for simplicity the green Pi Tx =&gt; GPS Rx was not connected. 
Judge the size of the Raspberry Pi by the Ethernet connector on the left!</SPAN></p>
<p><SPAN size="2">The next step was to set up the Raspberry Pi to talk to the new module, use serial
GPS data via <SPAN face="Courier New"> gpsd</SPAN> for the coarse time,
and to use its PPS
line to provide precise time just as described <a href="#pps">above</a>.  To preset the serial line
speed to 9600 at Linux start-up, edit the file <SPAN face="Courier New"> /boot/config.txt</SPAN>
to include the line:</SPAN></p>
<pre>    init_uart_baud=9600</pre>
<p><SPAN size="2">as it says in: <a href="http://elinux.org/RPi_config.txt">http://elinux.org/RPi_config.txt</a>. 
You can check using the command:</SPAN></p>
<blockquote>
  <pre>stty -F /dev/ttyAMA0</pre>
</blockquote>
<p><SPAN size="2">I found that the speed was already set to 9600 on a recent
Raspberry Pi installed from the NOOBS software.  I gather that on a typical Linux system you might use the
command: stty -F /dev/ttyAMA0 9600.</SPAN></p>
<h3><a name="GPIO">Raspberry Pi GPIO multi-pin connector</a></h3>
<p><SPAN size="2">This time , not only were PPS and ground connections required,
but also the serial port and the +5 V line, so I used a piece of 6-way ribbon cable to connect between
the Raspberry Pi and my serial GPS device.  I happened to have a 10-pin
header which fitted the GPIO connector, so these are the connections I chose to
make:</SPAN></p>
<table>
  <tbody><tr>
    <th><SPAN size="2">My</SPAN></th>
    <th><SPAN size="2">Raspberry Pi</SPAN></th>
    <th><SPAN size="2">Comment</SPAN></th>
    <th><SPAN size="2">GPS module</SPAN></th>
  </tr>
  <tr>
    <td><SPAN size="2">Red</SPAN></td>
    <td><SPAN size="2">+5V</SPAN></td>
    <td><SPAN size="2">My unit takes less than 50 mA</SPAN></td>
    <td><SPAN size="2">1 +5V</SPAN></td>
  </tr>
  <tr>
    <td> </td>
    <td><SPAN size="2">+5V</SPAN></td>
    <td> </td>
    <td> </td>
  </tr>
  <tr>
    <td><SPAN size="2">Blue</SPAN></td>
    <td><SPAN size="2">Ground</SPAN></td>
    <td><SPAN size="2">One of two ground connections</SPAN></td>
    <td><SPAN size="2">2 GND</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Green</SPAN></td>
    <td><SPAN size="2">TXD</SPAN></td>
    <td><SPAN size="2">Not connected, see below</SPAN></td>
    <td> </td>
  </tr>
  <tr>
    <td><SPAN size="2">Yellow</SPAN></td>
    <td><SPAN size="2">RXD</SPAN></td>
    <td><SPAN size="2">Receives serial data sent from the GPS</SPAN></td>
    <td><SPAN size="2">3 TXD</SPAN></td>
  </tr>
  <tr>
    <td> </td>
    <td><SPAN size="2">GPIO 18</SPAN></td>
    <td> </td>
    <td> </td>
  </tr>
  <tr>
    <td> </td>
    <td><SPAN size="2"> Ground</SPAN></td>
    <td> </td>
    <td> </td>
  </tr>
  <tr>
    <td> </td>
    <td><SPAN size="2">GPIO 23</SPAN></td>
    <td> </td>
    <td> </td>
  </tr>
  <tr>
    <td><SPAN size="2">White</SPAN></td>
    <td><SPAN size="2">GPIO 24</SPAN></td>
    <td><SPAN size="2">Receives the PPS sent from the GPS</SPAN></td>
    <td><SPAN size="2">PPS pin on u-blox device</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Black</SPAN></td>
    <td><SPAN size="2">Gnd</SPAN></td>
    <td><SPAN size="2">One of two ground connections</SPAN></td>
    <td><SPAN size="2">6 GND</SPAN></td>
  </tr>
</tbody></table>
<p><SPAN size="2">I actually didn&#39;t connect the TXD lead to the GPS module in the first instance,
as the GPS receiver powers up sending data in the correct format, so there is
nothing which the computer <i>needs</i> to control.  There is a description
of the GPIO pin header <a href="http://elinux.org/RPi_Low-level_peripherals#General_Purpose_Input.2FOutput_.28GPIO.29" target="_blank">here</a>. 
Again, please note that 3.3 V signals are required, <i> not</i> the 5 V level, and
definitely <i> not</i> the RS-232 level!</SPAN></p>
<h3>Converting from USB to native serial - software changes</h3>
<p><SPAN size="2">We need to tell gpsd to look for input from a different
source: ttyAMA0 instead of ttyACM0</SPAN></p>
<ol>
  <li>
    <pre><SPAN size="2" face="Courier New">$ sudo nano /etc/default/gpsd</SPAN></pre>
  </li>
</ol>
<p><SPAN size="2">and change the DEVICES= to point to
&#34;/dev/ttyAMA0&#34;.  You could use &#34;<SPAN face="Courier New">sudo
dpkg-reconfigure
gpsd</SPAN>&#34; instead.  You may also find that the delay between PPS and TXD
on the serial line is different from that over USB, so you may wish to edit your
</SPAN><SPAN face="Courier New">ntp.conf</SPAN><SPAN size="2"> accordingly - it makes the output look nicer at least and may make
prediction of the nearest second more accurate during the initial acquisition. 
In my case I needed to change the 0.35 second offset seen in the earlier device
to the 0.13 second offset seen with the U-blox device, hence the following
change in </SPAN><SPAN face="Courier New">ntp.conf</SPAN><SPAN size="2">, from</SPAN></p>
<blockquote>
  <pre>fudge 127.127.28.0 time1 +0.350 refid SHM stratum 15</pre>
</blockquote>
<p><SPAN size="2">to:</SPAN></p>
<blockquote>
  <pre>fudge 127.127.28.0 time1 +0.130 refid SHM stratum 15</pre>
</blockquote>
  <p><SPAN size="2">This configuration has been running since
  Friday, November 09, 2012.  </SPAN>
 </p>
<p><SPAN size="2">On reflection, I am unsure why I chose to make the stratum 15
in the example above,  Possible stratum 2 or 3 might be better, otherwise
NTP may think that the source is of very poor quality and fail to sync to it.</SPAN>
 </p>
<p><SPAN size="2">Robin Scwab notes that you could try using the value of
&#34;Time offset&#34; displayed in cgps -s as the starting value for the time1
parameter.  This sounds to be a gooo idea, except that on one system here I
happened to check Time Offset was about 0.640 seconds, but the best value for
time1 (i.e. the value resulting in the smallest average offset) was 0.130
seconds.  Where did the extra 0.5 seconds come from?</SPAN>
 </p>
<h3>Number of NTP clients which can be handled
 </h3>
<p><SPAN size="2">Someone asked: &#34;How many NTP clients can it
handle?&#34;  Well, I have not done any extensive testing on how many clients this
NTP server can handle (as it will easily be enough for use on a LAN, and it&#39;s
not fit for public use in its present security date).  I did manage to
locate one NTP stress-testing program <a href="http://www.atomic-clock.galleon.eu.com/ntp-server-tool.htm">here</a>,
but the maximum rate I could get it to produce on my PC was about 75 packets per
second, and the Raspberry Pi can handle that rate with ease.  With NTP
client PCs polling at 64 second intervals (the fastest normally used), that&#39;s
4800 clients, so easily enough for a small organisation.  If you were using
this for a small organisation, I would recommend using two or three stratum-1
servers so that you are covered in the event of failure - perhaps mark one of the
servers &#34;prefer&#34; on the client PCs to avoid NTP from
&#34;clock-hopping&#34;.  If you have better
stress-test data, as Kasper Pedersen did, please let me know and I can publish it here.</SPAN>
 </p>
<p><SPAN size="2"><i>Kasper Pedersen notes:</i></SPAN>
 </p>
<p><SPAN size="2">I did one a few years ago that goes a bit faster (Linux):</SPAN>
 </p>
<p><SPAN size="2"><a href="http://n1.taur.dk/permanent/ntpload.c" target="_blank">http://n1.taur.dk/permanent/ntpload.c</a></SPAN>
 </p>
<p><SPAN size="2">To test a Raspberry Pi you need 4 instances running, at which point the
Pi runs out of CPU, and settles on 3520/s.  That ought to be enough for most small homes.. :-)</SPAN>
 </p>
<h2><a name="user-mode">An alternative user-mode approach - no kernel changes
required</a>
 </h2>
<p><SPAN size="2">These are brief notes only, on an alternative approach which
does not require a modified kernel with PPS support in the OS, nor does it
require a recompiled NTP, but which produces
slightly less accurate timekeeping.  It may, however, be quite good enough
for most purposes.  The program was developed by Folkert van Heusden, see: <a href="http://vanheusden.com/time/rpi_gpio_ntp/">http://vanheusden.com/time/rpi_gpio_ntp/</a> 
and <a href="http://www.febo.com/pipermail/time-nuts/2013-June/077484.html">announced</a>
in the Time-Nuts mailing list.</SPAN>
 </p>
<h4>Base installation - check you meet these requirements
 </h4>
<p><SPAN size="2">You can start here assuming that you have your Raspberry Pi
basically working with GPSD and NTP, meaning that:</SPAN>
 </p>
<ul>
  <li><SPAN size="2">You have installed the current operating system and any
    updates.</SPAN></li>
  <li><SPAN size="2">You have made any optional IP address or computer name
    changes.</SPAN></li>
  <li><SPAN size="2">You have got NTP working, preferably connected to your
    country&#39;s pool servers.</SPAN></li>
  <li><SPAN size="2">You have <a href="#compile-ntp" target="_blank">recompiled
    and installed NTP</a> to get a full version (is a re-compile required?).</SPAN></li>
  <li><SPAN size="2">You have the PPS signal from the GPS receiver connected to
    the GPIO, e.g. GPIO-8 (pin 24), or GPIO 18 for the <a href="#no-soldering">no-soldering
    board</a>.</SPAN></li>
  <li><SPAN size="2">You have <a href="#trimble" target="_blank"> installed and configured the <i>gpsd</i> software
    and utilities</a>, and made <i>gpsd</i> auto-start.</SPAN></li>
  <li><SPAN size="2">You have a GPS device connected either via a serial or a
    USB connection.</SPAN></li>
  <li><SPAN size="2">The <i>cgps -s</i> command produces a correct display.</SPAN></li>
  <li><SPAN size="2">You have configured NTP to talk to the type 28.0 shared
    memory driver, and can see the GPSD output in <i>ntpq -pn</i>.</SPAN></li>
</ul>
<h4>Optional first step - to determine the offset of your GPS serial data from
the exact second
 </h4>
<p><SPAN size="2">The objective is to determine the offset between the PPS
signal from your GPS and the serial data which typically follows some hundred or
more milliseconds later.  NTP can use that offset either to make a better
sync when using just a GPS receiver with no PPS, or to present a slightly less
confusing <i>ntpq -p</i> output when using PPS.  To measure what offset
should be specified for the time1 factor when using a GPS receiver (type 28
reference clock driver), we can use NTP as a measuring tool by making it sync to
existing servers and have it monitor the
shared memory #0 written by <i> gpsd.  </i>I.e. NTP will not use server
28.0 for timekeeping, but it <i>will</i> display the offset in the <i>ntpq
-p </i> command.  To do this, you add these
lines to your ntp.conf (if they are not already there):</SPAN>
 </p>
<blockquote>
  <pre># Server to be monitored only, not selected for syncing
server 127.127.28.0 minpoll 4 maxpoll 4 noselect
fudge 127.127.28.0 time1 0.000 refid GPSD</pre>
</blockquote>
<p><SPAN size="2">Watch the results from  </SPAN><SPAN face="Courier New">ntpq
-pn</SPAN><SPAN size="2">, and look at the offset for the server with the refid GPSD.  On one of my RPi cards it varied between -322 and -336 (units are
ms in the ntpq report). 
Take an average of those values, and replace the 0.000 after time1 with that average.  In this case, the average was -329 ms, so try:</SPAN>
 </p>
<blockquote>
  <pre>server 127.127.28.0 minpoll 4 maxpoll 4 noselect
fudge 127.127.28.0 time1 0.329 refid GPSD</pre>
</blockquote>
<p><SPAN size="2">i.e. if the reported offset is negative, you need to make
time1 a positive value.  Now you should see much smaller offsets for the GPSD server. 
It&#39;s not
essential to do this, but it gives you confidence that things are working as
expected, it may help NTP in the early startup, and it produces a less
confusing <i> ntpq</i> display.  Don&#39;t forget to remove the <i>noselect</i>
when you have the best value for the offset, and set the flags to include <i>preferred
</i>so that NTP knows it can use that source as a seconds provider!  There
must be at least one <i>prefer</i> for PPS to work.  This now has the NTP daemon seeing the GPSD device, and
the next step is to
add in some PPS support.</SPAN>
 </p>
<blockquote>
  <pre>server 127.127.28.0 minpoll 4 maxpoll 4 prefer
fudge 127.127.28.0 time1 0.329 refid GPSD
 </pre>
</blockquote>
<h4><a name="average">Getting the average value for time1 automatically</a>
 </h4>
<p><i>Angelo Mileto writes:</i> What I did to get a good sample of data for the <i> time1</i> value was let the ntpq -p command run for a period of time and capture the results. 
Then, using awk, calculate the average/mean of the values collected.  If you look at the static ntpq -p output, you will see something like this:
 </p>
<pre>     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 SHM(0)          .GPSD.           2 l   13   16  377    0.000   33.837   3.510
oPPS(0)          .PPS.            2 l   12   32  377    0.000    0.002   0.002</pre>
<p>along with other pool/server entries.  As noted above, you need to ensure you have a good preferred source for the comparison to work. 
To capture your data, execute the following at the command line from your user&#39;s home directory:
 </p>
<blockquote>
<pre>watch -n0.5 &#34;ntpq -pn | grep &#39;.GPSD. 2 1 1&#39; | tee --append GPSD_Offset.txt&#34;</pre>
</blockquote>
<p>Pay attention to the single and double quotes in that command.  If you notice, the grep is looking for the exact line from the ntpq -p output that contains your GPSD
refid.  So if you named it something different, that would replace the .GPSD. in the command. 
Also, notice the &#34;2 1 1&#34;, that is the stratum, t and when fields.  The stratum is whatever you set it to in the ntp.conf; the t should always be a 1 and the when is going to capture the output
<i>only</i> when the timer reaches 1 meaning that it just updated the value. 
There is no sense in capturing data every .5 seconds for data that doesn&#39;t
change!  So the easiest way to get the value to grep for is to manually run the ntpq -p and copy from your refid through the when fields and that&#39;s what will be grepped for.
 </p>
<p>Let that run for a good while - probably not really needed - but that&#39;s the point of getting a good sample size. 
This will capture the output from the ntpq command filtering to just your SHM/GPSD. 
This will all be saved to a file GPSD_Offset.txt in the current directory - that&#39;s what the &#34;tee --append GPSD_Offset.txt&#34; does. 
You can tail -f that file if you want while the watch command is running to see what is going into that file. 
NOTE: If you need to restart the process because you changed some setting and want to start over, delete any existing GPSD_Offset.txt file first.
 </p>
<p>Once you have a sufficient sample size, you can then use the following command to get the actual value for the average/mean:
 </p>
<blockquote>
<pre>awk &#39;{total=total+$9; count=count+1} END {print &#34;Total:&#34;total; print &#34;Count:&#34;count; print &#34; Avg:&#34;total/count}&#39; GPSD_Offset.txt</pre>
</blockquote>
<p>This steps through every line of the GPSD_Offset.txt file and totals up all of the offset values. 
It will also count how many lines/values are there.  Finally it just prints the information: Total, count and average. 
The average value is what you would put into the <i> time1</i> value as noted above. 
Don&#39;t forget to change the noselect to prefer when you are in there to add the time1 value.</p>
<p>DJT: This is somewhat beyond my Linux, so my thanks to Angelo for the scripts and the
detailed explanation.  Angelo can be contacted <a href="mailto:angelomileto@comcast.net">here</a>.<br/>
 </p>
<h4>Downloading and compiling Folkert van Heusden&#39;s program
 </h4>
<p><SPAN size="2">As of 2013-Jun-18, rpi_gpio_ntp-0.3 was the current version,
but thanks to Thomas Erthner I know that version 1.5 is now available, so I&#39;ve
updated the lines below.  However, I&#39;ve only tested version 0.3, not
version 1.5.  For more details, please see Folkert&#39;s
page.</SPAN>
 </p>
<blockquote>
  <pre>$ wget http:<wbr/>
//vanheusden.<wbr/>
com/time/rpi_gpio_ntp/rpi_gpio<wbr/>
_ntp-1.5.<wbr/>
tgz<wbr/>

$ tar xvfz rpi_gpio_ntp-1.5.tgz
$ cd rpi_gpio_ntp-1.5
$ sudo make install</pre>
</blockquote>
<p><SPAN size="2">This places the resulting binary in </SPAN><SPAN face="Courier New">/usr/local/bin/. 
</SPAN>You don&#39;t need this for current Raspbian versions.
 </p>
<h4>To test you are receiving a PPS signal, from GPIO pin 8:
 </h4>
<blockquote>
  <pre>$ sudo rpi_gpio_ntp -g 8 -d
rpi_gpio_ntp v0.2, (C) 2013 by folkert@vanheusden.com

NTP unit: 0
GPIO pin: 8
Fudge : 0.000000
&#34;Fork into the background&#34; disabled because of debug mode.
1371475664.752146325] poll() GPIO 8 interrupt occurred
1371475665.000148935] poll() GPIO 8 interrupt occurred
1371475666.000147203] poll() GPIO 8 interrupt occurred
1371475667.000160470] poll() GPIO 8 interrupt occurred
1371475668.000159739] poll() GPIO 8 interrupt occurred
(Ctrl-C pressed)
$</pre>
</blockquote>
<h4>Running the program
 </h4>
<p><SPAN size="2">To make NTP read the PPS timings on the GPSD shared memory, we need to use
this command to start the program, assuming the PPS signal is being sent to
GPIO-8 (physical pin 24 - see <a href="http://elinux.org/RPi_Low-level_peripherals#General_Purpose_Input.2FOutput_.28GPIO.29">here</a>). 
Use pin 18 is you are using the <a href="#no-soldering">no-soldering board</a>.</SPAN>
 </p>
<blockquote>
  <pre>$ sudo rpi_gpio_ntp -N 1 -g 8</pre>
</blockquote>
<p><SPAN size="2">and to edit the ntp.conf file, to include the shared memory driver, on section 1
of the GPSD shared memory, replacing the earlier type 22 driver.  Add:</SPAN>
 </p>
<blockquote>
  <pre>server 127.127.28.1 minpoll 4 prefer
fudge 127.127.28.1 refid UPPS</pre>
</blockquote>
<p><SPAN size="2">I suggest &#34;UPPS&#34; to show it&#39;s user-mode PPS (and not
the more accurate Kernel mode).  If you want to see the output pulse from the program, you can add the &#34;-p 7&#34;
parameter:</SPAN>
 </p>
<blockquote>
  <pre>sudo rpi_gpio_ntp -N 1 -g 8 -p 7</pre>
</blockquote>
<p><SPAN size="2">and you can then use an oscilloscope to compare the time of
the PPS rising edge with the toggling line from GPIO pin 7 (connector pin
26).  On my system, there was a variable delay of between 270 and 390
microseconds between PPS and program response.</SPAN>
 </p>
<h4>Auto-start
 </h4>
<p><SPAN size="2">More information to follow, for now, this from Folkert: 
edit  <SPAN face="Courier New">/etc/rc.local</SPAN>  and add the
following (BEFORE the exit 0 statement and AFTER the #!/bin/sh line):</SPAN>
 </p>
<blockquote>
  <pre>/usr/local/bin/rpi_gpio_ntp -N 1 -g 8</pre>
</blockquote>
<p><SPAN size="2">Replace &#39;8&#39; by the gpio pin you are using, e.g. 18 for the <a href="#no-soldering">no-soldering
board</a>.</SPAN>
 </p>
<h3>Performance
 </h3>
<p><SPAN size="2">The original documentation suggested using minpoll=1, however on testing using 
<SPAN face="Courier New">ntpq -pn</SPAN>  it appeared that NTP will automatically accept 3 as the minimum value.  As
the previous testing with kernel-mode PPS had been with minpoll=4, it seemed
only fair to test with that value.  You can see the change from minpoll=1
to minpoll=4 just after 18:00 UTC.  Recall that 1 was replaced internally
by 3, the lowest value NTP will accept.  With minpoll=4,
both the reported offset and the averaged jitter were reduced.  First, what it looks like in MRTG for comparison with the results
above:</SPAN>
 </p>
<p>
 </p>
<p><SPAN size="2">Don&#39;t get confused: the MRTG plot above covers rather more
than one day, whereas the plots below cover just under half a day.  On the
plot MRTG plot, you can see that the RPi was changed from kernel-mode PPS at
13:00 UTC and run for a short while with Internet servers alone, visible as the
larger excursions on the plot above around 13:00).  </SPAN>
 </p>
<p><SPAN size="2">From my <a href="https://www.satsignal.eu/software/net.htm#NTPplotter" target="_blank">NTPplotter
program</a> graphs below, you can see more clearly that at 13:30
the user-mode PPS was started, and the just after 18:00 the minpoll was changed
from 1 (actually 3) to 4, resulting in a slight drop of RMS offset from around 5
microseconds to 4 microseconds:</SPAN>
 </p>
<p>
 </p>
<p><SPAN size="2">The jitter graph shows a drop in averaged jitter (green line) from just below 7 to just
above 5 microseconds after 18:00, when the minpoll was changed from 3 to 4. 
The initial higher value of averaged jitter is the tail resulting from
Internet-only sync.</SPAN>
 </p>
<p>
 </p>
<p><SPAN size="2"><a name="5S">On another Raspberry Pi</a> you can see the dramatic difference between NTP sync
over the network and that achieved with the user-mode PPS software.  In
this case, network sync was to a local NTP stratum-1 server over a Wi-Fi
connection, and the PPS sync was with a U-blox 5S module.  User-mode PPS
was started just after 11:00 UTC in the middle of the graphs below.  RMS
offset dropped from a rather variable 48-80 microseconds to around 2 microseconds,
and jitter averaged over 6 hours dropped from around 80 microseconds to under
2.5 microseconds.</SPAN>
 </p>
<p>
 </p>
<p><SPAN size="2">To monitor NTP you can edit the ntp.conf file to turn on the generation
of more detailed statistics data.  Note that this data maybe a megabyte or more per day, so think about keeping only a few days
worth, and only enabling statistics collection when needed as the number of
writes to the SD card flash memory is limited.  The detailed information
can now be found <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#ntp">here</a> and I
offer a program to plot the statistics data and produce offset and jitter graphs
such as those above <a href="https://www.satsignal.eu/software/net.htm#NTPplotter" target="_blank">here</a>.</SPAN>
</p>
<h2>Transients
 </h2>
<p><SPAN size="2">I may have been unfortunate in locating one Raspberry Pi with an Adafruit
module in a position where it was not getting as good a view of the sky as
others, or it may be that other devices connected to that RPi or software
installed is an issue.  I have been seeing transient poor time-keeping
(loss of GPS PPS signal) at intermittent times.  I did discover that
disconnections a lead to a USB hub connected to that RPi seemed to improve
things.</SPAN>
 </p>
<p> <SPAN size="2">If the problem was caused purely by unfortunate GPS satellite
position, I might have expected it to repeat in a near 24-hour pattern, which
does appear to be the case.  Thanks to John Ryan for correcting me about the GPS
orbital period, I was originally thinking sidereal time and 4 minute earlier
each day.  However, on the Time-Nuts mailing list, in connection with
another topic, Bob Camp <a href="https://www.febo.com/pipermail/time-nuts/2014-October/087270.html">commented</a>:</SPAN>
 </p>
<blockquote>
  <p><SPAN size="2">&#34;The GPS constellation repeats roughly once a day. 
  It is not at all uncommon to have a worst case satellite geometry for a given antenna location. 
  If you have one, it will repeat once a day and show up as a bump in the timing out of your GPS module....&#34;</SPAN>
 </p>
</blockquote>
<p><SPAN size="2">Although the GPS satellites repeat position in just less than
12 hours, there is also the rotation of the earth to take into account, so the
position repeats at your location every 24 hours (well, ~23 h 56 m).</SPAN>
 </p>
<p> <SPAN size="2"> On reflection, 2014 February 09, this is the only Raspberry
Pi with certain radio software installed, and the problem only seems to occur
some days after a reboot, so I think I will stop recording the problem
here.  I am editing out the transients as they affect the P/N timekeeping
graphs, but I leave the raw-graphs as-is.</SPAN>
 </p>
<table>
  <tbody><tr>
    <th><SPAN size="2">Date</SPAN></th>
    <th><SPAN size="2">Transient</SPAN></th>
  </tr>
  <tr>
    <td><SPAN size="2">Dec 31</SPAN></td>
    <td><SPAN size="2">20:00-20:45, 23:10-24:00</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-01</SPAN></td>
    <td><SPAN size="2">20:14-20:37, 23:17-23:28</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-02</SPAN></td>
    <td><SPAN size="2">01:29 02:45, 19:57..20:13, 20:14..20:29, 20:31, 23:13, 23:24 23:30..23:54</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-03</SPAN></td>
    <td><SPAN size="2">02:39, 03:44, 20:12..20:22, 23:24..23:52</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-04</SPAN></td>
    <td><SPAN size="2">02:37, 20:03..20:28, 23:06, 23:41..23:45</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-05</SPAN></td>
    <td><SPAN size="2">02:31</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2"> </SPAN></td>
    <td><SPAN size="2">Removed USB extension cable from RasPi-2</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-06</SPAN></td>
    <td><SPAN size="2">19:33..1940, 19:54, 22:49..22:55</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-10</SPAN></td>
    <td><SPAN size="2">20:30 22:07-22:17 22:37-22:45 23:03</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-11</SPAN></td>
    <td><SPAN size="2">21:57-23:18</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-12</SPAN></td>
    <td><SPAN size="2">22:01-22:54</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-13</SPAN></td>
    <td><SPAN size="2">19:17-19:37 22:37-22:54</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-14</SPAN></td>
    <td><SPAN size="2">01:46-01:47 19:30 19:40 22:46-22:51</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan 15</SPAN></td>
    <td><SPAN size="2">01:47 19:16-19:35 22:42-22:46 23:06</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-16</SPAN></td>
    <td><SPAN size="2">19:05-19:32 22:24-22:41</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-17</SPAN></td>
    <td><SPAN size="2">19:07-19:19 22:04-22:34</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-18</SPAN></td>
    <td><SPAN size="2">19:11-19:16 21:55-21:57</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-19</SPAN></td>
    <td><SPAN size="2">02:38-02:39</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-20</SPAN></td>
    <td><SPAN size="2">19:04-20:06</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-21</SPAN></td>
    <td><SPAN size="2">02:55-03:03 03:17 19:00 19:46 20:15 22:06</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-22</SPAN></td>
    <td><SPAN size="2">02:56-03:05</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-23</SPAN></td>
    <td><SPAN size="2">10:46 12:48</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-24</SPAN></td>
    <td><SPAN size="2">02:48-02:53 07:21 07:44 12:46-12:48 17:11 19:20-19:36</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-25</SPAN></td>
    <td><SPAN size="2">07:13 07:18 07:52-07:53 13:02-13:11</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-26</SPAN></td>
    <td><SPAN size="2">02:50 03:26 18:06 18:31-18:36 21:59</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-27</SPAN></td>
    <td><SPAN size="2">02:42 02:57-03:37 07:09 (reboot at 17:23)</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Jan-31</SPAN></td>
    <td><SPAN size="2">21:19-21:21</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Feb-05</SPAN></td>
    <td><SPAN size="2">20:59 23:12 (is this something which starts days after boot?)</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Feb-07</SPAN></td>
    <td><SPAN size="2">	18:03 18:41 20:52</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Feb-08</SPAN></td>
    <td><SPAN size="2">15:09 15:42 18:37 20:33 20:46 21:24</SPAN></td>
  </tr>
  <tr>
    <td><SPAN size="2">Feb-09</SPAN></td>
    <td><SPAN size="2">00:13 00:37-00:43 01:37</SPAN></td>
  </tr>
</tbody></table>
<h2><a name="leap-seconds">Leap-seconds</a>
 </h2>
<p><SPAN size="2">If you are operating completely alone, with no internet
connection and just the GPS, your NTP may need to be told about leap-seconds
which change the offset between GPS time and wall-clock time every so
often.  Some GPS receivers provide this information automatically, but this
can also be done by providing a file with the times of changes (in a standard format) and telling ntpd where to
find that file.  On my own
systems I have set up a Samba share on the NTP servers so that I can update the
leap-seconds file from a central location.  To do this, add a new writeable share named &#34;ntp-leapseconds&#34;
by adding lines to the end of smb.conf, and then restart Samba:</SPAN>
 </p>
<blockquote>
<pre>sudo nano /etc/samba/smb.conf</pre>
<pre>[ntp-leapseconds]
comment = NTP leapsecond.file
path = /home/pi/ntp
writeable = yes
guest ok = yes

sudo /etc/init.d/samba restart</pre>
</blockquote>
<p><SPAN size="2">I chose to put my leap-seconds file in the default user&#39;s
&#34;ntp&#34; home directory, but you may prefer some where more secure! 
To tell ntpd where to find the file,	add one line to the end of ntp.conf with
nano, and restart NTP:</SPAN>
 </p>
<blockquote>
  <pre>sudo nano /etc/ntp.conf</pre>
  <pre>leapfile /home/pi/ntp/leap-seconds.file</pre>
  <pre>sudo /etc/init.d/ntp restart</pre>
</blockquote>
<p><SPAN size="2">You can get the leap-seconds file from a variety of locations,
including:</SPAN>
 </p>
<p><SPAN size="2">    <a href="ftp://utcnist.colorado.edu/pub/">ftp://utcnist.colorado.edu/pub/<br/>
</a>    <a href="ftp://tycho.usno.navy.mil/pub/ntp/">ftp://tycho.usno.navy.mil/pub/ntp/</a></SPAN>
 </p>
<p><SPAN size="2">and it will be named  <SPAN face="Courier New">leap-seconds.3582403200</SPAN> 
with a different serial number as the information is updated (this file was found in December 2013).  I
copy the file to a constant name:  <SPAN face="Courier New">leap-seconds.file</SPAN> 
so that I don&#39;t need to alter NTP each time.  I then have a small Windows
command file to update all my systems - Windows, FreeBSD and Raspberry Pi
cards.  Here&#39;s an extract:</SPAN>
 </p>
<blockquote>
  <pre>XCOPY /D /Y  leap-seconds.file  \\Alta\ntp\etc\
XCOPY /D /Y  leap-seconds.file  \\Stamsund\ntp\etc\
XCOPY /D /Y  leap-seconds.file  \\Pixie-II\ntp-leapseconds\
XCOPY /D /Y  leap-seconds.file  \\RasPi-1\ntp-leapseconds\
XCOPY /D /Y  leap-seconds.file  \\RasPi-2\ntp-leapseconds\
PAUSE</pre>
</blockquote>
<p><SPAN size="2">You can check whether NTP is using the file, and whether your
file is stale, with the:  <SPAN face="Courier New">ntpq -crv </SPAN>
command:</SPAN>
 </p>
<blockquote>
  <pre>associd=0 status=01fd leap_none, sync_pps, 15 events, kern,
version=&#34;ntpd 4.2.7p408@1.2483 Sun Dec 29 14:36:43 UTC 2013 (1)&#34;,
processor=&#34;armv6l&#34;, system=&#34;Linux/3.6.11&#34;, leap=00, stratum=1,
precision=-19, rootdelay=0.000, rootdisp=1.180, refid=PPS,
reftime=d66b96d3.01b61b26  Mon, Dec 30 2013  6:53:07.006,
clock=d66b96df.3df17b24  Mon, Dec 30 2013  6:53:19.241, peer=61352, tc=4,
mintc=3, offset=0.000281, frequency=-34.180, sys_jitter=0.001907,
clk_jitter=0.001, clk_wander=0.003, tai=35, leapsec=201207010000,
expire=201406010000</pre>
</blockquote>
<p><SPAN size="2">Look at the two final lines.  leapsec shows the latest
leap second (here is was on 2012 Jul 01), and when the leap-second file expires
(here: 2014-Jun-01).  There will also be a warning if the file is stale.</SPAN>
 </p>
<p><SPAN size="2">Update the file from time-to-time - say every six months - in
May and November as leap-second changes usually happen at the end of June or
December.  There is more information <a href="http://support.ntp.org/bin/view/Support/ConfiguringNTP#Section_6.14.">here</a>.</SPAN>
 </p>
<h2><a name="DHCP">DHCP</a> Causing <a name="router">problems</a>
 </h2>
<p>If you keep getting a different ntp.conf from that which you edited, your
router may be the cause. It seems some routers give out NTP information in DHCP which the Pi by default uses over
/etc/ntp.conf.
 </p>
<p>The fix I think is:
 </p>
<blockquote>
<pre>rm /etc/dhcp/dhclient-exit-hooks.d/ntp</pre>
</blockquote>
<p>and if you have the following file, remove it as well:
 </p>
<blockquote>
<pre>rm /var/lib/ntp/ntp.conf.dhcp</pre>
</blockquote>
<p>Then reboot.<br/>
 </p>
<h2><a name="monitoring">Monitoring</a> NTP remotely
 </h2>
<p><SPAN size="2">You can produce real-time graphs like those below using MRTG
and a simple script which can get the statistics from NTP remotely.  There
is more information <a href="https://www.satsignal.eu/ntp/NTPandMRTG.html">here</a>.  I happen to run
MRTG and the collection scripts on a Windows PC.</SPAN>
 </p>
<blockquote>
  <p><SPAN size="1">RasPi-2</SPAN></p>
  <p><SPAN size="1">RasPi-4</SPAN></p>
</blockquote>
<h2><a name="snmp">Adding SNMP support for general remote monitoring</a>
 </h2>
<p><SPAN size="2">Information on general SNMP monitoring may now be found <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#snmp">here</a>. 
This allows monitoring of the network I/O on the card, and a number of other
parameters which are exposed for measurement by the OS.  An example graph
follows:</SPAN>
 </p>
<blockquote>
  <p><SPAN size="1">RasPi-3</SPAN></p>
</blockquote>
<h2><a name="cpu-temp">Adding SNMP support for CPU temperature monitoring</a>
 </h2>
<p><SPAN size="2">As NTP tries to keep the clock on the card at a constant
frequency, it is often compensating for the effects of temperature changes which
cause frequency changes.  It is therefore worthwhile monitoring the CPU temperature
monitoring as described <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#cpu-temp">here</a>,
although with a PPS signal and when used as a stratum-1 server, temperature
effects may be less obvious.  Example graphs are shown below:</SPAN>
 </p>
<blockquote>
<p><SPAN size="1">RasPi-4</SPAN></p>
<p><a href="https://www.satsignal.eu/mrtg/performance_raspi-4.php"></a>
 </p>
</blockquote>
<p><SPAN size="2">The MRTG monitoring scripts (which I actually run on a Windows PC
covering several Raspberry Pi cards) are in <a href="https://www.satsignal.eu/ntp/MRTG-scripts.zip">this Zip
archive</a>.</SPAN>
 </p>
<h2><a name="ambient">Monitoring Ambient Temperature</a>
 </h2>
<p><SPAN size="2">I extended the SNMP pass function to allow monitoring of
ambient temperature using the DS18B20 &#34;single-wire&#34; device.  This
is well written up for use with the Raspberry Pi <a href="http://webshed.org/wiki/RaspberryPI_DS1820">here</a>,
and the detailed description is now <a href="https://www.satsignal.eu/raspberry-pi/monitoring.html#ambient">here</a>. 
Sample results:</SPAN>
 </p>
<blockquote>
  <table>
    <tbody><tr>
      <td><SPAN size="1">RasPi-4</SPAN></td>
      <td><SPAN size="2">Indoor temperature °C</SPAN></td>
    </tr>
    <tr>
      <td><a href="https://www.satsignal.eu/mrtg/raspi4-ds2-temp.html"></a></td>
      <td><SPAN size="2">Outdoor temperature °F</SPAN></td>
    </tr>
  </tbody></table>
</blockquote>
<h4>Thanks!
 </h4>
<p><SPAN size="2">My thanks to <a href="http://webshed.org">Webshed</a> for
uploading the information about using the DS18B20.</SPAN>
 </p>
<h2>Running a publicly accessible NTP server
 </h2>
<p><SPAN size="2">If you are running a server which is accessible from the
public Internet - perhaps you are contributing to the <a href="http://www.pool.ntp.org/en/">NTP
Pool</a> project - there are some simple precautions you should take to ensure
that your server is not used as the source of an attack on other PCs.  Note
that this doesn&#39;t apply to most end-user clients sitting on your local PC, you
would need to have specially opened a port in your firewall or router to allow
public incoming unsolicited UDP port 123 packets into your local network. 
If you are using a development version (4.2.7p26 or later) you are already
protected.  The following notice explains more:</SPAN>
 </p>
<div><SPAN size="2"><i>NTP users are strongly urged to take immediate action to ensure that their NTP daemon is not susceptible to use in a reflected denial-of-service
      (DRDoS) attack. Please see the
      <a href="http://support.ntp.org/bin/view/Main/SecurityNotice#DRDoS_Amplification_Attack_using" target="_blank"> NTP Security Notice</a> for vulnerability and mitigation details, and the
      <a href="http://networktimefoundation.org/ntp-winter-2013-network-drdos-attacks/" target="_blank">Network
      Time Foundation Blog</a> for more information. (January 2014)</i></SPAN></div>
<blockquote>

</blockquote>
<h2><a name="extras">Extras</a>
 </h2>
<ul>
  <li><SPAN size="2"><a href="https://www.satsignal.eu/raspberry-pi/RaspberryPi-extra.html">When GPSD loses connection
    to the GPS device...</a></SPAN></li>
  <li><SPAN size="2"><a href="https://www.satsignal.eu/ntp/RaspberryPi-notes.html#EthernetLatency">Reducing
    the Ethernet latency...</a></SPAN></li>
  <li><SPAN size="2"><a href="https://www.satsignal.eu/ntp/RaspberryPi-notes.html#JorgeAmaralThoughts">Some thoughts
    on NTP from Jorge Amaral Portugal...</a></SPAN></li>
  <li><SPAN size="2"><a href="https://www.satsignal.eu/raspberry-pi/kernel-recompile.html">Recompiling
    the kernel...</a></SPAN></li>
  <li><SPAN size="2"><a href="https://www.satsignal.eu/ntp/RaspberryPi-notes.html#KernelChanges">NTP bug
    report 2314...
    </a></SPAN></li>
  <li><SPAN size="2"><a href="http://aardvarklabs.wordpress.com/2013/12/29/ntp-server-using-raspberry-pi-and-vp-oncore-gps-module-part-1/" target="_blank">NTP Server using Raspberry Pi and VP Oncore GPS Module</a>
    (from: Gavin Andrews)<a href="https://www.satsignal.eu/ntp/RaspberryPi-notes.html#KernelChanges"><br/>
    </a> </SPAN></li>
</ul>
<h2>
<a name="GPS-devices">GPS Devices used or considered - in alphabetical order!</a>
 </h2>
<ul>
  <li><SPAN size="2"><a href="https://buy.garmin.com/shop/shop.do?pID=27594&amp;pvID=14555" target="_blank">Garmin
    GPS 18x LVC</a> - used on <a href="https://www.satsignal.eu/ntp/FreeBSD-GPS-PPS.htm" target="_blank">previous projects</a></SPAN></li>
  <li><SPAN size="2"><a href="http://www.mediatek.com/en/News/news_content.php?sn=78" target="_blank">MTK3339
    chipset</a> - source <a href="http://www.adafruit.com/products/746" target="_blank">Adafruit
    Ultimate GPS Breakout (US)</a>, <a href="http://proto-pic.co.uk/ultimate-gps-breakout-66-channel-w-10-hz-updates-mtk3339-chipset/" target="_blank">UK
    supplier</a></SPAN></li>
  <li><SPAN size="2"><a href="http://www.sureelectronics.net/goods.php?id=99" target="_blank">Sure
    Electronics GPS evaluation board</a> - used on <a href="https://www.satsignal.eu/ntp/Sure-GPS.htm" target="_blank">previous
    projects</a></SPAN></li>
  <li><SPAN size="2"><a href="http://www.trimble.com/timing/resolution-smt.aspx" target="_blank">Trimble
    Resolution SMT</a> - source eBay
    (no longer available)</SPAN></li>
  <li><SPAN size="2">U-blox module and board with Raspberry Pi header - <a href="http://ava.upuaut.net/store/index.php?route=product/product&amp;path=59_60&amp;product_id=95" target="_blank">source
    HAB Supplies</a>, <a target="_blank" href="http://ava.upuaut.net/?p=600">blog
    and photos</a>
     </SPAN></li>
  <li><SPAN size="2"><a href="http://www.u-blox.com/de/lea-6t.html" target="_blank">U-blox
    LEA-6T</a> - source <a href="http://www.synergy-gps.com" target="_blank">Synergy
    (US)</a></SPAN></li>
  <li><SPAN size="2"><a href="http://www.u-blox.com/en/download/documents-a-resources/u-blox-6-gps-modules-resources.html" target="_blank">U-blox
    NEO 6M</a> - source eBay (similar items available)</SPAN></li>
</ul>
  <h2><a name="dimensions">Dimensions</a>
 </h2>
<ul>
  <li><SPAN size="2">Raspberry Pi: 86 x 56 x 21 mm</SPAN></li>
  <li><SPAN size="2">Raspberry Pi in its box: 91 x 62 x 29 mm</SPAN></li>
  <li><SPAN size="2">Trimble evaluation board: 66 x 32 x 8 mm</SPAN></li>
  <li><SPAN size="2">Trimble evaluation board with interface: 66 x 32 x 24 mm</SPAN></li>
  <li><SPAN size="2">U-blox interface card: 39 x 22 x 6 mm (excluding connection
    pins)</SPAN></li>
  <li><SPAN size="2">Patch antenna supplied with interface card: 25 x 25 x 7 mm</SPAN></li>
</ul>
<h2>What else have I done with the Raspberry Pi?</h2>
<ul>
  <li><SPAN size="2">Receiving ADS-B signals on
1.09 GHz with a cheap TV dongle and feeding the results to <a href="http://www.coaa.co.uk/planeplotter.htm" target="_blank">Plane
Plotter</a> over a Wi-Fi link.  This allows you to put the receiver right
up close to the antenna, avoiding an expensive (or lossy) piece of cable, and
avoiding the need for a cable run at all.  Details are <a href="https://www.satsignal.eu/raspberry-pi/dump1090.html">here</a>. 
It&#39;s not much more than following someone else&#39;s instructions, but at least I
can vouch for every command on that page, and perhaps you will find something
useful there.</SPAN></li>
  <li><SPAN size="2">A <a href="https://www.satsignal.eu/raspberry-pi/DigitalClock.html">Digital Wall clock</a>
    using the Raspberry Pi - synchronised with NTP, of course!</SPAN></li>
</ul>
<h2>Acknowledgments</h2>
<ul>
  <li><SPAN size="2">To Michael Tharp at <a href="http://partiallystapled.com/">partially
    stapled productions</a> for making the interface board available</SPAN></li>
  <li><SPAN size="2"> <a href="http://m3php.com/"> Peter Goodhall</a> (2E0SQL) for discovering the restrictions in
the as-supplied NTP configuration file</SPAN></li>
  <li><SPAN size="2">Peter Mount for writing about his
experiences of <a href="http://blog.retep.org/2012/06/18/getting-gps-to-work-on-a-raspberry-pi/">getting
GPS to work on the Raspberry Pi</a></SPAN></li>
  <li><SPAN size="2">DavidK for the <a href="https://github.com/davidk/adafruit-raspberrypi-linux-pps">PPS
    information</a></SPAN></li>
  <li><SPAN size="2">jbeal for the <a href="http://www.raspberrypi.org/phpBB3/viewtopic.php?f=41&amp;t=1970&amp;start=25">recompiling
    NTP details</a>, and <a href="https://www.satsignal.eu/raspberry-pi/kernel-recompile.html">my own
    notes</a></SPAN></li>
  <li><SPAN size="2"><a href="http://www.irrational.net/2012/04/19/using-the-raspberry-pis-serial-port/">Clayton
    Smith</a> for the serial port software commands</SPAN></li>
  <li><SPAN size="2"><a href="http://www.uhf-satcom.com/">UHF-Satcom</a> for
    help and encouragement</SPAN></li>
  <li><SPAN size="2">More detailed information about rebuilding the Raspberry Pi
    kernel and some hardware based on my project may be found on <a href="http://bd.hauke-lampe.de/raspberry-pi/raspberry-pi-stratum-1-ntp-server-with-pps-accurate-timekeeping-at-low-cost.html">Hauke
    Lampe&#39;s Web page</a>.</SPAN></li>
  <li><SPAN size="2">If you prefer a Fedora Remix variant of Linux, take a look
    at Adam Dosch&#39;s <a href="http://doschman.blogspot.co.uk/2013/01/1pps-support-with-gpio-for-raspberry-pi.html">Dosch-man&#39;s
    blog</a> entry.  He uses the <a href="http://www.coolcomponents.co.uk/catalog/em408-module-p-44.html">EM408
    GPS module</a> and his own PPS source.</SPAN></li>
  <li><SPAN size="2">and to the folk in the comp.protocols.time.ntp Usenet newsgroup
    (<a href="https://groups.google.com/forum/#!forum/comp.protocols.time.ntp">or
    on Google</a>) and <a href="http://leapsecond.com/time-nuts.htm">time-nuts
    mailing list</a> who
    were enormously helpful and patient with someone whose Linux knowledge is
    sparse!</SPAN></li>
</ul>
<h2>I would like to support NTP</h2>

 <!--msnavigation--></td></tr><!--msnavigation--></tbody></div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 14:54:58 +0000</pubDate>
      <source>https://www.satsignal.eu/ntp/Raspberry-Pi-NTP.html</source>
    </item>
    <item>
      <title>An Old Hacker&#39;s Tips on Staying Employed</title>
      <link>https://madned.substack.com/p/an-old-hackers-tips-on-staying-employed</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___madned_substack_com_p_an-old-hackers-tips-on-staying-employed/image.jpg" /> 
<div id="readability-page-1" class="page"><div><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffed3bd69-b2ce-4c57-ae44-50766c7209c5_853x480.jpeg"></a></figure></div><p>I know my core audience is expecting crusty old tales of computing past, but this time I wanted to talk about something a little more topical — about job security, whether it indeed exists in any form, and what if anything you can do to improve your odds of staying employed.</p><p>Most people are probably familiar with the concept of <a href="https://en.wikipedia.org/wiki/Impostor_syndrome">Impostor Syndrome</a>, the phenomenon whereby someone who is actually qualified in some area still feels like they are illegitimate in some way compared to their peers.   I don’t think I often feel this way in my day-to-day career as an engineer, but that insecurity rears its ugly head frequently when I start to write about things, especially in the form of published advice.</p><p>I’ve <a href="https://madned.substack.com/p/general-expert-major-havoc">talked before</a> about me being a generalist, and as such, I do not consider myself enough of an expert in anything to be telling others how to do stuff, which probably plays into the whole impostor thing.  But I realized after 35+ years on the job, maybe the thing I’m expert in is being an old guy in the tech field.  </p><p>So here are a few things that I think were not obvious to me earlier in my career that I picked up later, sometimes even embarrassingly later.  If I can give anyone a head start on learning these things before they reach their 50’s, so much the better!</p><h2>Ride The Bull</h2><p>When I started work at <a href="https://en.wikipedia.org/wiki/History_of_Digital_Equipment_Corporation">Digital Equipment Corporation</a> in 1986, the company had never had a layoff in its 30-year history.  This changed around 1987, after <a href="https://en.wikipedia.org/wiki/Black_Monday_(1987)">“Black Monday”</a> rattled the stock market with a 20+% single-day decline.  Digital had its first layoff not long after that, and they felt so guilty about having to do it, they gave all the lucky participants one year’s salary as part of the termination package.  Most people pocketed the money and went right back to work - because in truth, this layoff was not so much the symptom of some macroeconomic collapse as it was due to Digital’s mounting woes.</p><p>I kept my job, but even so, Black Monday was a watershed moment in time for me. In my mind, it divided the previous years of (mostly illusionary even then) company-based job stability, from the era that followed.  The era we live in now features a new kind of job stability, that is of the personal sort.  </p><p>After that first layoff at Digital, further layoffs at the company followed, but also further opportunities elsewhere as many new companies sprung up.  We had the dot-com bubble, a boom, a recession, another boom.  And through all the ups and downs, companies big and small would lay people off — sometimes as part of a regular process, and sometimes due to unforeseen events. </p><p>Although there may be a few pockets of workplaces with more durable guarantees of continued employment (thinking about countries with strict labor laws, tenured professors, government work, and so on) for the most part, we in the tech world today have come to accept that there is no such thing as job security, and losing your job could happen at almost any time. (And sadly, you will probably not be getting a year’s salary if it happens to you now.)</p><p>I myself have been very lucky, and despite some close calls over 3.5 decades, have never missed a paycheck.  I’ll go on next to talk about some things that I think helped me improve my odds, but to be clear, it is really mostly a matter of luck.   I have seen really great people get laid off just because they were in the wrong place at the wrong time, and I’ve seen large numbers of people get laid off because either their company or the general economy had a downturn.</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7b2b5fe5-6183-4c15-ac91-2365303e7fcd_1200x605.jpeg"></a></figure></div><p>I tend to think of employment in tech as riding a bull.  Not a big bull-riding spectator, but from what little I have seen, it really isn’t about whether you are going to stay on the bull or not.  You are going to get thrown off, it’s just a matter of when.   I am hoping to make it all the way to retirement and to be the rare guy who stays on the bull, but you never know.  I like to assume though that if I do get thrown, I’ll pick myself up, dust myself off, and just move on to the next bull.  So that is really the first piece of advice I’ll give; don’t take losing a job or having to move jobs too personally, because it’s almost certain to happen at some point.  </p><p>Hopefully, this starting advice is useful, even though it is somewhat simplistic, and coming from someone who’s a little bit of a layoff-victim impostor.</p><h2>Develop Your Personal Brand</h2><p>I mentor junior engineers from time to time, and one concern that comes up is when someone is working on a project that is not doing well.  In some cases, they are worried that they might lose their job if the project is canceled, and in some cases, it is more of a matter of them feeling bad about their career, partly because they have entangled their own self-worth with the success of their project.</p><p>In both cases, I offer the same advice, which is to develop your “personal brand”.  If you think about a company that you have had a high opinion of in the past, you will probably find that your trust level for that company’s latest products is also generally higher, and you are more likely to shop there versus somewhere else in the future.  Brand loyalty is a thing, and even if you do not slavishly follow it, you are probably at least somewhat likely to return to something you know is of good quality than take a risk on something unknown.</p><p>Personal brand also works like that.  When you work on a project, even a crappy project, your personal brand is on display for all potential shoppers to see.  And even when everything is going south, others will notice which people are easy to work with, which people can be relied upon to do what they said they would do, and which people consistently produce a quality result.</p><p>This is what job security looks like, in the post-Black-Monday world.  If your project was canceled or your company tanks and you get laid off in the process, your personal brand will control your fate.  If you were a complete pain-in-the-ass to work with, dropped the ball a lot, said things you were going to do but didn’t do them, then good luck.  Your brand is trash, and you’ll have to find a place that doesn’t do backreferences in your probably-long job hunt.</p><p>On the other hand, if your personal brand is highly regarded, magic things will often just happen for you in this situation.   Maybe some other project in the company will realize you are now free, and snatch you up.  Maybe one of your fellow laid-off coworkers will find a better job and refer you as well.  Maybe your boss will feel super-guilty you got laid off and give you a glowing recommendation, or even help find something for you. </p><p>So basically, your “brand” gives you not the security that you will keep your current job forever, but the security that you will be able to have <em>a</em> job, on some regular basis.</p><h2>Make Your Boss Afraid</h2><p>What about not getting laid off in the first place?  Isn’t that more the goal here, Mad Ned?  It sure is. I’ve given a lot of lip service so far to the idea that layoffs happen and just get used to it, but ideally, the best job security advice is about how to reduce the chance they will happen to you.</p><p>The best tip I have here is to make sure your boss is afraid.  </p><p>Afraid?  Yep.  Afraid of what will happen, if you ever decide to leave.  I am not talking about doing stupid stuff like leaving your resume conspicuously lying around on the printer, or brinksmanship games of claiming you have another job offer just to get a raise.  I have seen people play these games to varying degrees of short-term success in the past, but inevitably it’s a long-term lose, as it destroys trust relationships (in addition to being petty and sometimes backfiring).</p><p>In saying “make your boss afraid”, I do not suggest doing anything to undermine the trust you have with your boss - establishing trust is important, with any coworker.  What I mean is, you want to be so useful that it will create a lot of problems for your boss (and by extension project, and company) if you were not there for some reason. </p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F089a5407-af45-4625-95fb-736c2a8cd3c3_983x763.jpeg"></a><figcaption>Norman offers to coordinate, so you don’t have to.</figcaption></figure></div><p>There is an old Star Trek episode called <a href="https://memory-alpha.fandom.com/wiki/I,_Mudd_(episode)">“I, Mudd”</a> where Captain Kirk and company faced an interesting adversary, a race of androids whose plans for domination centered around helping and serving humans, to the point where humanity became so reliant on this help that the androids would end up with complete control of the universe.</p><p>Even though I love this idea for an enemy threat because of its sheer novelty, my version of this concept is not quite so sinister.   No <a href="https://en.wikipedia.org/wiki/Machiavellianism_(psychology)">Machiavellian plotting</a> is required to accomplish this goal, but like the Star Trek storyline, a willingness to do what needs to be done, and to take ownership of things.  If possible, important things.  In some cases, even things that no one wants to do.</p><p>In a previous job as manager I ended up inheriting a lot of smaller technical work that no one else wanted to do, like getting the license token code updated, writing simple translators for file formats, and other small projects that fit within the available time I had when not doing the manager side of things.  When my boss had something that was giving him a headache and he asked if I could look into it, I would do it without complaint, and my personal brand in this job was to be a reliable go-to guy that could solve problems, kind of like <a href="https://quentin-tarantino.fandom.com/wiki/The_Wolf">Winston Wolf</a> from Pulp Fiction.</p><p>This approach served me well through many a layoff, when management would hold a <a href="https://everything2.com/title/lifeboat+exercises">lifeboat meeting</a> to determine who would be cut.   The goal in these situations is always to not be the lowest-ranked person under consideration, and the management-fear-thing does well for you in situations like that.</p><p>But it is not foolproof.  In 2009 or so when we had the last tech recession, a lot of people lost their jobs at our company.  My boss I am reasonably sure resisted laying me off for a while, again because he was probably afraid of having to deal with all these things I was keeping at bay.  But none of my responsibilities, in this case, were key enough to prevent me from getting onto the layoff list, and I did actually lose that job because the manager position itself was eliminated.</p><p>My personal brand here saved me though, because my boss had a positive opinion of me after putting out so many fires for him, and he recommended me to another group that was looking for a developer (and in retrospect, turned out to be a better fit for me than the management job anyway).  </p><p>So while I may have failed at making myself fearfully indispensable in that case, trying to get there was still worth it because it helped my personal brand, and boosted my overall job security in the process.</p><h2>Make Your Boss Happy</h2><p>Wait, what?  Make your boss happy? I thought we were supposed to make our boss afraid.  Well yes, he or she should be afraid, but only at certain times, like maybe raise time, or when the layoff list is being planned, or late at night when up worrying about life, work, and other existential matters.   The majority of the time though you want him or her to be happy — happy because they are successful.</p><p>Doctors say that if exercise could be given in pill form, it would be the most prescribed drug because it affects so many health-related things in positive ways.  Making your boss successful is Dr. Ned’s similar prescription here, because of the numerous positive effects it has for your career.  It goes beyond just doing a good job with your own personal assignments.  What it means is, putting yourself in your boss’s shoes, and thinking about what they need to be able to succeed in <em>their</em> boss’s eyes.</p><p>Give your boss stuff they can use.  Maybe it’s ideas and suggestions about how to frame or present work the group has done, or explain problems you have faced.  Maybe it’s a little bit of bonus content from time to time, that can be highlighted in a status report.  Maybe it’s helping to defuse a situation that would make your boss or the group look bad.  Sometimes it takes a little imagination to put yourself in your boss’s place, or maybe even some investigation on your part to find out what your boss’s true care-about’s really are, but this is worthwhile to do in general, anyway.</p><p>If taken too far, I guess these “managing up” strategies begin to border on sucking up to the boss or being overly calculating and political, but doing just the right amount of this can have immensely positive effects for your career, no pill needed!</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2cce4724-e652-4b25-b1d3-92dc8eb686e4_1024x478.png"></a></figure></div><p>This strategy by the way also includes the idea of letting your boss take credit for your work.  That is super-not-cool if it’s say a peer doing it, but when your boss takes credit for your work, it is usually (unless you have an awful boss, anyway) done in a way that helps him or her, while simultaneously raising your visibility in the company, and improving your personal brand.   </p><p>Successful bosses get more resources, more freedoms. And that is also quite good for you, and your team.</p><h2>The “Do It Anyway” Principle</h2><p>It’s unavoidable. You will hopefully work on many cool projects and be very productive in your career, but sooner or later you will end up on some task or project that is difficult to accomplish and/or generally unpleasant, all due to factors outside of your control. </p><p>This has happened to me on some occasions, and it is super-frustrating when it does.  I spent a lot of time developing software features for a product once that had a horrible regression test and code versioning environment, such that it required a large number of very long test runs and updates and merges before any code could be checked in.  Tests were unreliable and produced false positives all the time. It took hours if not days to integrate code.</p><p>This was eventually straightened out, but it took I would say, a couple of years.  In that intervening time though, life went on.  Products had to be developed in spite of the challenges we had with the environment.   </p><p>At our team standup meetings, we would go over the progress of things, and I would see people divide into two general categories.   The first category was people who were similarly frustrated and fed-up with the work environment issues we faced.  They were more vocal about it though and would complain about these issues frequently, even though they were for the most part, out of our collective control.</p><p>Tasks assigned to people in this first group would frequently get delayed, due to the problems we faced with the development environment.  Each day when we met and someone said they could not finish something because of some obstacle that cropped up, it was never really challenged because it was pretty unreasonable to expect people to be productive given various problems we faced.  The net effect though was, the tasks assigned to this group of people were not really getting done.</p><p>The second category of people, which I would try my best to be in (but admittedly, not always succeed) would be the people that somehow got their tasks done anyway, even though there were ridiculous constraints imposed by our environment issues that made it a lot harder.  My own personal way of dealing with this would be to look at it as a challenge, like: Can I pick up wet grapes with chopsticks?  Can I run more than 100 feet without throwing up because I’m so out of shape?  View everything as unnecessary feats I clearly was not expected to do but would attempt, anyway.</p><p>Gamifying things this way helped me get through some of these situations, but I think others used different strategies that also worked.  In the end, it is really about mindset.  You can take the stance that you should not be expected to be productive in a bad work environment, and you would be right.  If that motivates you to affect change in the environment, it is great.  But sometimes, we do not have that luxury.</p><p>There is something called the <a href="https://en.wikipedia.org/wiki/Serenity_Prayer">Serenity Prayer</a>, written by theologian Reinhold Niehbur which goes like this:</p><blockquote><p><em>God, grant me the serenity to accept the things I cannot change,</em></p></blockquote><p>Applicable.  Whether you subscribe to a particular theology, or not.  Coming back to the job security theme though, I will say that at least one person from the first category did get laid off later because they just weren’t getting enough done.  Not fair I know, because you shouldn’t be expected to have your productivity measured when you are being held back by things beyond your control.</p><p>The brutal truth though is, you still do.  We are all constantly measured in the eyes of our employers by what we actually deliver, and those who can find a way to succeed even in the face of adversity will always have an advantage over those who cannot, or do not want to.</p><h2>The “Two-And-Done” Rule</h2><p>I’ve saved my favorite piece of advice to the end.  The “Two-And-Done” Rule as I call it was something that did not occur to me until I was almost in my 50’s, and I really wish I had come up with it earlier because it probably would have helped me in a lot of past situations.</p><p>It has to do with how to handle disagreements that come up at work.  In my younger days, I would have a pretty strong desire to always have the right answer to things, and also to be seen as someone who has the right answer to things.  When I got into situations where I thought I was right about something that others disagreed with, I would tenaciously argue my point, without letting go.  </p><p>It did not matter if the person was my peer, subordinate, or superior, I would let them know just why they were wrong, and why we needed to do it my way.  Sometimes the decision on what to do was out of my control, and management would pick the thing I was against.  And even after the decision was made, I would continue to try to find ways to criticize it, reverse it, undermine it, or revisit it, because something that I knew was better was being ignored (and by extension, I think I felt I was being ignored as well).</p><p>But this take-no-prisoners approach to debate did not do me any good.  Rarely would my continued efforts to upturn settled law ever yield any good results, and in fact much the opposite — it would negatively impact my ability to affect change.  If I did this too often, people would begin to see me as argumentative, difficult to work with, not a team player, and so on.  I started getting left out of meetings where decisions were being made because no one wanted to get into yet another argument over things.</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fca70603c-c514-4b0d-aa07-31363459d6c5_700x350.jpeg"></a></figure></div><p>I will admit, I am still a big fan of being right all the time, and of having people agree with me.  What’s changed over the years though is first, an enhanced understanding that I in fact am not right all the time. But also that even when I am right, it isn’t always a guarantee that others will follow my advice, and if they do not, that it’s still OK.</p><p>So the Two-And-Done rule was born, wherein I will state my case the first time, and if whoever is arguing to the contrary does not agree after hearing my position, I’ll let it go.  But the next time the opportunity comes up, I will argue my point again.  Maybe allowing for a gap of time for people to consider my original point, or maybe allowing me time to refine and rephrase my ideas to be more convincing.</p><p>If I fail to get my way after the second time though, I am done.  I will even say as much to whomever I am debating if they are the final decision-maker.  I will say something like, “OK, let’s go your way then.  I still don’t completely agree with everything proposed here, but I think I’ve made my case, and we need to move on.”</p><p>Yielding in an argument like this has some weird, powerful effects.  One is, it kind of releases you from responsibility if things should go wrong.  And if a truly bad decision has been made, it is actually pretty likely that things <em>will</em> start going wrong.  <em>(Important Note: If you fail to convince people after two tries, you really do have to get behind the decision, and not try to sabotage or undermine it)</em></p><p>If it goes bad anyway and you have been graceful enough in your concession on the original bad decision, you can then just skip over the “I told you so!” part (it will just be implied), and go straight to the getting your way part.  This is because people will be more likely to revisit their decision if you did not make your original disagreement with it a super-unpleasant experience for them.</p><p>The added bonus of that is, it bumps up your personal brand, as someone who should maybe be listened to more often.  Sometimes though you end up just having to live with not getting your way, and learn to truly let go of whatever it was.  But more often than you might expect, this technique will end up causing things to pivot back your way, eventually.  </p><p>Anyway, this is not strictly a job-security-specific tip, as much as it’s a “how to be more effective at work” tip.  But being effective, indirectly, means job security.  </p><p>You might also wonder — shouldn’t the rule be “Three and Done”?  Things come in threes right, like three strikes, or the <a href="https://en.wikipedia.org/wiki/Rule_of_three_(writing)">rule-of-three</a>?  I just think three arguments is one argument too many to have about something.  You give your opinion once, and once more just in case you screwed up your pitch the first time. Beyond that though it has diminishing returns, in my experience at least.</p><h2>Who Needs Job Security Anyway?</h2><p>I’m someone who’s worked at only a couple of companies, for a very long time at each.  I would say it’s pretty clear I’ve prioritized job security over other things, like maximizing my income or job title or taking risky bets on startups or equity-driven opportunities.</p><p>So maybe not all of this advice I’ve given applies to you if your priorities are different. Also, maybe I am not qualified to talk about how to deal with some situations like having to job hop between startups and so on.  I can only offer what I have learned myself, take away what is useful.</p><p>If you take away only one thing though, take away the idea that job security doesn’t come from your job, it comes from you.  The Buddhists have this <a href="https://tricycle.org/magazine/buddhism-and-happiness/">philosophy</a> regarding happiness, that you should not have any dependencies on external things to make you happy because happiness comes from within.</p><p>Like job security, subscribing to this philosophy has the downside of, it means you are doing all the work.  Nobody is going to make you happy but yourself, and no job or company is going to give you job security, you have to do it on your own.</p><p>But it also means your fate is in your own hands.  And that is a great thing.</p><h2>Explore Further</h2><ul><li><p><a href="https://www.youtube.com/watch?v=YMg9HXSYcUc">Bull Riding 101 (YouTube)</a></p></li><li><p><a href="https://www.psychologytoday.com/us/blog/fighting-fear/201209/the-proper-way-argue">The Proper Way To Argue (Psychology Today)</a></p></li><li><p><a href="https://www.indeed.com/career-advice/career-development/job-security">Other Job Security Tips (Indeed.com)</a></p></li></ul><blockquote><p><strong>Next Time:   </strong>What it looks like inside the supply chain, from someone who has visited both ends.  The world of bossy customers and needy sellers collide in: <em>Vendor, Vidi, Vici !</em></p></blockquote><p><em>If I subscribe to the <a href="https://madned.substack.com/">Mad Ned Memo</a>, won’t I get spam? No! Just strange and nerdy tales of computer technology, past present, and future - delivered to your inbox regularly. (check the link to see some past articles)  It’s cost-free and ad-free, and you can unsubscribe at any time.</em></p><pre><code><code>The Mad Ned Memo takes subscriber privacy seriously, and does not share email addresses or other information with third parties.  For more details, </code><a href="https://madned.substack.com/about"><code>click here</code></a><code>.</code></code></pre></div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 12:31:22 +0000</pubDate>
      <source>https://madned.substack.com/p/an-old-hackers-tips-on-staying-employed</source>
    </item>
    <item>
      <title>Interactive introduction to game theory and trust</title>
      <link>https://ncase.me/trust/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___ncase_me_trust_/image.jpg" /> 
<div id="readability-page-1" class="page">
	<p>loading...</p> <!-- TRANSLATE THIS -->
	
	
















<!-- Core Engine -->













<!-- Simulations -->






<!-- Slides -->











<!-- Main Code -->

</div>]]></content:encoded>
      <pubDate>Mon, 09 Aug 2021 09:32:19 +0000</pubDate>
      <source>https://ncase.me/trust/</source>
    </item>
    <item>
      <title>Sony&#39;s new curved image sensors could shake up the whole camera industry</title>
      <link>https://www.digitalcameraworld.com/news/sonys-new-curved-image-sensors-could-shake-up-the-whole-camera-industry</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_digitalcameraworld_com_news_sonys-new-curved-image-sensors-could-shake-up-the-whole-camera-industry/image.jpg" /> 
<div id="readability-page-1" class="page"><article data-id="ne8isCMAFrZLCjaMZHDHnT">
<header>
<nav aria-label="Breadcrumbs">
<ol>
<li><a href="https://www.digitalcameraworld.com/">Home</a></li>
<li><a href="https://www.digitalcameraworld.com/news">News</a></li>
</ol>
</nav>


</header>
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<div>
<div>
<picture><source type="image/webp" alt="Diagram showing curved lens technology" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-1024-80.jpg.webp 1024w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f.jpg"/><source type="image/jpeg" alt="Diagram showing curved lens technology" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f-1024-80.jpg 1024w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f.jpg"/></picture>
</div>
</div>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/inkSZsLvHkxvHa9aBN3t5f.jpg"/>
<meta itemprop="height" content="600"/>
<meta itemprop="width" content="338"/>
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Sony)</span>
</figcaption>
</div>

<div id="article-body">
<p>It’s happened to us all. You spot something big, beautiful and visually impressive. It might be a building, a landscape or a giant piece of street art. Then you go to photograph it, but suddenly one thing becomes shockingly clear: however expensive your camera is, it can never quite compete with the human eye. </p><p>Shaped by millions of years of evolution, our eyes are quite frankly incredible. And the history of camera development has essentially been all about trying to mimic that level of perfection. </p><p><strong>• Read more: </strong><a href="https://www.digitalcameraworld.com/buying-guides/best-360-cameras" target="_blank"><strong>Best 360 cameras</strong></a></p><p>You can see it in the emergence of stereoscopic photography, for example, or the more recent development of 360-degree VR. There is, however, another way that manufacturers are trying to replicate the human eye, which doesn’t get as much press but promises to be quite revolutionary: curved sensors. </p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="Diagram showing curved lens technology" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf-970-80.jpg.webp 970w" data-original-mos="https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf.jpg"/><source type="image/jpeg" alt="Diagram showing curved lens technology" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf-970-80.jpg 970w" data-original-mos="https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RcpKL3UW9krpyd84jCvYCf.jpg"/></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Sony)</span></figcaption></figure><p>Rather than follow the curve of our eyes, the sensors in conventional digital cameras are flat. This results in an unnatural curvature in the image, and so the accompanying lenses have to be made in a way that corrects this distortion. This makes camera optics larger and heavier than they otherwise need to be. </p><p>Swapping a flat sensor for a curved one, consequently, should allow for lenses with a shorter and smaller diameter, with greater aperture and reduced light fall-off at the edge of the photo. </p><p>We say ‘should’ because while <a href="https://www.digitalcameraworld.com/news/is-sonys-medium-format-camera-with-a-curved-sensor-almost-ready" target="_blank">Sony has been developing this tech</a> for some time, as have other manufacturers <a href="https://www.digitalcameraworld.com/news/canon-has-just-patented-its-first-lenses-for-a-curved-sensor" target="_blank">such as Canon</a>, neither is bringing new cameras based on curved sensors to market just yet. </p><p>However, the big news is that Sony has just filed a new patent for the production of a curved sensor – one that looks like it has smartphone users in mind. </p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="Diagram showing curved lens technology" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf-970-80.jpg.webp 970w" data-original-mos="https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf.jpg"/><source type="image/jpeg" alt="Diagram showing curved lens technology" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf-970-80.jpg 970w" data-original-mos="https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/8XrEfxsGFGqx8mHkxkdzLf.jpg"/></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Sony)</span></figcaption></figure><p>You can see the patent in full <a href="https://www.j-platpat.inpit.go.jp/c1800/PU/JP-2021-108427/1D0D737B2A79C124B9DF0B3F0B72667F4CEDCEDB6369633E895CF467339C5D2D/11/ja" target="_blank" data-url="https://www.j-platpat.inpit.go.jp/c1800/PU/JP-2021-108427/1D0D737B2A79C124B9DF0B3F0B72667F4CEDCEDB6369633E895CF467339C5D2D/11/ja"><u>here</u></a>, although you’ll need a good knowledge of Japanese (not to mention camera technology) to understand it. Suffice to say that following its 2020 patent in this area, Sony is clearly serious about developing this new tech and getting it into the hands of consumers, before rivals can steal a march on it. </p><p>And Sony&#39;s interest is far from academic; there is a real commercial need at play here. Smartphones are increasingly at the cutting edge of photography tech and, with multi-lens cameras and 108MP sensors becoming increasingly common, brands are desperate for new ways to stand out from the crowd. </p><p>Curved sensors will potentially enable makers of both digital cameras and smartphones to leave their competitors in the dust, so there&#39;s an awful lot to play for here.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="Diagram showing curved lens technology" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf-970-80.jpg.webp 970w" data-original-mos="https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf.jpg"/><source type="image/jpeg" alt="Diagram showing curved lens technology" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf-970-80.jpg 970w" data-original-mos="https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/yKchNsgdCUo4oMkW4NytUf.jpg"/></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Sony)</span></figcaption></figure><p>We&#39;ve been saying for some time that curved sensors represent the next big leap forward in imaging technology, and this new patent from Sony makes us even surer of that. We imagine that rival manufacturers are quaking in their boots right now – unless, that is, they&#39;ve got their own curved sensors in the works. </p><p>Either way, we&#39;ll bring you all the news in the development of this game-changing tech the moment we learn of it, so keep watching.</p><p><strong>Read more: </strong></p><p><a href="https://www.digitalcameraworld.com/buying-guides/best-camera-phone" target="_blank">Best camera phones</a></p>

</div>
<div>
<div>
<picture><source type="image/webp" alt="Tom May" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" sizes="99vw" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/9gGAGRPzJeEG2f5kxRw4SM-300-80.jpg.webp 300w" data-original-mos="https://cdn.mos.cms.futurecdn.net/9gGAGRPzJeEG2f5kxRw4SM.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/9gGAGRPzJeEG2f5kxRw4SM.jpg" data-pin-nopin="true"/><source type="image/jpeg" alt="Tom May" onerror="if(this.src &amp;&amp; this.src.indexOf(&#39;missing-image.svg&#39;) !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" sizes="99vw" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/9gGAGRPzJeEG2f5kxRw4SM-300-80.jpg 300w" data-original-mos="https://cdn.mos.cms.futurecdn.net/9gGAGRPzJeEG2f5kxRw4SM.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/9gGAGRPzJeEG2f5kxRw4SM.jpg" data-pin-nopin="true"/></picture>
</div>

<p><span><p>Tom May is a freelance writer and editor specializing in art, photography, design and travel. He has been editor of Professional Photography magazine, associate editor at <a href="https://www.creativebloq.com" target="_blank">Creative Bloq</a>, and deputy editor at net magazine. He has also worked for a wide range of mainstream titles including The Sun, Radio Times, NME, T3, Heat, Company and Bella.</p></span>
</p></div>

<!-- No tag-links -->

</section>









<!-- No tag-links -->

</article></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 02:49:21 +0000</pubDate>
      <source>https://www.digitalcameraworld.com/news/sonys-new-curved-image-sensors-could-shake-up-the-whole-camera-industry</source>
    </item>
    <item>
      <title>Mastering Web Scraping in Python: Crawling from Scratch</title>
      <link>https://www.zenrows.com/blog/mastering-web-scraping-in-python-crawling-from-scratch</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_zenrows_com_blog_mastering-web-scraping-in-python-crawling-from-scratch/image.jpg" /> 
<div id="readability-page-1" class="page"><div> <p> Have you ever tried to crawl thousands of pages? Scale that even further? Handle and recover from system failures? </p> <p> After seeing how to <a href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-from-zero-to-hero">extract content from a website</a> and <a href="https://www.zenrows.com/blog/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja">how to avoid being blocked</a>, we&#39;ll take a look at the crawling process. To get data at scale, getting a few URLs by hand is not an option. We need to use an automated system that will discover new pages and visit them. </p> <p> <i>Disclaimer: for real-world usage, find suitable software. Below is more info on that. This guide pretends to be an introduction to how the crawling process works and doing the basics. But there are tons of details that need addressing.</i> </p> <h3 id="prerequisites">Prerequisites</h3> <div> <p> For the code to work, you will need <a href="https://www.python.org/downloads/" target="_blank" rel="noopener">python3 installed</a>. Some systems have it pre-installed. After that, install all the necessary libraries by running <code>pip install</code>. </p> <pre><code>pip install requests beautifulsoup4 </code></pre> </div> <h2 id="how-to-get-all-the-links-on-the-page">How to Get all the Links on the Page</h2> <p> From the first article in the series, we know that getting data from a webpage is easy with <code>requests.get</code> and <code>BeautifulSoup</code>. We will start by finding the links in a <a href="https://scrapeme.live/shop/page/1/" target="_blank" rel="noopener noreferrer nofollow">fake shop prepared for testing scraping</a>. </p> <p> The basics to get the content are the same. Then we get all the links on the paginator and add the links to a <code>set</code>. We chose set to avoid duplicates. As you can see, we hardcoded the selector for the links, meaning that it is not a universal solution. For the moment, we&#39;ll focus on the page at hand. </p> <pre><code>import requests 
from bs4 import BeautifulSoup 
 
to_visit = set() 
response = requests.get(&#39;https://scrapeme.live/shop/page/1/&#39;) 
soup = BeautifulSoup(response.content, &#39;html.parser&#39;) 
for a in soup.select(&#39;a.page-numbers&#39;): 
	to_visit.add(a.get(&#39;href&#39;)) 
 
print(to_visit) 
# {&#39;https://scrapeme.live/shop/page/2/&#39;, &#39;.../3/&#39;, &#39;.../46/&#39;, &#39;.../48/&#39;, &#39;.../4/&#39;, &#39;.../47/&#39;} </code></pre> <h2 id="one-url-at-a-time,-sequential">One URL at a Time, Sequential</h2> <p> Now we have several links but no way to visit them all. We need some kind of loop that will execute the extracting part for every URL available to fix that. Maybe the most straightforward way, although not the scalable one, is to use the same loop. But before that, there is a missing piece: avoid crawling the same page twice. </p> <p> We&#39;ll keep track of already visited links in another <code>set</code> and avoid duplicates by checking them before every request. In this case, <code>to_visit</code> is not being used, just maintained for demo purposes. To prevent visiting every page, we&#39;ll also add a <code>max_visits</code> variable. For now, we ignore the <code>robots.txt</code> file, but we have to be civil and nice. </p> <pre><code>visited = set() 
to_visit = set() 
max_visits = 3 
 
def crawl(url): 
	print(&#39;Crawl: &#39;, url) 
	response = requests.get(url) 
	soup = BeautifulSoup(response.content, &#39;html.parser&#39;) 
	visited.add(url) 
	for a in soup.select(&#39;a.page-numbers&#39;): 
		link = a.get(&#39;href&#39;) 
		to_visit.add(link) 
		if link not in visited and len(visited) &lt; max_visits: 
			crawl(link) 
 
crawl(&#39;https://scrapeme.live/shop/page/1/&#39;) 
 
print(visited) # {&#39;.../3/&#39;, &#39;.../1/&#39;, &#39;.../2/&#39;} 
print(to_visit) # { ... new ones added, such as pages 5 and 6 ... } </code></pre> <p> It is a recursive function with two exit conditions: there are no more links to visit, or we reached the maximum visits. In either case, it will exit and print the visited links and the ones pending. </p> <p> It is important to note that the same link can be added many times, but it will only get crawled once. In a big project, the idea would be to set a timer and only request each URL after a few days. </p> <h2 id="separation-of-concerns">Separation of Concerns</h2> <p> We said this is not about extracting or parsing content, but we need to separate concerns before it becomes entangled. For that, we&#39;ll create three helper functions: get HTML, extract links, and extract content. As their names imply, each of them will perform one of the main tasks of web scraping. </p> <p> The first one will get the HTML from a URL using the same library as earlier but wrapping it in a <code>try</code> block for security. </p> <pre><code>def get_html(url): 
	try: 
		return requests.get(url).content 
	except Exception as e: 
		print(e) 
		return &#39;&#39; </code></pre> <p> The second one, extracting the links, will work just as before. </p> <pre><code>def extract_links(soup): 
	return [a.get(&#39;href&#39;) for a in soup.select(&#39;a.page-numbers&#39;) 
		if a.get(&#39;href&#39;) not in visited] </code></pre> <p> The last one will be the placeholder for extracting the content we want. Since we are simplifying this part, it will get basic info from the same page, no need to enter on the detail page. </p> <pre><code>def extract_content(soup): 
	for product in soup.select(&#39;.product&#39;): 
		print(product.find(&#39;h2&#39;).text) 
 # Bulbasaur, Ivysaur, ... </code></pre> <p> Assembling it all together. </p> <pre><code>def crawl(url): 
	if not url or url in visited: 
		return 
	print(&#39;Crawl: &#39;, url) 
	visited.add(url) 
	html = get_html(url) 
	soup = BeautifulSoup(html, &#39;html.parser&#39;) 
	extract_content(soup) 
	links = extract_links(soup) 
	to_visit.update(links) </code></pre> <p> Noticed something different? The crawling logic is not attached to the link extracting part. Each of the helpers handles a single piece. And the <code>crawl</code> function acts as an orchestrator by calling them and applying the results. </p> <p> As the project evolves, all these parts could be moved to files or passed as parameters/callbacks. We can generalize the use cases if the core is independent of the selected page and content. </p> <p> Are we missing something? 🤔 </p> <pre><code>to_visit.add(&#39;https://scrapeme.live/shop/page/1/&#39;) 
 
while (len(to_visit) &gt; 0 and len(visited) &lt; max_visits): 
	crawl(to_visit.pop()) </code></pre> <h2 id="parallel-requests">Parallel Requests</h2> <p> There is a significant part missing: parallelism. HTTP request handlers are idle most of the time, waiting for the response to come back. It means that we can send several of them at the same time without overloading the machine. And then process them as they came back. </p> <p> It is relevant to note that this approach only works if the order is not imperative. But we are already using sets, which according to <a href="https://docs.python.org/3/tutorial/datastructures.html#sets" target="_blank" rel="noopener noreferrer nofollow">Python&#39;s definition</a>, &#34;a set is an <strong>unordered collection</strong> with no duplicate elements.&#34; Meaning that our process was unordered from the start. </p> <p> Before diving deep into the parallel requests, we have to understand a couple of concepts: synchronization and queues. </p> <h3 id="synchronized-queues">Synchronized Queues</h3> <div><p> There is a huge risk in threaded or <a href="https://en.wikipedia.org/wiki/Parallel_computing" target="_blank" rel="noopener noreferrer nofollow">parallel computing</a>: modifying the same variables or data structures from different threads. It means two of our requests would be adding new links to a set (i.e., <code>to_visit</code>). Since the data structure is not protected, both could read and write it like this: </p><ul> <li>Both read its content, i.e. <code>(1, 2, 3)</code> <i>(simplified)</i></li> <li>Thread one adds links to pages <code>4, 5</code>: <code>(1, 2, 3, 4, 5)</code></li> <li>Thread two adds links to pages <code>6, 7</code>: <code>(1, 2, 3, 6, 7)</code></li> </ul><p> How did this happen? When thread two wrote the new links, it added them to a set with only three elements. </p></div> <p> What can we do to avoid these conflicts? Synchronization or locking. From the <a href="https://docs.python.org/3/library/queue.html" target="_blank" rel="noopener noreferrer nofollow">docs</a>: &#34;queues use locks to temporarily block competing threads.&#34; It means that thread one would acquire a lock on the set, read and write without any problem, and then release the lock automatically. Meanwhile, thread two would have to wait until the lock becomes available. Only then read and write. </p> <pre><code>import queue 
 
q = queue.Queue() 
q.put(&#39;https://scrapeme.live/shop/page/1/&#39;) 
 
def crawl(url): 
	... 
	links = extract_links(soup) 
	for link in links: 
		if link not in visited: 
			q.put(link) </code></pre> <p> For the moment, it does not work. Do not worry. The changes in the existing code are minimum: we replaced <code>to_visit</code> with a queue. But queues need handlers or workers to process their content. With the above, we have created a Queue and added an item (the original one). We also modified the <code>crawl</code> function to put links in the queue instead of updating the previous set. </p> <p> We&#39;ll create a worker using the <a href="https://docs.python.org/3/library/threading.html" target="_blank" rel="noopener noreferrer nofollow">threading module</a> to process that queue. </p> <pre><code>from threading import Thread 
 
def queue_worker(i, q): 
	while True: 
		url = q.get() # Get an item from the queue, blocks until one is available 
		print(&#39;to process:&#39;, url) 
		q.task_done() # Notifies the queue that the item has been processed 
 
q = queue.Queue() 
Thread(target=queue_worker, args=(0, q), daemon=True).start() 
 
q.put(&#39;https://scrapeme.live/shop/page/1/&#39;) 
q.join() # Blocks until all items in the queue are processed and marked as done 
print(&#39;Done&#39;) 
 
# to process: https://scrapeme.live/shop/page/1/ 
# Done </code></pre> <p> We defined a new function that will handle the queued items. For that, we enter into an infinite loop that will stop when all the processing finishes. </p> <p> Now we need two more things: process items and create more threads (it would not be parallel with just one, would it?). </p> <pre><code>def queue_worker(i, q): 
	while True: 
		url = q.get() 
		if (len(visited) &lt; max_visits and url not in visited): 
			crawl(url) 
		q.task_done() 
 
q = queue.Queue() 
num_workers = 4 
for i in range(num_workers): 
	Thread(target=queue_worker, args=(i, q), daemon=True).start() </code></pre> <p> Be careful when running it since big numbers in <code>num_workers</code> and <code>max_visits</code> would start lots of requests. If the script had some minor bug for any reason, you could perform hundreds of requests in a few seconds. </p> <h3 id="performance">Performance</h3> <div><p> We run benchmarks with different settings only as a reference. </p><ul> <li>Sequential requests: 29,32s</li> <li>Queue with one worker (<code>num_workers = 1</code>): 29,41s</li> <li>Queue with two workers (<code>num_workers = 2</code>): 20,05s</li> <li>Queue with five workers (<code>num_workers = 5</code>): 11,97s</li> <li>Queue with ten workers (<code>num_workers = 10</code>): 12,02s</li> </ul><p> There is almost no difference between sequential requests and having one worker. Threads carry some overhead, but it is barely noticeable here. It would require a more severe load test. Once we start adding workers, that overhead pays off. We could add even more, but it won&#39;t affect the outcome since they will be idle most of the time. </p></div> <h2 id="distributed-processing">Distributed Processing</h2> <p> We won&#39;t cover the following scale-up step: distributing the crawling process among several servers. <a href="https://docs.python.org/3/library/multiprocessing.html#using-a-remote-manager" target="_blank" rel="noopener noreferrer nofollow">Python allows it</a>, and some libraries can help you with it (<a href="https://docs.celeryproject.org/en/stable/" target="_blank" rel="noopener noreferrer nofollow">Celery</a> or <a href="https://python-rq.org/" target="_blank" rel="noopener noreferrer nofollow">Redis Queue</a>). It is a huge step, and we have already covered enough for the day. </p> <p> As a quick preview, the idea behind it is the same as the one with the threads. Each item will be processed as we&#39;ve seen until now but in different threads or even machines running the same code. With this approach, we can scale even further; theoretically, with no limit. But in reality, there is always a limit or bottleneck, usually the central node that handles the distribution. </p> <h2 id="take-into-account-when-scaling-up">Take into Account when Scaling Up</h2> <p> We&#39;ve shown a simplified version of a crawling process for educational purposes. To apply all this at scale, you should consider several things first. </p> <h3 id="build-vs-buy-vs-open-source">Build vs Buy vs Open Source</h3> <p> Before you write your own library for crawling, try some of the options out there. Many great Open Source libraries can achieve it: <a href="https://docs.scrapy.org/en/latest/" target="_blank" rel="noopener noreferrer nofollow">Scrapy</a>, <a href="http://docs.pyspider.org/en/latest/" target="_blank" rel="noopener noreferrer nofollow">pyspider</a>, <a href="https://github.com/bda-research/node-crawler" target="_blank" rel="noopener noreferrer nofollow">node-crawler</a> (Node.js), or <a href="https://github.com/gocolly/colly" target="_blank" rel="noopener noreferrer nofollow">Colly</a> (Go). And many companies and services that provide you with scraping and crawling solutions. </p> <h3 id="avoid-being-blocked">Avoid being blocked</h3> <p> As we saw in a previous post, <a href="https://www.zenrows.com/blog/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja">there are several actions we can take to avoid blocking</a>. A couple of them are proxies and headers. Here is a simple snippet adding those to our current code. </p> <pre><code>proxies = { 
	&#39;http&#39;: &#39;http://190.64.18.177:80&#39;, 
	&#39;https&#39;: &#39;http://49.12.2.178:3128&#39;, 
} 
 
headers = { 
	&#39;authority&#39;: &#39;httpbin.org&#39;, 
	&#39;cache-control&#39;: &#39;max-age=0&#39;, 
	&#39;sec-ch-ua&#39;: &#39;&#34;Chromium&#34;;v=&#34;92&#34;, &#34; Not A;Brand&#34;;v=&#34;99&#34;, &#34;Google Chrome&#34;;v=&#34;92&#34;&#39;, 
	&#39;sec-ch-ua-mobile&#39;: &#39;?0&#39;, 
	&#39;upgrade-insecure-requests&#39;: &#39;1&#39;, 
	&#39;user-agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36&#39;, 
	&#39;accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&#39;, 
	&#39;sec-fetch-site&#39;: &#39;none&#39;, 
	&#39;sec-fetch-mode&#39;: &#39;navigate&#39;, 
	&#39;sec-fetch-user&#39;: &#39;?1&#39;, 
	&#39;sec-fetch-dest&#39;: &#39;document&#39;, 
	&#39;accept-language&#39;: &#39;en-US,en;q=0.9&#39;, 
} 
 
def get_html(url): 
	try: 
		response = requests.get(url, headers=headers, proxies=proxies) 
		return response.content 
	except Exception as e: 
		print(e) 
		return &#39;&#39; </code></pre>  <p> We won&#39;t go into details here, only a simple snippet for extracting id, name, and price per item. We store everything in a <code>data</code> array, which is not a great idea. But it is enough for demo purposes. </p> <pre><code>data = [] 
 
def extract_content(soup): 
	for product in soup.select(&#39;.product&#39;): 
		data.append({ 
			&#39;id&#39;: product.find(&#39;a&#39;, attrs={&#39;data-product_id&#39;: True})[&#39;data-product_id&#39;], 
			&#39;name&#39;: product.find(&#39;h2&#39;).text, 
			&#39;price&#39;: product.find(class_=&#39;amount&#39;).text 
		}) 
 
print(data) 
# [{&#39;id&#39;: &#39;759&#39;, &#39;name&#39;: &#39;Bulbasaur&#39;, &#39;price&#39;: &#39;£63.00&#39;}, {&#39;id&#39;: &#39;729&#39;, &#39;name&#39;: &#39;Ivysaur&#39;, &#39;price&#39;: &#39;£87.00&#39;}, ...] </code></pre> <h3 id="persistency">Persistency</h3> <p> We haven&#39;t persisted anything, and that does not scale. In a real-world case, we should store the content and even the HTML itself for later processing. And all the discovered URLs with the timestamp time. It all starts to sound like a database is needed. Depending on the necessities, we could store just the actual content or the whole URLs, dates, HTML, etcetera generically. </p> <h3 id="canonicals">Canonicals</h3> <p> The link extraction part does not take into consideration <a href="https://en.wikipedia.org/wiki/Canonical_link_element" target="_blank" rel="noopener noreferrer nofollow">canonical links</a>. A page can have more than one URL: query strings or hashes might modify it. In our case, we would crawl it twice. It&#39;s not a problem now, but something to consider. </p> <p> The right approach would be to add the canonical URL (if present) to the visited list. Then we could arrive at that same page from a different origin URL, but we would detect it as duplicate. We could also remove some query string parameters using <a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.url_query_cleaner" target="_blank" rel="noopener noreferrer nofollow">url_query_cleaner</a>. </p> <h3 id="robotstxt">Robots.txt</h3> <p> We have not checked it because we are using a test website prepared for scraping. But please check the robots file and comply with it when crawling an actual target. And above it, do not cause more traffic than they can handle. Once again, be civil and nice ;) </p> <h2 id="final-code">Final Code</h2> <pre><code>import requests 
from bs4 import BeautifulSoup 
import queue 
from threading import Thread 
 
starting_url = &#39;https://scrapeme.live/shop/page/1/&#39; 
visited = set() 
max_visits = 100 # careful, it will crawl all the pages 
num_workers = 5 
data = [] 
 
def get_html(url): 
	try: 
		response = requests.get(url) 
		# response = requests.get(url, headers=headers, proxies=proxies) 
		return response.content 
	except Exception as e: 
		print(e) 
		return &#39;&#39; 
 
def extract_links(soup): 
	return [a.get(&#39;href&#39;) for a in soup.select(&#39;a.page-numbers&#39;) 
			if a.get(&#39;href&#39;) not in visited] 
 
def extract_content(soup): 
	for product in soup.select(&#39;.product&#39;): 
		data.append({ 
			&#39;id&#39;: product.find(&#39;a&#39;, attrs={&#39;data-product_id&#39;: True})[&#39;data-product_id&#39;], 
			&#39;name&#39;: product.find(&#39;h2&#39;).text, 
			&#39;price&#39;: product.find(class_=&#39;amount&#39;).text 
		}) 
 
def crawl(url): 
	visited.add(url) 
	print(&#39;Crawl: &#39;, url) 
	html = get_html(url) 
	soup = BeautifulSoup(html, &#39;html.parser&#39;) 
	extract_content(soup) 
	links = extract_links(soup) 
	for link in links: 
		if link not in visited: 
			q.put(link) 
 
def queue_worker(i, q): 
	while True: 
		url = q.get() # Get an item from the queue, blocks until one is available 
		if (len(visited) &lt; max_visits and url not in visited): 
			crawl(url) 
		q.task_done() # Notifies the queue that the item has been processed 
 
q = queue.Queue() 
for i in range(num_workers): 
	Thread(target=queue_worker, args=(i, q), daemon=True).start() 
 
q.put(starting_url) 
q.join() # Blocks until all items in the queue are processed and marked as done 
 
print(&#39;Done&#39;) 
print(&#39;Visited:&#39;, visited) 
print(&#39;Data:&#39;, data) </code></pre> <h2 id="conclusion">Conclusion</h2> <div><p> We&#39;d like you to part with three main points: </p><ol> <li>Separate getting the HTML and extracting the links from the crawling itself.</li> <li>Choose the appropriate system for your use case: simple sequential, parallel, or distributed.</li> <li>Building from scratch to a vast scale will probably hurt. Take a look at free or paid libraries/solutions.</li> </ol> </div> <p> We are close to finishing this series on Web Scraping. Stay tuned for the next one on scaling this crawling process even further. </p> <p> <span>Did you find the content helpful? Spread the word and share it on <a target="_blank" rel="noopener" href="https://twitter.com/share?text=✍🏻Mastering%20Web%20Scraping%20in%20Python:%20Crawling🕸️.%20Learn%20how%20to%20build%20a%20website%20crawler%20for%20scraping%20at%20scale🔥&amp;url=https%3A%2F%2Fwww.zenrows.com%2Fblog%2Fmastering-web-scraping-in-python-crawling-from-scratch%2F%3Futm_source%3Dtwitter%26utm_medium%3Dshared%26rd%3D719674152">Twitter</a>, <a target="_blank" rel="noopener" href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fwww.zenrows.com%2Fblog%2Fmastering-web-scraping-in-python-crawling-from-scratch%2F%3Futm_source%3Dlinkedin%26utm_medium%3Dshared%26rd%3D1230575265">LinkedIn</a> or <a target="_blank" rel="noopener" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.zenrows.com%2Fblog%2Fmastering-web-scraping-in-python-crawling-from-scratch%2F%3Futm_source%3Dfacebook%26utm_medium%3Dshared%26rd%3D1934292713">Facebook</a>.</span> </p> <div> <p><strong>Rock-Solid Automated Data Extraction Infrastructure</strong> <span>Forget about blocks, managing headless browsers and servers. We handle the hassle for you.</span> </p> <p><a href="https://app.zenrows.com/register"> <span>Try ZenRows</span> </a> </p></div> </div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 13:55:49 +0000</pubDate>
      <source>https://www.zenrows.com/blog/mastering-web-scraping-in-python-crawling-from-scratch</source>
    </item>
    <item>
      <title>Show HN: Lona – Framework for responsive web apps in full Python without JS</title>
      <link>http://lona-web.org</link>
      <description></description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page">

   
  <div>
    
    <nav data-toggle="wy-nav-shift">
      
    </nav>

    <section data-toggle="wy-nav-shift">

      
      <nav aria-label="top navigation">
        
          <i data-toggle="wy-nav-top"></i>
          <a href="http://lona-web.org/">Lona</a>
        
      </nav>


      <div>
        
        <div>
        
          


















          <div role="main" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  

<ul>
    
        <li>    <a href="#what-is-lona">What is Lona?</a></li>
    
        <li>    <a href="#how-does-it-work">How does it work?</a></li>
    
</ul>

<div id="what-is-lona">
<h2>What is Lona?</h2>
<p>Lona is a web application framework, designed to write responsive web apps in
<strong>full</strong> Python.</p>
<p>Web is a solved problem in Python since ages, but traditionally Python handles
only the server side. If you want to have client side interaction like
click events or you want update content live, you have to write an additional
Javascript application.</p>
<p>Lona handles the server side and the client side, and provides a simple,
pythonic API to write self contained views.</p>
<div>
  <div><pre><span></span><span>from</span> <span>lona.html</span> <span>import</span> <span>HTML</span><span>,</span> <span>Button</span><span>,</span> <span>Div</span><span>,</span> <span>H1</span>
<span>from</span> <span>lona.view</span> <span>import</span> <span>LonaView</span>


<span>class</span> <span>MyView</span><span>(</span><span>LonaView</span><span>):</span>
    <span>def</span> <span>handle_request</span><span>(</span><span>self</span><span>,</span> <span>request</span><span>):</span>
        <span>message</span> <span>=</span> <span>Div</span><span>(</span><span>&#39;Button not clicked&#39;</span><span>)</span>
        <span>button</span> <span>=</span> <span>Button</span><span>(</span><span>&#39;Click me!&#39;</span><span>)</span>

        <span>html</span> <span>=</span> <span>HTML</span><span>(</span>
            <span>H1</span><span>(</span><span>&#39;Click the button!&#39;</span><span>),</span>
            <span>message</span><span>,</span>
            <span>button</span><span>,</span>
        <span>)</span>

        <span>self</span><span>.</span><span>show</span><span>(</span><span>html</span><span>)</span>

        <span># this call blocks until the button was clicked</span>
        <span>input_event</span> <span>=</span> <span>self</span><span>.</span><span>await_click</span><span>(</span><span>button</span><span>)</span>

        <span>if</span> <span>input_event</span><span>.</span><span>node</span> <span>==</span> <span>button</span><span>:</span>
            <span>message</span><span>.</span><span>set_text</span><span>(</span><span>&#39;Button clicked&#39;</span><span>)</span>

        <span>return</span> <span>html</span>
</pre></div>

  
</div></div>
<div id="how-does-it-work">
<h2>How does it work?</h2>
<p>Lona comes with a Javascript based browser library that speaks a specialized
protocol with the backend.
This protocol specifies messages like &#34;hey frontend, please show $HTML&#34; and
&#34;hey backend, someone clicked on node XY&#34;
<a href="http://lona-web.org/end-user-documentation/basic-concept.html">read more</a></p>
</div>

<!-- Root element of PhotoSwipe. Must have class pswp. -->


           </div>
           
          </div>
          <hr/><p>
Built with <a href="http://www.flamingo-web.org">Flamingo</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> by <a href="https://readthedocs.org">Read the Docs</a>.
        </p></div>
      </div>

    </section>

  </div>
  

  

  
  
    
   


</div>]]></content:encoded>
      <pubDate>Tue, 10 Aug 2021 13:19:45 +0000</pubDate>
      <source>http://lona-web.org</source>
    </item>
    <item>
      <title>MagicBell (YC W21) Is Hiring a Founding Front End Engineer (Remote)</title>
      <link>https://magicbell.notion.site/Founding-Frontend-Engineer-Fully-Remote-5d6c0eceed334d6e95f71dec6167453c</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___magicbell_notion_site_Founding-Frontend-Engineer-Fully-Remote-5d6c0eceed334d6e95f71dec6167453c/image.jpg" /> 
]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 12:00:51 +0000</pubDate>
      <source>https://magicbell.notion.site/Founding-Frontend-Engineer-Fully-Remote-5d6c0eceed334d6e95f71dec6167453c</source>
    </item>
    <item>
      <title>Open-sourcing a more precise time appliance</title>
      <link>https://engineering.fb.com/2021/08/11/open-source/time-appliance/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___engineering_fb_com_2021_08_11_open-source_time-appliance_/image.jpg" /> 
<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>Facebook engineers have built and open-sourced an Open Compute Time Appliance, an important component of the modern timing infrastructure.</span></li>
<li aria-level="1"><span>To make this possible, we came up with the Time Card — a PCI Express (PCIe) card that can turn almost any commodity server into a time appliance. </span></li>
<li aria-level="1"><span>With the help of the OCP community, we established the </span><a href="http://ocptap.com"><span>Open Compute Time Appliance Project</span></a><span> and open-sourced every aspect of the </span><a href="http://opentimeserver.com/"><span>Open Time Server</span></a><span>.</span></li>
</ul>
<p><span>In March 2020, we announced that we were in the process of switching over the servers in our data centers (together with our consumer products) to </span><a href="https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/"><span>a new timekeeping service based on the Network Time Protocol (NTP)</span></a><span>. The new service, built in-house and later open-sourced, was more scalable and improved the accuracy of timekeeping in the Facebook infrastructure from 10 milliseconds to 100 microseconds. </span><span>More accurate time keeping enables more advanced infrastructure management across our data centers, as well as faster performance of distributed databases.</span></p>
<p><span>The new NTP-based time architecture uses a </span><a href="https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/"><span>Stratum 1</span></a><span> — an important component that is directly linked to an authoritative source of time, such as a global navigation satellite system (GNSS) or a cesium clock.</span></p>
<p></p>
<p><span>Many companies rely on public NTP pools such as time.facebook.com to act as their Stratum 1. However, this approach has its drawbacks. These pools add dependency on internet connectivity and can impact overall security and reliability of the system. For instance, if connectivity is lost or an external service is down, it can result in outages or drift in timing for the dependent system.</span></p>
<p><span>To remove these dependencies, we’ve built a new dedicated piece of hardware called Time Appliance, which consists of a GNSS receiver and a miniaturized atomic clock (MAC). Users of time appliances can keep accurate time, even in the event of GNSS connectivity loss. While building our Time Appliance, we also invented a Time Card, a PCIe card that can turn any commodity server into a time appliance.</span></p>
<h2><span>Why do we need a new time device?</span></h2>
<p><span>Off-the-shelf time appliances have their own benefits. They work right out of the box and because many of these devices have been on the market for decades, they are battle-tested and generally stable enough to work without supervision for a long time.</span></p>
<p><span>However, these solutions also come with trade-offs:</span></p>
<ul>
<li aria-level="1"><span>In most cases, they are outdated and often vulnerable to software security concerns. Feature requests and security fixes may take months or even years to implement.</span></li>
<li aria-level="1"><span>These devices come with closed source software, which makes configuring and monitoring them limited and challenging. While configuration is done manually via a proprietary CLI or Web UI, monitoring often uses SNMP, a protocol that was not designed for this purpose.</span></li>
<li aria-level="1"><span>They include proprietary hardware that is not user-serviceable. When a single component breaks, there is no easy way to replace it. You have to either ship it to the vendor for repair or buy an entire new appliance.</span></li>
<li aria-level="1"><span>Since off-the-shelf devices are made in low quantities, they come with a higher markup and can become very costly to operate over time. The high cost associated with off-the-shelf devices create limitations for many in the industry. An open source version would open the door to broader applications.</span></li>
</ul>
<p><span>Until now, companies have had to accept these trade-offs and work within the constraints described above. We</span> <span>decided it was time to try something different, so we took a serious look at what it would take to build our new Time Appliance — specifically, one using the x86 architecture.</span></p>
<h2><span>Prototyping the Time Appliance </span></h2>
<p><span>Here’s a block diagram of what we envisioned:</span></p>
<p><span> </span><span><br/>
</span><span>It all starts from a GNSS receiver that provides the time of day (ToD) as well as the one pulse per second (PPS). When the receiver is backed by a high-stability oscillator (e.g., an atomic clock or an oven-controlled crystal oscillator), it can provide time that is nanosecond-accurate. The time is delivered across the network via an off-the-shelf network card which supports PPS in/out and hardware time stamping of packets, such as the NVIDIA Mellanox ConnectX-6 Dx used in our initial appliance.</span></p>
<p><span>The output of the GPS disciplined oscillator (GPSDO) was fed into the EXT time-stamping of the ConnectX-6 Dx network card. In addition, the GNSS receiver provides the ToD via a serial port and a popular GPS reporting protocol called NMEA. Using </span><span>ts2phc</span><span> tool allowed us to synchronize the physical hardware clock of the network interface controller (NIC) down to a couple of tens of nanoseconds, as shown below:</span></p>
<p></p>
<p><span>Our prototype gave us confidence that building such an appliance was possible. However, there was a lot of room for improvement.</span></p>
<p><span>To increase the reliability of the system, we divided it into two major parts: payload and delivery. The payload is the precision time that is essentially an interpolation system driven by a local oscillator to create nanoseconds of time measurement between consecutive PPS signals received by the GNSS receiver. We considered putting the GNSS receiver, the high-stability local oscillator, and the necessary processing logic into a PCIe form factor, and we called it the Time Card.</span></p>
<p></p>
<p><span>Here is the sketch of the Time Card we initially envisioned on a napkin:</span></p>
<p></p>
<p><span>We used an onboard MAC, a multiband GNSS receiver, and a field-programmable gate array (FPGA) to implement the time engine. The time engine’s job is to interpolate in nanoseconds the granularity required between consecutive PPS signals. The GNSS receiver also provides a ToD in addition to a 1 PPS signal. In the event of the loss of GNSS reception, the time engine relies on the ongoing synchronization of the atomic clock based on an average ensemble of the consecutive PPS pulses. </span></p>
<p><span>The time engine consists of a set of processing blocks implemented on the FPGA of the Time Card. These processing blocks include various filtering, synchronization, error checking, time-stamping, and PCIe-related subsystems to allow the Time Card to perform as a system peripheral that provides precision time for the open time server.</span></p>
<p><span>It should be noted that the accuracy of a GNSS receiver is within tens of nanoseconds, while the required ongoing synchronization (calibration) of the MAC is within 10 picoseconds (1,000 times more accurate). </span></p>
<p><span>At first, this sounds impossible. However, the GNSS system provides timing based on continuous communication with standard time. This ability allows the GNSS onboard clock to be constantly synchronized with a source of time provided to its constellation, giving it virtually no long-term drifting error. Therefore, the MAC’s calibration is performed via a comparison of a MAC-driven counter and the GNSS-provided PPS pulse. Taking more time for the comparison allows us to achieve a higher precision of calibration for the MAC. Of course, this is with the consideration that the MAC is a linear time invariant system.</span></p>
<p></p>
<p><span>In this block diagram, you can see a 10 MHz signal from the rubidium clock entering the time engine. This clock signal can be replaced by a 10 MHz SMA input. The clock signal feeds into a digital clock module and a digital PLL (12.5x resulted from 25 up and divided by 2), resulting in a 125 MHz frequency. The 125 MHz (8-nanosecond periods) feeds into the ToD unit.</span></p>
<p></p>
<p><span>The ToD unit associates the 8-nanosecond increments in digital values of 0b000001 since the LSB (least significant bit) is associated to 250 picoseconds (driven from 32 bits of subsecond accuracy on the gPTP).</span></p>
<p><span>On the other hand, the PPS signal coming filtered from the GNSS is used to snapshot the result of the increments. If the 125 MHz is accurate, the accumulated increments should result in exactly 1-second intervals. However, in reality, there is always a mismatch between the accumulated value and a theoretical 1-second interval.</span></p>
<p><span> </span><span><br/>
</span><span>The values can be adjusted using an internal PI (proportional and integral) control loop. The adjustment can be done by either altering the 0b000001 value by steps of 250 picoseconds or fine-tuning the 12.5x PPL. In addition, further (more finely tuned) adjustments can be applied by steering the rubidium oscillator. </span></p>
<p><span>The longer a GNSS isn’t available, the more time accuracy is lost. The rate of the time accuracy deterioration is called holdover. Usually, holdover is described as a timeframe for accuracy and how long it takes to exceed it. For example, the holdover of a MAC is within 1 microsecond for 24 hours. This means that after 24 hours, the time accuracy is nondeterministic but accurate within 1 microsecond.</span></p>
<p><span>As an alternative approach, we are counting on the new generation of chip-scale and miniaturized atomic clocks with their capability to receive PPS inputs. This allows the time engine of the Time Card to hand off the ultraprecision syntonization of the high-stability oscillator to the component rather than use digital resources to reach the target. </span></p>
<p><span>As a general principle, the more accurate the tuning, the better the holdover performance that can be achieved. In terms of delivery, using a NIC with precision timing ensures that network packets receive very accurate time stamps, which is critical for keeping the time precise as it is shared with other servers across the network. Such a NIC can also receive a PPS signal directly from the Time Card.</span></p>
<p><span>After conceptualizing the idea and various implementation iterations, we were able to put together a prototype. </span></p>
<p></p>
<h2><span>The Time Appliance in action</span></h2>
<p><span>The Time Card allows any x86 machine with a NIC capable of hardware time-stamping to be turned into a time appliance. This system is agnostic to whether it runs for NTP, PTP, SyncE, or any other time synchronization protocol, since the accuracy and stability provided by the Time Card is sufficient for almost any system. </span></p>
<p><span></span></p>
<p><span>The next step would be to install Linux. The Time Card driver is included in Linux kernel </span><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=773bda96492153e11d21eb63ac814669b51fc701"><span>5.15</span></a><span> or newer. Or, it can be built from the</span><a href="https://github.com/opencomputeproject/Time-Appliance-Project/tree/master/Time-Card/DRV"> <span>OCP GitHub repository</span></a><span> on kernel 5.12 or newer.</span></p>
<p><span>The driver will expose several devices, including the PHC clock, GNSS, PPS, and atomic clock serial:</span></p>
<pre><code>$ ls -l /sys/class/timecard/ocp0/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 device -&gt; ../../../0000:04:00.0/
-r--r--r--. 1 root 4096 Aug  3 19:49 gnss_sync
lrwxrwxrwx. 1 root    0 Aug  3 19:49 i2c -&gt; ../../xiic-i2c.1024/i2c-2/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 pps -&gt; ../../../../../virtual/pps/pps1/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 ptp -&gt; ../../ptp/ptp2/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 ttyGNSS -&gt; ../../tty/ttyS7/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 ttyMAC -&gt; ../../tty/ttyS8/
</code></pre>
<p><span>The driver also allows us to monitor the Time Card, the GNSS receiver, and the atomic clock status and flash a new FPGA bitstream using the <code>devlink</code> cli.</span></p>
<p><span>The only thing left to do is to configure the NTP and/or PTP server to use the Time Card as a reference clock. To configure chrony, one simply needs to specify <code>refclock</code> attribute:</span></p>
<pre><code>$ grep refclock /etc/chrony.conf
refclock PHC /dev/ptp2 tai poll 0 trust
</code></pre>
<p><span>And enjoy a very precise and stable NTP Stratum 1 server:</span></p>
<pre><code>$ chronyc sources
210 Number of sources = 1
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
#* PHC0                          0   0   377     1     +4ns[   +4ns] +/-   
36ns
</code></pre>
<p><span>For the PTP server (for example, </span><a href="https://github.com/facebookincubator/ptp"><span>ptp4u</span></a><span>) one will first need to synchronize Time Card PHC with the NIC PHC. This can be easily done by using the <code>phc2sys</code> tool which will sync the clock values with the high precision usually staying within single digits of nanoseconds:</span></p>
<pre><code>$ phc2sys -s /dev/ptp2 -c eth0 -O 0 -m</code></pre>
<p><span>For greater precision, it’s recommended to connect the Time Card and the NIC to the same CPU PCIe lane. For greater precision, one can connect the PPS output of the Time Card to the PPS input of the NIC.</span></p>
<p><span>To validate and confirm the precision, we’ve used an external validation device called Calnex Sentinel connected to the same network via several switches and an independent GNSS antenna. It can perform PPS testing as well as NTP and/or PTP protocols: </span></p>
<p></p>
<p><span>The blue line represents NTP measurement results. The precision stays within ±40 microseconds throughout the 48-hour measurement interval.</span></p>
<p><span>The orange line represents PTP measurement results. The offset is practically 0 ranging within nanoseconds range.</span></p>
<p><span>Indeed, when we compare 1 PPS between Time Card output and the internal reference of the Calnex Sentinel, we see that the combined error ranges within ±200 nanoseconds:</span></p>
<p></p>

<p><span>But what’s even more important is that these measurements demonstrate stability of the Time Appliance outputs.</span></p>
<p><span>In the event of the GNSS signal loss, we need to make sure the time drift (aka holdover) of the atomic-backed Time Card stays within 1 microsecond per 24 hours. Here is a graph showing the holdover of the atomic clock (SA.53s) over a 24-hour interval. As you can see, the PPS drift stays within 300 nanoseconds, which is within the atomic clock spec.</span></p>
<p></p>
<p><span>The modular design of the Time Card allows the swap of the atomic clock with an oven-controlled crystal oscillator (OCXO) or a temperature compensated crystal oscillator (TCXO) for a budget solution with the compromise on the holdover capabilities. </span></p>
<h2><span>Open-sourcing the design of the Time Appliance</span></h2>
<p><span>Building a device that is very precise, inexpensive, and free from vendor lock was an achievement on its own. But we wanted to have a greater impact on the industry. We wanted to truly set it free and make it open and affordable for everyone, from a research scientist to a large cloud data center. That’s why we engaged with the Open Compute Project (OCP) to create a brand-new </span><a href="http://www.ocptap.com/"><span>Time Appliance Project</span></a><span> (TAP). Under the OCP umbrella, we open-sourced at the Time Appliance Project </span><a href="https://github.com/opencomputeproject/Time-Appliance-Project"><span>GitHub repository</span></a><span>, including the specs, schematics, mechanics, BOM, and the source code. Now, as long as printing the PCB and soldering tiny components does not sound scary, anyone can build their own Time Card for a fraction of the cost of a regular time appliance. We also worked with several vendors such as </span><a href="https://www.orolia.com/about-the-atomic-reference-time-card-art-card/"><span>Orolia</span></a><span> who will be building and selling time cards, and NVIDIA who are selling the precision timing-capable ConnectX-6 Dx (and the precision timing-capable BlueField-2 DPU).</span></p>
<p><span>We published an Open Time Server spec at </span><a href="http://www.opentimeserver.com"><span>www.opentimeserver.com</span></a><span>, which explains in great detail how to combine the hardware (Time Card, Network Card, and a commodity server) and the software (OS driver, NTP, and/or PTP server) to build the Time Appliance. Building an appliance based on this spec will give full control to the engineers maintaining the device, improving monitoring, configuration, management, and security.</span></p>
<p><span>The Time Appliance is an important step in the journey to improve the timing infrastructure for everyone, but there is more to be done. We will continue to work on other elements, including improving the precision and accuracy of the synchronization of our own servers, and we intend to continue sharing this work with the Open Compute community.  </span></p>

		
	</div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 11:37:48 +0000</pubDate>
      <source>https://engineering.fb.com/2021/08/11/open-source/time-appliance/</source>
    </item>
    <item>
      <title>An intro to Pen Plotters (2019)</title>
      <link>https://medium.com/quarterstudio/an-intro-to-pen-plotters-29b6bd4327ba</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___medium_com_quarterstudio_an-intro-to-pen-plotters-29b6bd4327ba/image.jpg" /> 
<div id="readability-page-1" class="page"><article><span></span><div><section><div><div><div><div><div><div><p><a rel="noopener" href="https://medium.com/@tobiastoft?source=post_page-----29b6bd4327ba--------------------------------"></a></p></div></div></div></div><p id="c3ad"><em>I originally posted this on my website back in July 2014. It seems that interest in pen plotters has </em><a href="https://news.ycombinator.com/item?id=16495236" rel="noopener nofollow"><em>picked up recently</em></a><em>, so I decided to move the post to Medium which is arguably a much better place for it.</em></p></div></div><div><figure><p></p><figcaption>HP 7475A plotter — I paid $50 for it on eBay. The Delaunay triangulated Marilyn was one of my first experiments.</figcaption></figure></div><div><p id="cf90">I’ve been fascinated with pen plotters for a long time now and I’ve owned several of them — but for some reason never posted any of my explorations or findings online. Until now, at least.</p></div></section></div></article></div>]]></content:encoded>
      <pubDate>Tue, 10 Aug 2021 09:44:52 +0000</pubDate>
      <source>https://medium.com/quarterstudio/an-intro-to-pen-plotters-29b6bd4327ba</source>
    </item>
    <item>
      <title>The emergence of heat and humidity too severe for human tolerance (2020)</title>
      <link>https://advances.sciencemag.org/content/6/19/eaaw1838</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___advances_sciencemag_org_content_6_19_eaaw1838/image.jpg" /> 
<div id="readability-page-1" class="page"><div><div id="abstract-2"><h2>Abstract</h2><p id="p-3">Humans’ ability to efficiently shed heat has enabled us to range over every continent, but a wet-bulb temperature (TW) of 35°C marks our upper physiological limit, and much lower values have serious health and productivity impacts. Climate models project the first 35°C TW occurrences by the mid-21st century. However, a comprehensive evaluation of weather station data shows that some coastal subtropical locations have already reported a TW of 35°C and that extreme humid heat overall has more than doubled in frequency since 1979. Recent exceedances of 35°C in global maximum sea surface temperature provide further support for the validity of these dangerously high TW values. We find the most extreme humid heat is highly localized in both space and time and is correspondingly substantially underestimated in reanalysis products. Our findings thus underscore the serious challenge posed by humid heat that is more intense than previously reported and increasingly severe.</p></div><div id="sec-1"><h2>INTRODUCTION</h2><p id="p-4">Humans’ bipedal locomotion, naked skin, and sweat glands are constituents of a sophisticated cooling system (<a id="xref-ref-1-1" href="#ref-1"><em>1</em></a>). Despite these thermoregulatory adaptations, extreme heat remains one of the most dangerous natural hazards (<a id="xref-ref-2-1" href="#ref-2"><em>2</em></a>), with tens of thousands of fatalities in the deadliest events so far this century (<a id="xref-ref-3-1" href="#ref-3"><em>3</em></a>, <a id="xref-ref-4-1" href="#ref-4"><em>4</em></a>). The additive impacts of heat and humidity extend beyond direct health outcomes to include reduced individual performance across a range of activities, as well as large-scale economic impacts (<a id="xref-ref-5-1" href="#ref-5"><em>5</em></a>–<a id="xref-ref-7-1" href="#ref-7"><em>7</em></a>). Heat-humidity effects have prompted decades of study in military, athletic, and occupational contexts (<a id="xref-ref-8-1" href="#ref-8"><em>8</em></a>, <a id="xref-ref-9-1" href="#ref-9"><em>9</em></a>). However, consideration of wet-bulb temperature (TW) from the perspectives of climatology and meteorology began more recently (<a id="xref-ref-10-1" href="#ref-10"><em>10</em></a>, <a id="xref-ref-11-1" href="#ref-11"><em>11</em></a>).</p><p id="p-5">While some heat-humidity impacts can be avoided through acclimation and behavioral adaptation (<a id="xref-ref-12-1" href="#ref-12"><em>12</em></a>), there exists an upper limit for survivability under sustained exposure, even with idealized conditions of perfect health, total inactivity, full shade, absence of clothing, and unlimited drinking water (<a id="xref-ref-9-2" href="#ref-9"><em>9</em></a>, <a id="xref-ref-10-2" href="#ref-10"><em>10</em></a>). A normal internal human body temperature of 36.8° ± 0.5°C requires skin temperatures of around 35°C to maintain a gradient directing heat outward from the core (<a id="xref-ref-10-3" href="#ref-10"><em>10</em></a>, <a id="xref-ref-13-1" href="#ref-13"><em>13</em></a>). Once the air (dry-bulb) temperature (T) rises above this threshold, metabolic heat can only be shed via sweat-based latent cooling, and at TW exceeding about 35°C, this cooling mechanism loses its effectiveness altogether. Because the ideal physiological and behavioral assumptions are almost never met, severe mortality and morbidity impacts typically occur at much lower values—for example, regions affected by the deadly 2003 European and 2010 Russian heat waves experienced TW values no greater than 28°C (fig. S1). In the literature to date, there have been no observational reports of TW exceeding 35°C and few reports exceeding 33°C (<a id="xref-ref-9-3" href="#ref-9"><em>9</em></a>, <a id="xref-ref-11-2" href="#ref-11"><em>11</em></a>, <a id="xref-ref-14-1" href="#ref-14"><em>14</em></a>, <a id="xref-ref-15-1" href="#ref-15"><em>15</em></a>). The awareness of a physiological limit has prompted modeling studies to ask how soon it may be crossed. Results suggest that, under the business-as-usual RCP8.5 emissions scenario, TW could regularly exceed 35°C in parts of South Asia and the Middle East by the third quarter of the 21st century (<a id="xref-ref-14-2" href="#ref-14"><em>14</em></a>–<a id="xref-ref-16-1" href="#ref-16"><em>16</em></a>).</p><p id="p-6">Here, we use quality-assured station observations from HadISD (<a id="xref-ref-17-1" href="#ref-17"><em>17</em></a>, <a id="xref-ref-18-1" href="#ref-18"><em>18</em></a>) and high-resolution reanalysis data from ERA-Interim (<a id="xref-ref-19-1" href="#ref-19"><em>19</em></a>, <a id="xref-ref-20-1" href="#ref-20"><em>20</em></a>), verified against radiosondes and marine observations (see the Supplementary Materials) (<a id="xref-ref-21-1" href="#ref-21"><em>21</em></a>, <a id="xref-ref-22-1" href="#ref-22"><em>22</em></a>), to compute TW baseline values, geographic patterns, and recent trends. Uncertainties in TW from station data due to instrumentation and procedures are on the order of 0.5° to 1.0°C in all regions considered, an important consideration for proper interpretation of the results. Our approach of using TW and sea surface temperature (SST) observations as guidance for future TW projections offers a different line of evidence from previous research that used coupled or regional models without explicitly including historical station data.</p></div><div id="sec-2"><h2>RESULTS</h2><p id="p-7">Our survey of the climate record from station data reveals many global TW exceedances of 31° and 33°C and two stations that have already reported multiple daily maximum TW values above 35°C. These conditions, nearing or beyond prolonged human physiological tolerance, have mostly occurred only for 1- to 2-hours’ duration (fig. S2). They are concentrated in South Asia, the coastal Middle East, and coastal southwest North America, in close proximity to extraordinarily high SSTs and intense continental heat that together favor the occurrence of extreme humid heat (<a id="xref-ref-2-2" href="#ref-2"><em>2</em></a>, <a id="xref-ref-14-3" href="#ref-14"><em>14</em></a>). Along coastlines, the marine influence is manifest via anomalous onshore low-level winds during midday and afternoon hours, and these wind shifts can cause rapid dew point temperature (Td) increases in arid and semiarid coastal areas (figs. S3 to S9). Regionally coherent observational evidence supports these intense values: Of the stations along the Persian Gulf coastline with at least 50% data availability over 1979 to 2017, all have a historical 99.9th percentile of TW (the value exceeded roughly 14 times in 39 years) above 31°C (<a id="xref-fig-1-1" href="#F1">Fig. 1</a>; see fig. S1 for the all-time maximum). In the ERA-Interim reanalysis, the highest values are similarly located over the Persian Gulf and immediately adjacent land areas, as well as parts of the Indus River Valley (fig. S10). The spatiotemporal averaging inherent in reanalysis products causes ERA-Interim to be unable to represent the short durations and small areas of critical heat stress, causing its extreme TW values to be substantially lower than those of weather stations across the tropics and subtropics (fig. S11). In the Persian Gulf and adjacent Gulf of Oman, these differences are consistently in the range of −2° to −4°C (fig. S12). Larger bias but similar consistency is present along the eastern shore of the Red Sea, presenting a basis for future studies examining the reasons for this behavior, as well as further comparisons between station and reanalysis data.</p><figure id="F1">
  <div>
    <p><a href="https://advances.sciencemag.org/content/advances/6/19/eaaw1838/F1.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Observed global extreme humid heat. Color symbols represent the 99.9th percentile of observed daily maximum TW for 1979–2017 for HadISD stations with at least 50% data availability over this period. Marker size is inversely proportional to station density." rel="gallery-fragment-images-89026794" data-figure-caption="&lt;div class=&#34;highwire-markup&#34;&gt;&lt;span class=&#34;fig-label&#34;&gt;Fig. 1&lt;/span&gt; &lt;span class=&#34;caption-title&#34;&gt;Observed global extreme humid heat.&lt;/span&gt;&lt;p id=&#34;p-8&#34; class=&#34;first-child&#34;&gt;Color symbols represent the 99.9th percentile of observed daily maximum TW for 1979–2017 for HadISD stations with at least 50% data availability over this period. Marker size is inversely proportional to station density.&lt;/p&gt;&lt;div class=&#34;sb-div caption-clear&#34;/&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span></span></a></p>        
      </div>
    <figcaption id="F1-caption">
    <span>Fig. 1</span> <span>Observed global extreme humid heat.</span><p id="p-8">Color symbols represent the 99.9th percentile of observed daily maximum TW for 1979–2017 for HadISD stations with at least 50% data availability over this period. Marker size is inversely proportional to station density.</p>  </figcaption>
  </figure><p id="p-9">Other &gt;31°C hotspots in the weather station record emerge through surveying the globally highest 99.9th TW percentiles: eastern coastal India, Pakistan and northwestern India, and the shores of the Red Sea, Gulf of California, and southern Gulf of Mexico (<a id="xref-fig-1-2" href="#F1">Fig. 1</a>). All are situated in the subtropics, along coastlines (typically of a semienclosed gulf or bay of shallow depth, limiting ocean circulation and promoting high SSTs), and in proximity to sources of continental heat, which together with the maritime air comprise the necessary combination for the most exceptional TW (<a id="xref-ref-11-3" href="#ref-11"><em>11</em></a>). That subtropical coastlines are hotspots for heat stress has been noted previously (<a id="xref-ref-23-1" href="#ref-23"><em>23</em></a>, <a id="xref-ref-24-1" href="#ref-24"><em>24</em></a>); our analysis makes clear the broad geographic scope but also the large intraregional variations (<a id="xref-fig-1-3" href="#F1">Fig. 1</a>). Western South Asia stands as the main exception to this coastline rule, likely due to the efficient inland transport of humid air by the summer monsoon together with large-scale irrigation (<a id="xref-ref-15-2" href="#ref-15"><em>15</em></a>, <a id="xref-ref-25-1" href="#ref-25"><em>25</em></a>). Tropical forest and oceanic areas generally experience TW no higher than 31° to 32°C, perhaps a consequence of the high evapotranspiration potential and cloud cover, along with the greater instability of the tropical atmosphere. However, more research is needed on the thermodynamic mechanisms that prevent these areas from attaining higher values.</p><p id="p-10">Steep and statistically significant upward trends in extreme TW frequency (exceedances of 27°, 29°, 31°, and 33°C) and magnitude are present across weather stations globally (<a id="xref-fig-2-1" href="#F2">Fig. 2</a>). Each frequency trend represents more than a doubling of occurrences of the corresponding threshold between 1979 and 2017. Trends in ERA-Interim are strongly correlated with those of HadISD but are smaller for the highest values (<a id="xref-fig-2-2" href="#F2">Fig. 2</a>), consistent with ERA-Interim’s underestimation of extreme TW that is largest for the most extreme conditions (fig. S11). We also find a sharp peak in the number of global TW = 27°C and TW = 29°C extremes during the strong El Niño events of 1998 and 2016. Linearly detrending this global-TW-extremes time series reveals that the El Niño–Southern Oscillation (ENSO) correlation is largest for TW values that are high but not unusual (~27° to 28°C) across the tropics and subtropics (fig. S13). Further work is necessary to test to what extent this relationship may be related to the effect of ENSO on hydrological extremes at the global scale, on tropospheric-mean temperatures, or on SSTs in particular basins, and the implications of these effects for TW predictability (<a id="xref-ref-26-1" href="#ref-26"><em>26</em></a>, <a id="xref-ref-27-1" href="#ref-27"><em>27</em></a>). Overall, TW extremes in the tropics largely correspond on an interannual basis to mean TW (fig. S14), indicating that climate forcings and modes of internal variability resulting in mean temperature shifts can be expected to modulate tropical TW extremes. This is the case in the subtropics as well, although to a somewhat lesser extent.</p><figure id="F2">
  <div>
    <p><a href="https://advances.sciencemag.org/content/advances/6/19/eaaw1838/F2.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Global trends in extreme humid heat. (A to D) Annual global counts of TW exceedances above the thresholds labeled on the respective panel, from HadISD (black, right axes, with units of station days) and ERA-Interim grid points (gray, left axes, with units of grid-point days). We consider only HadISD stations with at least 50% data availability over 1979–2017. Correlations between the series are annotated in the top left of each panel, and dotted lines highlight linear trends. (E) Annual global maximum TW in ERA-Interim. (F) The line plot shows global mean annual temperature anomalies (relative to 1850–1879) according to HadCRUT4 (40), which we use to approximate each year’s observed warming since preindustrial; circles indicate HadISD station occurrences of TW exceeding 35°C, with radius linearly proportional to global annual count, measured in station days." rel="gallery-fragment-images-89026794" data-figure-caption="&lt;div class=&#34;highwire-markup&#34;&gt;&lt;span class=&#34;fig-label&#34;&gt;Fig. 2&lt;/span&gt; &lt;span class=&#34;caption-title&#34;&gt;Global trends in extreme humid heat.&lt;/span&gt;&lt;p id=&#34;p-11&#34; class=&#34;first-child&#34;&gt;(&lt;strong&gt;A&lt;/strong&gt; to &lt;strong&gt;D&lt;/strong&gt;) Annual global counts of TW exceedances above the thresholds labeled on the respective panel, from HadISD (black, right axes, with units of station days) and ERA-Interim grid points (gray, left axes, with units of grid-point days). We consider only HadISD stations with at least 50% data availability over 1979–2017. Correlations between the series are annotated in the top left of each panel, and dotted lines highlight linear trends. (&lt;strong&gt;E&lt;/strong&gt;) Annual global maximum TW in ERA-Interim. (&lt;strong&gt;F&lt;/strong&gt;) The line plot shows global mean annual temperature anomalies (relative to 1850–1879) according to HadCRUT4 (&lt;a id=&#34;xref-ref-40-1&#34; class=&#34;xref-bibr&#34; href=&#34;#ref-40&#34;&gt;&lt;em&gt;40&lt;/em&gt;&lt;/a&gt;), which we use to approximate each year’s observed warming since preindustrial; circles indicate HadISD station occurrences of TW exceeding 35°C, with radius linearly proportional to global annual count, measured in station days.&lt;/p&gt;&lt;div class=&#34;sb-div caption-clear&#34;/&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span></span></a></p>        
      </div>
    <figcaption id="F2-caption">
    <span>Fig. 2</span> <span>Global trends in extreme humid heat.</span><p id="p-11">(<strong>A</strong> to <strong>D</strong>) Annual global counts of TW exceedances above the thresholds labeled on the respective panel, from HadISD (black, right axes, with units of station days) and ERA-Interim grid points (gray, left axes, with units of grid-point days). We consider only HadISD stations with at least 50% data availability over 1979–2017. Correlations between the series are annotated in the top left of each panel, and dotted lines highlight linear trends. (<strong>E</strong>) Annual global maximum TW in ERA-Interim. (<strong>F</strong>) The line plot shows global mean annual temperature anomalies (relative to 1850–1879) according to HadCRUT4 (<a id="xref-ref-40-1" href="#ref-40"><em>40</em></a>), which we use to approximate each year’s observed warming since preindustrial; circles indicate HadISD station occurrences of TW exceeding 35°C, with radius linearly proportional to global annual count, measured in station days.</p>  </figcaption>
  </figure><p id="p-12">We also observe modulation on a seasonal scale, by considering as an illustrative example the South Asian monsoon region. There, the timing of peak TW varies with the advance of the summer monsoon (<a id="xref-ref-15-3" href="#ref-15"><em>15</em></a>). Splitting South Asia into “early monsoon” and “late monsoon” subregions, we find that the number of TW extremes is largest around the time of the local climatological monsoon onset date (<a id="xref-fig-3-1" href="#F3">Fig. 3</a>). Although equivalent extreme values of TW are possible before, during, and after the monsoon rains in any given year, they are of a different character; especially in the northern and western parts of the subcontinent, they become continually moister and have lower dry-bulb temperatures as summer progresses. Across the globe, such temperature and humidity variations occur within a well-defined bivariate space (fig. S15). That these variations are systematically associated with the summer monsoon in South Asia emphasizes the important role of moisture, and of weather systems on synoptic to subseasonal time scales, in controlling extreme TW (<a id="xref-ref-15-4" href="#ref-15"><em>15</em></a>, <a id="xref-ref-28-1" href="#ref-28"><em>28</em></a>). Our findings underscore the diversity of conditions that can lead to extreme humid heat in the same location at different times, suggesting that impacts adaptation strategies may benefit from taking this recognition into account. Such intraseasonal variability in TW also matters for physiological acclimation, which requires several-day time scales to develop (<a id="xref-ref-29-1" href="#ref-29"><em>29</em></a>); TW character is especially relevant when considering effects on human systems that vary in their sensitivity to humidity and temperature—for example, thermoregulation and energy demand for artificial cooling are strongly affected by TW, whereas the materials that make up the built environment are principally affected by temperature alone (<a id="xref-ref-13-2" href="#ref-13"><em>13</em></a>, <a id="xref-ref-30-1" href="#ref-30"><em>30</em></a>).</p><figure id="F3">
  <div>
    <p><a href="https://advances.sciencemag.org/content/advances/6/19/eaaw1838/F3.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Monsoon-modulated seasonality of extreme humid heat. (A) Early monsoon areas (light orange shading; " rel="gallery-fragment-images-89026794" data-figure-caption="&lt;div class=&#34;highwire-markup&#34;&gt;&lt;span class=&#34;fig-label&#34;&gt;Fig. 3&lt;/span&gt; &lt;span class=&#34;caption-title&#34;&gt;Monsoon-modulated seasonality of extreme humid heat.&lt;/span&gt;&lt;p id=&#34;p-13&#34; class=&#34;first-child&#34;&gt;(&lt;strong&gt;A&lt;/strong&gt;) Early monsoon areas (light orange shading; &lt;June 15 average onset date) and late monsoon areas (green shading; ≥June 15 average onset date) in South Asia. (&lt;strong&gt;B&lt;/strong&gt;) (Solid line) Mean annual number of TW exceedances of 31°C per station, by pentad, in the early monsoon areas. (Dashed line) Mean relative humidity associated with these exceedances. The division between the brown- and blue-shaded sections represents the station-weighted-average climatological monsoon onset date. (&lt;strong&gt;C&lt;/strong&gt;) Same as in (B), but for the late monsoon areas.&lt;/p&gt;&lt;div class=&#34;sb-div caption-clear&#34;/&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span></span></a></p>        
      </div>
    <figcaption id="F3-caption">
    <span>Fig. 3</span> <span>Monsoon-modulated seasonality of extreme humid heat.</span><p id="p-13">(<strong>A</strong>) Early monsoon areas (light orange shading; &lt;June 15 average onset date) and late monsoon areas (green shading; ≥June 15 average onset date) in South Asia. (<strong>B</strong>) (Solid line) Mean annual number of TW exceedances of 31°C per station, by pentad, in the early monsoon areas. (Dashed line) Mean relative humidity associated with these exceedances. The division between the brown- and blue-shaded sections represents the station-weighted-average climatological monsoon onset date. (<strong>C</strong>) Same as in (B), but for the late monsoon areas.</p>  </figcaption>
  </figure><p id="p-14">While our analysis of weather stations indicates that TW has already been reported as having exceeded 35°C in limited areas for short periods, this has not yet occurred at the regional scale represented by reanalysis data, which is also the approximate scale of model projections of future TW extremes considered in previous studies (<a id="xref-ref-14-4" href="#ref-14"><em>14</em></a>, <a id="xref-ref-15-5" href="#ref-15"><em>15</em></a>). To increase the comparability of our station findings with these model projections, we implement a generalized extreme value (GEV) analysis to estimate the amount of global warming from the preindustrial period until TW will regularly exceed 35°C at the global hottest ERA-Interim grid cells, currently all located in the Persian Gulf area (<a id="xref-fig-4-1" href="#F4">Fig. 4</a>). Complete details of this procedure are in Materials and Methods. In brief, we fit a nonstationary GEV model to the grid cells experiencing the highest TW values, with the GEV location parameter a function of the annual global-mean air-temperature anomaly. This enables us to quantify how much global warming is required for annual maximum TW ≥ 35°C to become at most a 1-in-30-year event at any grid cell. We conduct this analysis solely for grid cells where the nonstationary GEV model is a significantly (<em>P</em> &lt; 0.05) better fit to the annual maximum time series (1979–2017) than a stationary alternative. We then define the temperature of emergence (ToE) as the amount of global warming required until TW ≥35°C is at most a 1-in-30-year event at the ERA-Interim spatiotemporal scale, such that the lowest ToE at any grid cell approximates the first occurrences of TW = 35°C that are widespread and sustained enough to cause serious or fatal health impacts, as estimated from physiological studies (<a id="xref-ref-6-1" href="#ref-6"><em>6</em></a>, <a id="xref-ref-10-4" href="#ref-10"><em>10</em></a>, <a id="xref-ref-31-1" href="#ref-31"><em>31</em></a>).</p><figure id="F4">
  <div>
    <p><a href="https://advances.sciencemag.org/content/advances/6/19/eaaw1838/F4.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Projections of extreme humid heat exceeding the physiological survivability limit. (A) Shading shows the amount of global warming (since preindustrial) until TW = 35°C is projected to become at least a 1-in-30-year event at each grid cell according to a nonstationary GEV model. In blank areas, more than 4°C of warming is necessary. Black dots indicate ERA-Interim grid cells with a maximum TW (1979–2017) in the hottest 0.1% of grid cells worldwide. (B) Total area with TW of at least 35°C, as a function of mean annual temperature change 〈T〉 from the preindustrial period. Red (green) vertical lines highlight the lowest 〈T〉 for which there are nonzero areas over land (sea)—the respective ToE. (C) Bootstrap estimates of the ToE. See text for details of this definition and calculation." rel="gallery-fragment-images-89026794" data-figure-caption="&lt;div class=&#34;highwire-markup&#34;&gt;&lt;span class=&#34;fig-label&#34;&gt;Fig. 4&lt;/span&gt; &lt;span class=&#34;caption-title&#34;&gt;Projections of extreme humid heat exceeding the physiological survivability limit.&lt;/span&gt;&lt;p id=&#34;p-15&#34; class=&#34;first-child&#34;&gt;(&lt;strong&gt;A&lt;/strong&gt;) Shading shows the amount of global warming (since preindustrial) until TW = 35°C is projected to become at least a 1-in-30-year event at each grid cell according to a nonstationary GEV model. In blank areas, more than 4°C of warming is necessary. Black dots indicate ERA-Interim grid cells with a maximum TW (1979–2017) in the hottest 0.1% of grid cells worldwide. (&lt;strong&gt;B&lt;/strong&gt;) Total area with TW of at least 35°C, as a function of mean annual temperature change 〈&lt;em&gt;T&lt;/em&gt;〉 from the preindustrial period. Red (green) vertical lines highlight the lowest 〈&lt;em&gt;T&lt;/em&gt;〉 for which there are nonzero areas over land (sea)—the respective ToE. (&lt;strong&gt;C&lt;/strong&gt;) Bootstrap estimates of the ToE. See text for details of this definition and calculation.&lt;/p&gt;&lt;div class=&#34;sb-div caption-clear&#34;/&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span></span></a></p>        
      </div>
    <figcaption id="F4-caption">
    <span>Fig. 4</span> <span>Projections of extreme humid heat exceeding the physiological survivability limit.</span><p id="p-15">(<strong>A</strong>) Shading shows the amount of global warming (since preindustrial) until TW = 35°C is projected to become at least a 1-in-30-year event at each grid cell according to a nonstationary GEV model. In blank areas, more than 4°C of warming is necessary. Black dots indicate ERA-Interim grid cells with a maximum TW (1979–2017) in the hottest 0.1% of grid cells worldwide. (<strong>B</strong>) Total area with TW of at least 35°C, as a function of mean annual temperature change 〈<em>T</em>〉 from the preindustrial period. Red (green) vertical lines highlight the lowest 〈<em>T</em>〉 for which there are nonzero areas over land (sea)—the respective ToE. (<strong>C</strong>) Bootstrap estimates of the ToE. See text for details of this definition and calculation.</p>  </figcaption>
  </figure><p id="p-16">Our method yields a ToE of 1.3°C over the waters of the Persian Gulf (90% confidence interval, 0.81° to 1.73°C) and of 2.3°C for nearby land grid cells (1.4° to 3.3°C) (<a id="xref-fig-4-2" href="#F4">Fig. 4</a>). Adjusting these numbers for ERA-Interim’s robust Persian Gulf differences of approximately −3°C for extreme TW (fig. S12) supports the conclusion from the station observations that recent warming has increased exceedances of TW = 35°C, but that this threshold has most likely been achieved on occasion throughout the observational record (<a id="xref-fig-2-3" href="#F2">Fig. 2</a>). The strong marine influence on these values is also apparent in <a id="xref-fig-1-4" href="#F1">Fig. 1</a>.</p><p id="p-17">To further assess the physical realism of our GEV extrapolation, we additionally examine observed annual maximum (monthly mean) SSTs. An atmospheric boundary layer fully equilibrated with the ocean surface would be at saturation and have the same temperature as the underlying SSTs, meaning that, in principle, 35°C is the lowest SST that could sustain the critical 35°C value of TW in the air above. In reality, equilibrium will not be achieved if air-mass residence times over extreme SSTs are too short, which is more likely if the vertical profile of the atmosphere allows strong surface heating to trigger deep convection (<a id="xref-ref-10-5" href="#ref-10"><em>10</em></a>). Current large-scale SSTs and their trends may therefore provide some guidance as to whether our projections of extreme TW are physically plausible. It is in this context that we note monthly mean SSTs exceeding the 35°C threshold for the first time, reaching 35.2°C in the Persian Gulf in 2017 (<a id="xref-fig-5-1" href="#F5">Fig. 5</a>). As a result, our GEV projection of large-scale maritime TW ≥ 35°C, for less than 1.5°C warming, appears physically consistent with SST observations at the same scale. Analogous corroboration of station-based TW ≥ 35°C events is provided by point scale, hourly SST and TW across the Persian Gulf from an independent database of marine observations (see the Supplementary Materials) (<a id="xref-ref-21-2" href="#ref-21"><em>21</em></a>), in which we find SSTs have exceeded 35°C in every year since 1979, with ~33% of July to September 2017 observations above this threshold. During the summer of 2017, reports of Persian Gulf over-water TW ≥ 35°C also peaked at ~6% of all TW measurements there.</p><figure id="F5">
  <div>
    <p><a href="https://advances.sciencemag.org/content/advances/6/19/eaaw1838/F5.large.jpg?width=800&amp;height=600&amp;carousel=1" title="Trends and maxima of observed SST. (A) Annual maximum of monthly SST across all grid cells in the HadISST dataset; orange dashed line is a running 30-year average, and red line marks 35°C. (B) All-time maximum SST around the Persian Gulf and Arabian Sea. The blue points mark locations where monthly mean SST rose above 35°C in 2017." rel="gallery-fragment-images-89026794" data-figure-caption="&lt;div class=&#34;highwire-markup&#34;&gt;&lt;span class=&#34;fig-label&#34;&gt;Fig. 5&lt;/span&gt; &lt;span class=&#34;caption-title&#34;&gt;Trends and maxima of observed SST.&lt;/span&gt;&lt;p id=&#34;p-18&#34; class=&#34;first-child&#34;&gt;(&lt;strong&gt;A&lt;/strong&gt;) Annual maximum of monthly SST across all grid cells in the HadISST dataset; orange dashed line is a running 30-year average, and red line marks 35°C. (&lt;strong&gt;B&lt;/strong&gt;) All-time maximum SST around the Persian Gulf and Arabian Sea. The blue points mark locations where monthly mean SST rose above 35°C in 2017.&lt;/p&gt;&lt;div class=&#34;sb-div caption-clear&#34;/&gt;&lt;/div&gt;" data-icon-position="" data-hide-link-title="0"><span></span></a></p>        
      </div>
    <figcaption id="F5-caption">
    <span>Fig. 5</span> <span>Trends and maxima of observed SST.</span><p id="p-18">(<strong>A</strong>) Annual maximum of monthly SST across all grid cells in the HadISST dataset; orange dashed line is a running 30-year average, and red line marks 35°C. (<strong>B</strong>) All-time maximum SST around the Persian Gulf and Arabian Sea. The blue points mark locations where monthly mean SST rose above 35°C in 2017.</p>  </figcaption>
  </figure></div><div id="sec-3"><h2>DISCUSSION</h2><p id="p-19">The station-based approach that we take here and the model-based approach taken in previous studies (<a id="xref-ref-14-5" href="#ref-14"><em>14</em></a>–<a id="xref-ref-16-2" href="#ref-16"><em>16</em></a>) represent different methods for obtaining valuable perspective on the genesis and characteristics of global TW extremes. The primary strength of station data is its ability to precisely capture local conditions, but even the best-available station data have limitations, uncertainties, and potential unobserved humidity biases (for example, due to observational procedures, instrumentation type, or siting), as well as highly incomplete spatial coverage (see discussion in the Supplementary Materials) (<a id="xref-ref-32-1" href="#ref-32"><em>32</em></a>, <a id="xref-ref-33-1" href="#ref-33"><em>33</em></a>). In contrast, reanalysis products and high-resolution regional models satisfy the need for spatiotemporal continuity and consistency and allow analysis of additional variables, but often underestimate extremes (<a id="xref-ref-34-1" href="#ref-34"><em>34</em></a>).</p><p id="p-20">In this study, we demonstrate that efforts to better understand extreme TW would benefit from further close examination, and improved standardization and integration, of station data to alleviate model shortcomings—especially along coasts where TW can vary markedly over small distances and where high-quality humidity data are therefore essential—but that station-based and physical modeling–based approaches are fundamentally complementary. Further research into the origins of extreme-TW biases in gridded products and continued advances in data assimilation would also help enable the development of a more unified approach drawing on all available sources of knowledge. For instance, it is important to understand the treatment of extreme values in reanalyses, and whether false-positive or false-negative rejections might be taking place, particularly as temperature and humidity distributions shift toward ever-higher values. Key multiscale TW processes necessitating closer comparison between observations and models include coastal upwelling, atmospheric convection, land-atmosphere interactions, and atmospheric variability linked to SSTs (<a id="xref-ref-28-2" href="#ref-28"><em>28</em></a>)—for instance, at the hourly, 1- to 10-km scale. Detailed analyses of individual events could help illuminate the unfolding interactions of processes and provide additional investigative power, such as in tracing and forecasting the rapid increases in humidity, which tend to accompany TW extremes (fig. S5), and in assessing the role of topography and land use/land cover in creating apparent TW hotspots (fig. S4). Studies comparing biases and trends in TW and SSTs among reanalyses, models, and regions would be especially beneficial, as would investigation of the sensitivity of extreme-TW projections to historical variability, changes in forcing patterns, and statistical methodologies.</p><p id="p-21">Imminent severe humid heat provides incentive for a broad interdisciplinary research initiative to better characterize health impacts. Increased collection of high-resolution health data, international collaborations with public health experts and social scientists, and dedicated modeling projects would aid in answering questions about how vulnerable populations (such as the elderly, outdoor laborers, and those with preexisting health conditions) will be adversely affected as peak TW advances further into the extreme ranges we consider here. Of particular salience is the need to ascertain how acclimation to high-heat-stress conditions is diminished as the physiological survivability limit is approached. Such efforts may also help resolve the reasons for the paucity of reported mortality and morbidity impacts associated with observed near 35°C conditions (<a id="xref-ref-11-4" href="#ref-11"><em>11</em></a>, <a id="xref-ref-14-6" href="#ref-14"><em>14</em></a>).</p><p id="p-22">Our findings indicate that reported occurrences of extreme TW have increased rapidly at weather stations and in reanalysis data over the last four decades and that parts of the subtropics are very close to the 35°C survivability limit, which has likely already been reached over both sea and land. These trends highlight the magnitude of the changes that have taken place as a result of the global warming to date. At the spatial scale of reanalysis, we project that TW will regularly exceed 35°C at land grid points with less than 2.5°C of warming since preindustrial—a level that may be reached in the next several decades (<a id="xref-ref-35-1" href="#ref-35"><em>35</em></a>). According to our weather station analysis, emphasizing land grid points underplays the true risks of extreme TW along coastlines, which tends to occur when marine air masses are advected even slightly onshore (<a id="xref-ref-14-7" href="#ref-14"><em>14</em></a>). The southern Persian Gulf shoreline and northern South Asia are home to millions of people, situating them on the front lines of exposure to TW extremes at the edge of and outside the range of natural variability in which our physiology evolved (<a id="xref-ref-36-1" href="#ref-36"><em>36</em></a>). The deadly heat events already experienced in recent decades are indicative of the continuing trend toward increasingly extreme humid heat, and our findings underline that their diverse, consequential, and growing impacts represent a major societal challenge for the coming decades.</p></div><div id="sec-4"><h2>MATERIALS AND METHODS</h2><div id="sec-5"><h3>Weather station observations</h3><p id="p-23">We use HadISD, version 2.0.1.2017f, which is produced by the Met Office Hadley Centre as a more rigorously quality-controlled version of the National Climatic Data Center Integrated Surface Database (ISD) (<a id="xref-ref-17-2" href="#ref-17"><em>17</em></a>, <a id="xref-ref-18-2" href="#ref-18"><em>18</em></a>). HadISD results from the implementation of additional data availability and quality control procedures to ISD, including checks on both temperature and Td, the two variables required for computing TW. Because of a lack of good-quality data in the tropics, our conclusions are most reliable in the subtropics and midlatitudes, especially where multiple nearby stations are in agreement. TW uncertainties range from ~0.5°C for the most recent data from North America and Europe to ~1.2°C for the oldest data and that from South Asia, Africa, and Latin America. Data validation is considered in depth in the Supplemental Materials.</p><p id="p-24">We use a MATLAB implementation (<a id="xref-ref-37-1" href="#ref-37"><em>37</em></a>) of the formula of (<a id="xref-ref-38-1" href="#ref-38"><em>38</em></a>) for computing TW. We compute TW daily maxima irrespective of stations’ temporal resolutions, which vary from 1 to 6 hours. TW values are for 2 m above ground level, with station surface pressure calculated from its elevation using a standard atmosphere and an assumed sea-level pressure of 1013 mb. A sensitivity analysis reveals the error in TW owing to this assumption to be on the order of 0.1°C.</p><p id="p-25">We additionally eliminate HadISD station data that fail any one of the following meteorological and climatological tests. Tests are listed in the order implemented, with the fraction of HadISD 31+°C readings removed at each successive step shown in parentheses:</p><p id="p-26">1. A TW extreme occurs in conjunction with a dew point depression of ≤0.5°C (65/10,492).</p><p id="p-27">2. The Td associated with a TW extreme is more than 10°C different from the elevation-adjusted value at the closest grid cell and time step in the ERA-Interim reanalysis (289/10,427).</p><p id="p-28">3. A TW extreme occurring in 1979–1993 is greater than the maximum in 2003–2017 (67/10,138).</p><p id="p-29">4. A TW extreme is followed at any point by at least 1000 consecutive days of missing Td data (365/10,071).</p><p id="p-30">5. A TW extreme occurs on a day when the daily maximum and daily minimum T or Td are identical (53/9706).</p><p id="p-31">6. A TW extreme is more than 7.5°C higher than any other TW value co-occurring in a 7.5° × 7.5° box centered on the station (405/9653).</p><p id="p-32">7. A TW extreme is associated with a Td change of more than 8°C in 1 hour or 12°C in 3 hours (77/9248).</p><p id="p-33">8. A TW extreme is associated with a Td greater than the previously reported, although unofficial, global maximum value of 35°C recorded at Dhahran, Saudi Arabia, on 8 July 2003 (18/9171).</p><p id="p-34">9. A TW extreme occurs during a period with two or more consecutive identical daily maximum TW and Td values (289/9153).</p><p id="p-35">10. A TW extreme before 2001 is higher than any value recorded since 2001 (270/8864).</p><p id="p-36">11. The top five TW extremes at a station all occur within a 365-day period (60/8594).</p><p id="p-37">12. The Td associated with a TW extreme is higher than the 99.5th percentile of the first 5000 days, only at stations where this value is more than 1°C larger than the 99.9th percentile of the last 5000 days (55/8534).</p><p id="p-38">13. The Td associated with a TW extreme is higher than the 99.5th percentile of the last 5000 days, only at stations where this value is more than 6°C larger than the 99.9th percentile of the first 5000 days (362/8479).</p><p id="p-39">14. A TW extreme is associated with a relative humidity of ≥95% (29/8117).</p><p id="p-40">15. A TW extreme occurs on a day when the daily maximum TW takes place before 11:00 a.m. or after 8:00 p.m. local standard time (26/8088).</p><p id="p-41">16. A TW extreme is the all-time maximum at a station and is more than 2°C higher than the next largest value (6/8062).</p><p id="p-42">17. A remaining ≥33°C TW extreme is manually ascertained to be associated with a significant changepoint or not fully supported by gridded humidity and temperature data (508/8056).</p><p id="p-43">Remaining TW = 35°C readings are also closely examined on a subdaily basis so as to ensure validity to the extent possible. We deem valid all other values that pass the above additional quality control measures, beyond the original quality control and homogenization (<a id="xref-ref-17-3" href="#ref-17"><em>17</em></a>, <a id="xref-ref-18-3" href="#ref-18"><em>18</em></a>). Summaries of the TW = 33°C and 35°C values in the final dataset are given in tables S1 and S2.</p><p id="p-44">Interannual trends are calculated using an ordinary least squares regression, with significance evaluated using a <em>t</em> test on the slope coefficient. Our assessment of extreme TW frequency considers threshold exceedances in 2°C increments from 35° to 27°C, so as to strike a balance between values that are sufficiently distinct from one another while being high enough to remain relevant from an impact perspective.</p></div><div id="sec-6"><h3>Marine observations</h3><p id="p-45">We use monthly SSTs from the 1° HadISST version 1.1 dataset (<a id="xref-ref-20-2" href="#ref-20"><em>20</em></a>) to assess the physical realism of our GEV extrapolations and use in situ point observations of SST and TW from International Comprehensive Ocean-Atmosphere Data Set (ICOADS) (<a id="xref-ref-21-3" href="#ref-21"><em>21</em></a>) as an independent (versus HadISD) check on the extreme TW values reported at nearby land-based weather stations. Details of these comparisons are provided in the Supplementary Materials.</p></div><div id="sec-7"><h3>Marine and vertical profile data</h3><p id="p-46">The ICOADS integrated dataset (<a id="xref-ref-21-4" href="#ref-21"><em>21</em></a>) is used as validation of near-surface conditions over water. Radiosondes are from the Integrated Global Radiosonde Archive (<a id="xref-ref-22-2" href="#ref-22"><em>22</em></a>, <a id="xref-ref-39-1" href="#ref-39"><em>39</em></a>).</p></div><div id="sec-8"><h3>GEV modeling of TW extremes in reanalysis data</h3><p id="p-47">We fit a GEV distribution to the time series of annual maximum TW from selected grid cells in ERA-Interim, a reanalysis dataset that optimally blends observations with a numerical hindcast and, thus, provides an estimate of the atmospheric state less sensitive to observation error and microclimatic variability (<a id="xref-ref-19-2" href="#ref-19"><em>19</em></a>). While well suited to identifying and extrapolating global trends, it is inevitable in such an approach that decadal temperature trends and other large-scale variability may affect our results modestly.</p><p id="p-48">The cumulative distribution function of the GEV is given by<span id="disp-formula-1"><span><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:semantics definitionurl="" encoding=""><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:msup><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">к</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">ζ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">β</mml:mi></mml:mfrac><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">к</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:msup></mml:mrow><mml:annotation encoding=""></mml:annotation></mml:semantics></mml:math></span><span>(1)</span></span></p><p id="p-49">The TW quantile for an <em>n</em>-year return period can be evaluated by inverting <a id="xref-disp-formula-1-1" href="#disp-formula-1">Eq. 1</a><span id="disp-formula-2"><span><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:semantics definitionurl="" encoding=""><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">ζ</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi mathvariant="normal">β</mml:mi><mml:mi mathvariant="normal">к</mml:mi></mml:mfrac><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mtext>ln</mml:mtext><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">к</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:annotation encoding=""></mml:annotation></mml:semantics></mml:math></span><span>(2)</span></span>where the location, scale, and shape parameters are denoted ζ, β, and к, respectively. Note that, in our analysis, we use <em>n</em> = 30 (and hence <em>P</em> = 0.967), although we expect different choices of <em>n</em> would not qualitatively affect the results. We estimate these parameters using the method of maximum likelihood, only fitting distributions to series from grid cells whose maximum value over 1979 to 2017 was in the highest 0.1% worldwide (top 119 grid cells), corresponding to a TW threshold of 30.6°C.</p><p id="p-50">We incorporate the effect of global warming on the return period by parameterizing ζ as a function of the annual global mean air temperature anomaly<span id="disp-formula-3"><span><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:semantics definitionurl="" encoding=""><mml:mrow><mml:mi mathvariant="normal">ζ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">α</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">〈</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">〉</mml:mo></mml:mrow><mml:annotation encoding=""></mml:annotation></mml:semantics></mml:math></span><span>(3)</span></span>where α<sub>2</sub> and α<sub>3</sub> are the intercept and slope coefficients of a linear regression.</p><p id="p-51">The extent of improvement in this nonstationary model for each grid cell is evaluated using a likelihood ratio test, with test statistic lambda<span id="disp-formula-4"><span><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:semantics definitionurl="" encoding=""><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">[</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:annotation encoding=""></mml:annotation></mml:semantics></mml:math></span><span>(4)</span></span>where <em>L</em> is the log-likelihood of the nonstationary (subscript <em>A</em>) and stationary (subscript 0) models. Under the null hypothesis (that the nonstationary model is not superior), lambda has a chi-squared distribution with one degree of freedom. Of the 119 grid cells fitted with a GEV distribution, for ~83% of them (99 grid cells), parameterizing zeta as a function of 〈<em>T</em>〉 results in a statistically significant improvement at the <em>P</em> = 0.05 level.</p><p id="p-52">We use the nonstationary model to infer the amount of global warming required for annual maximum TW = 35°C to be at most a 1-in-30-year event. This is calculated by substituting <a id="xref-disp-formula-3-1" href="#disp-formula-3">Eq. 3</a> into <a id="xref-disp-formula-2-1" href="#disp-formula-2">Eq. 2</a> and solving for 〈<em>T</em>〉<span id="disp-formula-5"><span><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:semantics definitionurl="" encoding=""><mml:mrow><mml:mo stretchy="false">〈</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">〉</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi mathvariant="normal">β</mml:mi><mml:mi mathvariant="normal">к</mml:mi></mml:mfrac><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mtext>ln</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>к</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo><mml:mo>+</mml:mo><mml:mn>35</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi mathvariant="normal">α</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mfrac></mml:mrow><mml:annotation encoding=""></mml:annotation></mml:semantics></mml:math></span><span>(5)</span></span></p><p id="p-53">Applying <a id="xref-disp-formula-5-1" href="#disp-formula-5">Eq. 5</a> to the 99 grid cells with nonstationary models enables spatially explicit assessments of the amount of global warming required until TW = 35°C should be expected, on average, once per 30-year period at each cell. Here, we have used the HadCRUT4 dataset (version 4.6.0.0) to characterize observed warming (<a id="xref-ref-40-2" href="#ref-40"><em>40</em></a>).</p></div><div id="sec-9"><h3>Temperature of TW = 35°C emergence and its uncertainty estimation</h3><p id="p-54">The spatially resolved estimates of 〈<em>T</em>〉 from <a id="xref-disp-formula-5-2" href="#disp-formula-5">Eq. 5</a> provide the means for identifying the ToE, which we define as the lowest of the 99 values of 〈<em>T</em>〉 returned by <a id="xref-disp-formula-5-3" href="#disp-formula-5">Eq. 5</a> and which we highlight with vertical dotted lines in <a id="xref-fig-4-3" href="#F4">Fig. 4</a>. Uncertainty in the ToE is assessed with a 10,000-member bootstrap simulation. We randomly select with replacement 30 years of TW and SST data from within the period 1979–2017, fitting parameters (slope, intercept, shape, and scale for <a id="xref-disp-formula-5-4" href="#disp-formula-5">Eq. 5</a>) for each subset. For each bootstrap iteration, we repeat the calculation of the ToE. These 10,000 estimates are then sorted to identify the 5th, 50th, and 95th percentiles; the most likely estimate; and the 90% confidence intervals.</p></div></div><p id="p-1">This is an open-access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by-nc/4.0/" rel="license">Creative Commons Attribution-NonCommercial license</a>, which permits use, distribution, and reproduction in any medium, so long as the resultant use is <strong>not</strong> for commercial advantage and provided the original work is properly cited.</p><div id="ref-list-1"><h2>REFERENCES AND NOTES</h2><ol><li><a href="#xref-ref-1-1" title="View reference 1 in text" id="ref-1">↵</a></li><li><a href="#xref-ref-2-1" title="View reference 2 in text" id="ref-2">↵</a></li><li><a href="#xref-ref-3-1" title="View reference 3 in text" id="ref-3">↵</a></li><li><a href="#xref-ref-4-1" title="View reference 4 in text" id="ref-4">↵</a></li><li><a href="#xref-ref-5-1" title="View reference 5 in text" id="ref-5">↵</a></li><li><a href="#xref-ref-6-1" title="View reference 6 in text" id="ref-6">↵</a></li><li><a href="#xref-ref-7-1" title="View reference 7 in text" id="ref-7">↵</a></li><li><a href="#xref-ref-8-1" title="View reference 8 in text" id="ref-8">↵</a><div id="cit-6.19.eaaw1838.8"><p>M. N. Sawka, C. B. Wenger, S. J. Montain, M. A. Kolka, B. Bettencourt, S. Flinn, J. Gardner, W. T. Matthew, M. Lovell, C. Scott, <em>Heat Stress Control and Heat Casualty Management</em> (US Army Research Institute of Environmental Medicine Technical Bulletin 507, 2003).</p></div></li><li><a href="#xref-ref-9-1" title="View reference 9 in text" id="ref-9">↵</a></li><li><a href="#xref-ref-10-1" title="View reference 10 in text" id="ref-10">↵</a></li><li><a href="#xref-ref-11-1" title="View reference 11 in text" id="ref-11">↵</a></li><li><a href="#xref-ref-12-1" title="View reference 12 in text" id="ref-12">↵</a></li><li><a href="#xref-ref-13-1" title="View reference 13 in text" id="ref-13">↵</a></li><li><a href="#xref-ref-14-1" title="View reference 14 in text" id="ref-14">↵</a></li><li><a href="#xref-ref-15-1" title="View reference 15 in text" id="ref-15">↵</a></li><li><a href="#xref-ref-16-1" title="View reference 16 in text" id="ref-16">↵</a></li><li><a href="#xref-ref-17-1" title="View reference 17 in text" id="ref-17">↵</a></li><li><a href="#xref-ref-18-1" title="View reference 18 in text" id="ref-18">↵</a></li><li><a href="#xref-ref-19-1" title="View reference 19 in text" id="ref-19">↵</a></li><li><a href="#xref-ref-20-1" title="View reference 20 in text" id="ref-20">↵</a></li><li><a href="#xref-ref-21-1" title="View reference 21 in text" id="ref-21">↵</a></li><li><a href="#xref-ref-22-1" title="View reference 22 in text" id="ref-22">↵</a><div id="cit-6.19.eaaw1838.22"><p>I. Durre, Y. Xungang, R. S. Vose, S. Applequist, J. Arnfield, <em>Integrated Global Radiosonde Archive (IGRA) Version 2</em>. (NOAA National Centers for Environmental Information, 2016).</p></div></li><li><a href="#xref-ref-23-1" title="View reference 23 in text" id="ref-23">↵</a></li><li><a href="#xref-ref-24-1" title="View reference 24 in text" id="ref-24">↵</a></li><li><a href="#xref-ref-25-1" title="View reference 25 in text" id="ref-25">↵</a></li><li><a href="#xref-ref-26-1" title="View reference 26 in text" id="ref-26">↵</a></li><li><a href="#xref-ref-27-1" title="View reference 27 in text" id="ref-27">↵</a></li><li><a href="#xref-ref-28-1" title="View reference 28 in text" id="ref-28">↵</a></li><li><a href="#xref-ref-29-1" title="View reference 29 in text" id="ref-29">↵</a></li><li><a href="#xref-ref-30-1" title="View reference 30 in text" id="ref-30">↵</a></li><li><a href="#xref-ref-31-1" title="View reference 31 in text" id="ref-31">↵</a></li><li><a href="#xref-ref-32-1" title="View reference 32 in text" id="ref-32">↵</a></li><li><a href="#xref-ref-33-1" title="View reference 33 in text" id="ref-33">↵</a></li><li><a href="#xref-ref-34-1" title="View reference 34 in text" id="ref-34">↵</a></li><li><a href="#xref-ref-35-1" title="View reference 35 in text" id="ref-35">↵</a><div id="cit-6.19.eaaw1838.35"><p>G. J. Van Oldenborgh, M. Collins, J. Arblaster, J. H. Christensen, J. Marotzke, S. B. Power, M. Rummukainen, T. Zhou, Annex I: Atlas of global and regional climate projections. in <em>Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change</em>, T. F. Stocker, D. Qin, G.-k. Plattner, M. Tignor, S. K. Allen, Eds. (Cambridge Univ. Press, Cambridge, U.K, 2013).</p></div></li><li><a href="#xref-ref-36-1" title="View reference 36 in text" id="ref-36">↵</a></li><li><a href="#xref-ref-37-1" title="View reference 37 in text" id="ref-37">↵</a></li><li><a href="#xref-ref-38-1" title="View reference 38 in text" id="ref-38">↵</a></li><li><a href="#xref-ref-39-1" title="View reference 39 in text" id="ref-39">↵</a></li><li><a href="#xref-ref-40-1" title="View reference 40 in text" id="ref-40">↵</a></li><li><div id="cit-6.19.eaaw1838.41"><p>Copernicus Climate Change Service (C3S) (2017): ERA5: Fifth generation of ECMWF atmospheric reanalyses of the global climate. Copernicus Climate Change Service Climate Data Store (CDS), <a href="https://cds.climate.copernicus.eu/cdsapp#!/home">https://cds.climate.copernicus.eu/cdsapp#!/home</a> [accessed 10 November 2019].</p></div></li><li></li><li></li><li></li></ol></div><p><strong>Acknowledgments: </strong>Code for computing TW using the Davies-Jones formulae was provided by R. Kopp at Rutgers University. Part of this work was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration. <strong>Funding:</strong> Funding for R.M.H. and C.R. was provided by the National Oceanic and Atmospheric Administration’s Regional Integrated Sciences and Assessments program, grant NA15OAR4310147. <strong>Author contributions:</strong> C.R. and T.M. produced the datasets and conducted the analyses. C.R., T.M., and R.M.H. collectively developed ideas and wrote the manuscript. <strong>Competing interests:</strong> The authors declare that they have no competing interests. <strong>Data and materials availability:</strong> Datasets are described in the Supplementary Materials. Data and code used in the analysis are publicly available in a Github repository at <a href="https://github.com/cr2630git/humidheat">https://github.com/cr2630git/humidheat</a>. All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials. Additional data related to this paper may be requested from the authors.</p><ul><li id="copyright-statement-1">Copyright © 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).</li></ul></div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 01:17:10 +0000</pubDate>
      <source>https://advances.sciencemag.org/content/6/19/eaaw1838</source>
    </item>
    <item>
      <title>Default disappearing messages</title>
      <link>https://signal.org/blog/disappearing-by-default/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___signal_org_blog_disappearing-by-default_/image.jpg" /> 
<div id="readability-page-1" class="page"><div><div><p></p><p>As the norms for how people connect have changed, much of the communication that once took place through the medium of coffee shops, bars, and parks now takes place through the medium of digital devices. One side effect of this shift from <em>analog</em> to <em>digital</em> is the conjoined shift from the <em>ephemeral</em> to the <em>eternal:</em> words once transiently spoken are now – more often than not – data stored forever.</p><p>We’ve designed Signal so that your data always stays in your hands. We think there’s something special about sharing a private fleeting moment between friends, so Signal also supports disappearing messages. Now, we’ve added the ability to preconfigure all conversations you initiate with a default disappearing messages timer.</p><h2 id="a-primer-on-the-timer">A primer on the timer</h2><p><a href="https://signal.org/blog/disappearing-messages/">Disappearing messages</a> provide a way to keep your message history tidy. When enabled for a conversation, messages will be deleted for the sender and recipients after the specified time. This is not for situations where your contact is your adversary — after all, if someone who receives a disappearing message really wants a record of it, they can always use another camera to take a photo of the screen before the message disappears. However, this is a nice way to automatically save storage space on your devices and limit the amount of conversation history that remains on your device if you should find yourself physically separated from it.</p><p>You can even enable disappearing message for <em>Note To Self</em> to create an ephemeral cross-device personal clipboard of sorts.</p><h2 id="disappearing-by-default-and-customized-timers">Disappearing by default and customized timers</h2><p>Until now, disappearing messages had to be enabled on a per-conversation basis, but for those who want to take ephemerality to the fullest, Signal now supports the ability to preconfigure all conversations you initiate with a default timer.</p><p>We’ve also added the ability to set custom timer durations on your conversations, so that some content can be <em>gone in 60 seconds</em> and others can exist for 18 minutes or 4 weeks. <a href="https://signal.org/install/">Install Signal</a>, and give it a shot today!</p><p></p></div></div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 05:00:28 +0000</pubDate>
      <source>https://signal.org/blog/disappearing-by-default/</source>
    </item>
    <item>
      <title>State of netbooting Raspberry Pi in 2021</title>
      <link>https://blog.alexellis.io/state-of-netbooting-raspberry-pi-in-2021/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___blog_alexellis_io_state-of-netbooting-raspberry-pi-in-2021_/image.jpg" /> 
<div id="readability-page-1" class="page"><div role="main">
   <article>

        

        <section>
            <div><blockquote>
<p>TLDR; When I tried netbooting 24 RPi3s in 2017, it failed miserably. Find out what changed when I built my private cloud with the newer Raspberry Pi 4s and K3s.</p>
</blockquote>
<p>Just over three years ago, <a href="https://blog.alexellis.io/the-state-of-netbooting-raspberry-pi/">I wrote up a blog post exploring why netbooting was interesting for Raspberry Pi</a>, and what kind of problems I ran into due to issues with the firmware available at the time.</p>
<p><a href="https://blog.alexellis.io/the-state-of-netbooting-raspberry-pi/">My original rig</a> had 24 early-generation Raspberry Pi 3s, which had a timing bug rendering them unreliable for netbooting.</p>
<p>In this post, I&#39;ll give you an update (things are much better now) and I&#39;ll also tell you about my new rig which I&#39;m running K3s on.</p>
<p></p>
<blockquote>
<p>My private cloud with 40GB of RAM for running K3s</p>
</blockquote>
<p>You&#39;ll also find a link to <a href="https://gumroad.com/l/netbooting-raspberrypi">my new workshop and eBook</a> which has a Bill of Materials and step-by-step instructions for netbooting your own Raspberry Pis.</p>
<h2 id="whydidiwanttodoitin2017">Why did I want to do it in 2017?</h2>
<p>My original hopes were that I could save on the costs and management of 24 SD cards. These are notoriously unreliable due to their limited write cycles. When used in a Raspberry Pi cluster running <a href="https://kubernetes.io">Kubernetes</a>, the cards would be written to constantly meaning the cards will eventually fail.</p>
<p></p>
<blockquote>
<p>Given the bug with netbooting the variant of the RPi3 that I had, this was the only way to get it working</p>
</blockquote>
<p>Even with automation, it&#39;s a time-consuming and manual process to flash, label and update 24 SD cards. I did actually go through this process and got all of the nodes functioning.</p>
<p></p>
<blockquote>
<p>My original rig refreshed with K3s</p>
</blockquote>
<p>I ran Docker Swarm on the cluster originally, before Swarm fell out of fashion, and Kubernetes took over to bring clustering to containers. Swarm was always light-weight and fast on Raspberry Pi, and Kubernetes using kubeadm was always slow and difficult to use.</p>
<p>Fortunately <a href="https://twitter.com/ibuildthecloud">Darren Shepherd</a> came up with a novel idea of taking way the things he didn&#39;t want in Kubernetes and created a &#34;distro&#34; called <a href="https://k3s.io">K3s</a> (3 is less than 8).</p>
<p>Where <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/">kubeadm</a> created a headache for Raspberry Pi, K3s made things simple with an easy boostrap, low resource requirements and a single binary for distribution and updates.</p>
<p>Given how much I&#39;d liked the Docker experience of &#34;docker swarm init&#34; and &#34;docker swarm join&#34;, I decided to re-create a similar experience for K3s in the <a href="https://k3sup.dev">k3sup</a> tool.</p>
<p><a href="https://k3sup.dev">k3sup</a> installs K3s over SSH and also makes creating a multi-server configuration easier through CLI flags.</p>
<p></p>
<blockquote>
<p>Conceptual workflow showing the <code>install</code> and <code>join</code> commands run with a hostname or IP address over SSH.</p>
</blockquote>
<h2 id="whydidirevisitnetbootingin2020">Why did I revisit netbooting in 2020?</h2>
<p>I was helping the team at <a href="https://metal.equinix.com/">Equinix Metal</a> get their Open Source provisioning software <a href="https://tinkerbell.org">Tinkerbell</a> ready for general use. That meant going through examples to check that they worked, building new examples and giving feedback on the developer- and user-experience.</p>
<p>As part of the project I wrote up a feature-length article for <a href="https://thenewstack.io/">The New Stack</a> to link the ideas of Cloud Native computing (a relatively new and trendy area) to bare-metal provisioning (something which has used the same basic tools for 20-25 years).</p>
<p>My post <a href="https://thenewstack.io/bare-metal-in-a-cloud-native-world/">Bare Metal in a Cloud Native World</a> also helped me refresh my own memory on TFTP, DHCP and on NFS.</p>
<p></p>
<blockquote>
<p>My conceptual diagram showing how PXE boot works</p>
</blockquote>
<p>One of the other things I built out for the client was a GitHub repository collecting other Open Source awesome baremetal projects: <a href="https://github.com/alexellis/awesome-baremetal">awesome-baremetal</a></p>
<p>As I was kicking the tires with Tinkerbell and testing out netbooting with a few Intel NUCs, my interest in the Raspberry Pi got stirred again. With the popularity of <a href="https://k3s.io">K3s</a> and my <a href="https://k3sup.dev/">K3s bootstrapping tool k3sup (&#39;k3sup&#39;)</a>, it seemed like a good time to revisit the problems I&#39;d had in 2017.</p>
<h2 id="mybuildin2021">My build in 2021</h2>
<p>In 2021, I wanted to take what I&#39;d learned during the previous year and bundle it up into a workshop that anyone could follow to access netbooting. It took me about two weeks to fully understand, document, automate and test the setup.</p>
<p>Whilst you can spend a lot of money here, I was fortunate to have an old PC I could use as a server, and the kit that BitScope had sent me. Note: the 2GB Raspberry Pi only costs 35 USD now, and you only really need one to try this out.</p>
<p>For the initial configuration I used an Intel NUC as the netbooting server with a four Raspberry Pi 4s with 2GB of RAM each. That would be enough to run Kubernetes and a few workloads like <a href="https://openfaas.com/">OpenFaaS</a>.</p>
<p>I used the official 32-bit Raspberry Pi Operating System which supports PXE booting. Ubuntu 20.04 (Focal Fossa), a 64-bit Operating System also works with some modifications and is needed if you want to run a project that is only made available for 64-bit Operating Systems. The Raspberry Pi is capable of running both, but the 32-bit OS is better supported and understood by the community.</p>
<p>After assembling the hardware, the process involves flashing a new firmware to each node, setting up a boot order, configuring a server with TFTP, NFS, a DHCP server and IP forwarding to act as a router.</p>
<p></p>
<blockquote>
<p>I flashed each of the Raspberry Pis using the same SD card, then I could close the case and take a note of each MAC address and serial number for later on.</p>
</blockquote>
<p>This is the network topology, where everything runs in its own subnet, on a separate LAN and switch.</p>
<p></p>
<blockquote>
<p>The network topology from my workshop</p>
</blockquote>
<p>Once you have everything configured, it&#39;s then a case of picking a case or enclosure to host your nodes.</p>
<p><a href="https://bitscope.com">BitScope</a> had sent me a newer version of their product called the <a href="https://bitscope.com/blog/JK/?p=JK38B">Cluster Blade</a> - this time just for 8 nodes. It was aimed at edge computing and could be rack-mounted. Do not confuse this with the previous generation product called &#34;The BitScope Blade&#34;, which is designed for Raspberry Pi 3.</p>
<p>They also kindly sent over an additional 4x nodes for me so that I could fully populate the cluster with 8 nodes.</p>
<p></p>
<p>This gave me a total of 40GB of RAM and 32 CPU cores. My netbooting server also had a 500GB NVMe installed and shared for NFS, so there was plenty of space available too.</p>
<p>With each of the nodes running, I observed around 30-31 Watts of power being drawn through the power supply. The 24-node RPi3 cluster drew ~ 21 Watts, so considering the increase in I/O speed and RAM per node, this seems reasonable.</p>
<h3 id="aprivatecloudforproduction">A private cloud for production</h3>
<p>The use-case that BitScope see for these racks is where customers want to move off AWS EC2 and onto their own private cloud, which they can run on-premises or at the edge at a customer site.</p>
<p>If an application already runs in a container, then it shouldn&#39;t take too much work to port it over to K3s. One of the tricky parts is in the ecosystem, and making sure that everything that needs to run is available for an Arm CPU.</p>
<p><strong>App ecosystem</strong></p>
<p>Things are much better than they were in 2017, with the following key components available with Arm support: cert-manager, ingress-nginx, Postgresql, OpenFaaS, NATS, Prometheus, Docker registry, Grafana, Minio, Portainer, .NET Core, Node.js, Python 3, Go and many others. If you&#39;re a Kubernetes or K3s user, then you may also find <a href="https://github.com/alexellis/arkade">the arkade tool</a> useful for discovering and installing apps with a single command line.</p>
<p><strong>Storage</strong></p>
<p>People often ask me about storage. Many modern microservices are built to be stateless, and not to need to read or write from a disk, but you may need underlying object storage or a SQL database. That is where I tend to need persistent storage when working with OpenFaaS functions.</p>
<p>Fortunately, K3s comes with a storage driver that can use the local disk of the node and is called <a href="https://github.com/rancher/local-path-provisioner">Local path provisioner</a>. There are other options available such as <a href="https://openebs.io/">OpenEBS</a> and <a href="https://github.com/longhorn/longhorn">Longhorn</a>, but since we already have NFS in place, we may as well use the <a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">NFS volume provisioner</a>.</p>
<p><strong>Ingress and remote access</strong></p>
<p>Some people are content with having no Ingress to their clusters at all, but I&#39;m not one of those people. If I need to expose a Kubernetes LoadBalancer, then I want it to have a public and accessible IP address. For that reason I built the <a href="https://github.com/inlets/inlets-operator">inlets-operator</a>, a Kubernetes controller that can provision a VM and then create a network tunnel to enable TCP or HTTPS traffic to enter the cluster from behind NAT, firewalls and captive portals.</p>
<p></p>
<blockquote>
<p>Credit goes to <a href="https://iximiuz.com/en/posts/kubernetes-operator-pattern">Ivan Velichko</a> for the animation.</p>
</blockquote>
<p><strong>GitOps</strong></p>
<p>Loosely defined, GitOps (coined by <a href="https://twitter.com/monadic">Alexis Richardson</a>) is storing deployment configuration in a Git repository for continuous deployment and is very much in vogue in 2021.</p>
<p>Johan Siebens wrote up a blog post showing how ArgoCD can be used to bring GitOps to one or more Raspberry Pi clusters, or a fleet of clusters. <a href="https://johansiebens.dev/posts/2020/08/argo-cd-for-your-private-raspberry-pi-k3s-cluster/">ArgoCD for your private Raspberry Pi k3s cluster</a></p>
<p><a href="https://fleet.rancher.io/multi-cluster-install/">Rancher&#39;s Fleet</a> project (another one of of Darren&#39;s inventions) is designed to manage <a href="https://rancher.com/blog/2020/scaling-fleet-kubernetes-million-clusters">fleets of up to 1 Million clusters</a>.</p>
<h2 id="wrappingup">Wrapping up</h2>
<p>Whatever I said about netbooting in 2017 is still large correct apart from one thing. It is now much more reliable and easier than ever to get started with, whether you have one or 100 Raspberry Pis.</p>
<p>So however many Raspberry Pis you have, booting over the network means you can quickly provision and manage your nodes, even if they are just running PiHole, Plex, or a Ghost blog. Removing the SD cards makes the filesystem faster, and more reliable.</p>
<p>It should be noted that a local NVMe over USB3 is faster, by a large margin, however it is also much more costly to scale 8x NVMe drives plus their caddies, vs. one NVMe in an old PC or server that you may already own.</p>
<p>Let me leave you with some resources and further links so that you can keep on learning.</p>
<h3 id="takeitfurther">Take it further</h3>
<p>Is there something I didn&#39;t cover? It may be included in my workshop, or you can <a href="https://twitter.com/alexellisuk/status/1378998948992589830?s=20">reach out to me over Twitter</a> if you already have a copy.</p>
<p>If you&#39;d like to build your own private cloud, or just learn more about networking, you can pick up <a href="https://gumroad.com/l/netbooting-raspberrypi">my eBook or video workshop package</a> with 30 USD off until 9th April.</p>
<p><a href="https://gumroad.com/l/netbooting-raspberrypi"></a></p>
<ul>
<li><a href="https://gumroad.com/l/netbooting-raspberrypi">Check out my eBook &amp; video workshop</a></li>
</ul>
<p>Would you like to learn more of the journey over the past 5 years of Docker and Kubernetes on Raspberry Pi? Are you curious about the costs and the real-life use-cases? <a href="https://www.youtube.com/watch?v=jfUpF40--60">The Past, Present, and Future of Kubernetes on Raspberry Pi</a></p>
<p>Would you like to try booting your Raspberry Pi from a local disk? <a href="https://alexellisuk.medium.com/upgrade-your-raspberry-pi-4-with-a-nvme-boot-drive-d9ab4e8aa3c2">Upgrade your Raspberry Pi 4 with a NVMe boot drive</a></p>
<p>Want to try K3s? You can do so with an SD card in around 15 minutes with my <a href="https://alexellisuk.medium.com/walk-through-install-kubernetes-to-your-raspberry-pi-in-15-minutes-84a8492dc95a">K3s walk-through</a></p>
</div>
        </section>

        

    </article>
</div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 07:32:30 +0000</pubDate>
      <source>https://blog.alexellis.io/state-of-netbooting-raspberry-pi-in-2021/</source>
    </item>
    <item>
      <title>Practical introduction to algebraic datatypes (ADTs) in TypeScript</title>
      <link>https://medium.com/@el3ng/practical-introduction-to-algebraic-datatypes-adts-in-typescript-1cb6952e4c6d</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___medium_com_@el3ng_practical-introduction-to-algebraic-datatypes-adts-in-typescript-1cb6952e4c6d/image.jpg" /> 
<div id="readability-page-1" class="page"><section><div><div><p id="9ffc">Product types are essentially containers for other types, a <em>thing</em> that has <em>fields. </em><strong>Tuple</strong> and <strong>Record</strong> are the most common product types. They are both types that contain <em>fields</em> that contain some other types. All values of a specific product type contain the same set of fields.</p><p id="807d">TypeScript supports <strong>tuples</strong> that one might also think of as fixed length, typed arrays. They are also type safe so you get compile errors if accessing the third item of a 2-tuple or trying to set a string to a number field etc.</p><figure><p></p><figcaption>A “2-tuple of [number, number]” has two fields that are always both numbers.</figcaption></figure><p id="78cc">Records in TS is a bit of a stretch perhaps but you can certainly make structures that behave very close to a record. The rough idea is that a tuple has it’s fields numbered and a record has named fields. So you can just use an object literal, and as with tuples, the type system has got your back:</p><figure><div><div><div><p></p><p></p></div></div></div><figcaption>Record { a: number, b: string } has two fields a and b that are always a number and a string. Everything else is a compile error.</figcaption></figure><p id="a3e5">So why are these called Product types? This takes us to type theory, category theory, mathematics: “<em>the set of all possible values of a product type is the set-theoretic product, i.e., the </em><a href="https://en.wikipedia.org/wiki/Cartesian_product" rel="noopener"><em>Cartesian product</em></a><em>, of the sets of all possible values of its field types”. </em>Maybe we can stick with tuples and records for now.</p><p id="e356">TypeScript kind of does not really have Sum types although the <a href="https://www.typescriptlang.org/docs/handbook/unions-and-intersections.html" rel="noopener">union type</a> comes very close. If a Product type was a container of fields, then a Sum type is like a container of possibilities. A <em>thing </em>that is either <em>this</em> or <em>that</em> but not both. In TypeScript a union type that has type string or number ensures that any value of that type is either a string or a number:</p><pre><span id="c840">type Thing = string | number // string or number</span></pre><p id="275a">To see a bit more powerful version of the Sum type, we need to change our language. Let’s look at a code sample from ReasonML (or ReScript nowadays) in the context of our original problem:</p><figure><div role="button" tabindex="0"><div><div><div><p></p><p></p></div></div></div></div><figcaption>“response” is a Sum type that is either Loading, Error or Data. “api” is of type response (inferred automatically).</figcaption></figure><p id="1e51">There are several new concepts here so bear with me. The “response” type is our Sum type, known as “variant type” in ReasonML/ReScript. It’s similar to the TS union type with the difference that we need to package the possible types into type constructors like “Loading” or “Error”.</p><figure><div role="button" tabindex="0"><div><div><div><p></p><p></p></div></div></div></div><figcaption>Type inference can calculate the function’s signature.</figcaption></figure><p id="3d73">The “render” function takes an argument called “api” that is actually of type response. This is inferred automatically by the type system.</p><p id="48d8">The cool thing is that we can use <a href="https://en.wikipedia.org/wiki/Pattern_matching" rel="noopener">pattern matching</a> to investigate the contents of the response by using the switch statement. If the response is Loading then it will match the first branch and the output will be “Still loading”, if it’s Error then we can print “Error! {s}“. We know the s is a string because response type has Error(string). The type system can also figure out if we are not handling all the possibilities that our Sum type has:</p><figure><div><div><div><p></p><p></p></div></div></div><figcaption>Oops! You didn’t check if foo doesn’t have a value!</figcaption></figure><p id="0386">So why is this called a Sum type? Again, a bit theoretical, but here we go: <em>“The set of all possible values of a sum type is the set-theoretic sum, i.e., the </em><a href="https://en.wikipedia.org/wiki/Disjoint_union" rel="noopener"><em>disjoint union</em></a><em>, of the sets of all possible values of its variants.”. </em>So instead of the Cartesian product of it’s fields like in Product type, this time it’s a sum of the possibilities. If this doesn’t make any sense, don’t worry. For the purposes if this article, understanding the theoretical aspects is not necessary.</p><p id="30af">So no we have learned the basic ADTs and it’s time to apply them to our original problem and see if we can get rid of that nasty if-else mess.</p><figure><div role="button" tabindex="0"><div><div><div><p></p><p></p></div></div></div></div><figcaption>Much cleaner and safer than the original.</figcaption></figure><p id="78ee">“APIResponse” is a union type of different records. While not a completely proper ADT, it simulates a Sum type well enough for our use case.</p><p id="996e">“render” uses a regular switch case to simulate pattern matching a Sum type. We use the special field “t” to provide a <a href="https://en.wikipedia.org/wiki/Tagged_union" rel="noopener">tagged union</a>.</p><p id="cd92">TypeScript is actually smart enough here to figure out several things at compile time:</p><ul><li id="b57a"><strong>The switch-case has to handle all of the tags</strong>, <em>if you add a new tag then the compiler will tell you</em>.</li><li id="e127"><strong>Branches in the switch-case are able to identify which record should be used. </strong><em>For example, “.foo” is only available in the case ‘Data’ branch while “.error” is only available in the case ‘Error’ branch.</em></li><li id="991e"><strong>Render must return a string so all return statements are checked normally. </strong><em>As is expected.</em></li></ul><p id="bd99">We have now introduced the ADTs: Product and Sum type via a practical example. With a couple basic building blocks we were able to refactor a complex if-else scenario into a beautiful pattern matching over a tagged union (a certain type of Sum type, if you will) in TypeScript. The outcome is clear, safe and tolerates changes like adding/removing cases very well.</p></div></div></section></div>]]></content:encoded>
      <pubDate>Tue, 10 Aug 2021 12:10:19 +0000</pubDate>
      <source>https://medium.com/@el3ng/practical-introduction-to-algebraic-datatypes-adts-in-typescript-1cb6952e4c6d</source>
    </item>
    <item>
      <title>The British Library puts 1M newspaper pages online for free</title>
      <link>https://www.ianvisits.co.uk/blog/2021/08/11/british-library-puts-1-million-newspaper-pages-online-for-free/</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_ianvisits_co_uk_blog_2021_08_11_british-library-puts-1-million-newspaper-pages-online-for-free_/image.jpg" /> 
<div id="readability-page-1" class="page"><div id="post-46394">


<div>

<p>The British Library project to digitise copies of newspapers from its archive is to release one million pages as a free resource.</p>
<p>The <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2F" data-wpel-link="external" rel="nofollow noopener">British Newspaper Archive</a> (BNA) has over 44 million newspaper pages, mostly British and Irish titles, ranging from 1699 to 2009, or just under 10 per cent of all newspapers held by the <a href="https://www.ianvisits.co.uk/blog/tag/british-library/" data-wpel-link="internal">British Library</a>. Currently, around half a million pages are being added to the BNA every month.</p>
<p>At the moment, access to the service requires a subscription, as that helps fund the cost of scanning and recording each newspaper added to the online archive. To open up more of the archive for researchers though, one million pages have been released for <strong>free</strong>, with a commitment to release a million more each year for the next four years.</p>
<p>So that’s five million pages of old newspaper articles in total.</p>
<p><a href="https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage.jpg" data-wpel-link="internal"><picture><source srcset="https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-1024x614.jpg.webp 1024w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-600x360.jpg.webp 600w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-250x150.jpg.webp 250w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-768x461.jpg.webp 768w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-1536x922.jpg.webp 1536w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-100x60.jpg.webp 100w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-150x90.jpg.webp 150w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-200x120.jpg.webp 200w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-300x180.jpg.webp 300w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-450x270.jpg.webp 450w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage-900x540.jpg.webp 900w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Free-Titles-Collage.jpg.webp 1800w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"/></picture></a></p>
<p>They won’t add all papers, as they say that while they consider newspapers made before 1881 to be in the public domain, that does not mean that will make all pre-1881 digitised titles available for free, as the archive is dependent on subscriptions to cover its costs. If like me you do a lot of historical research, then the cost of the full subscription is not that bad – <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Faccount%2Fsubscribe" data-wpel-link="external" rel="nofollow noopener">just £80 a year</a> for the full archive.</p>
<p>For the free service, there are 158 newspaper titles on offer, ranging from 1720 to 1880. The latter date is significant. All of the newspapers that make up the ‘free to view’ offer are out-of-copyright. The British Library keeps to a ‘safe date’ when determining when a newspaper can be considered to be entirely out-of-copyright, which is 140 years after the date of publication.</p>
<p>To make use of the service, go <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2F" data-wpel-link="external" rel="nofollow noopener"><strong>here</strong></a>, and you need to register for an account, and then the search functionality is fairly simple to use, and the advanced search really helps to narrow the results down.</p>
<p>When you do a search, it will return all the pages in the archive, but if you scroll down the sidebar, there’s an option to filter by free pages only.</p>
<p>You can also browse editions and read entire newspapers from the past.</p>
<p><a href="https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1.jpg" data-wpel-link="internal"><picture><source srcset="https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1.jpg.webp 1024w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1-600x312.jpg.webp 600w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1-768x399.jpg.webp 768w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1-100x52.jpg.webp 100w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1-150x78.jpg.webp 150w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1-200x104.jpg.webp 200w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1-300x156.jpg.webp 300w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1-450x234.jpg.webp 450w, https://www.ianvisits.co.uk/blog/wp-content/uploads/2021/08/Blackpool-Gazette-Herald-Friday-23-March-1877-2021-08-06-at-16.34.05-1024x532-1-900x468.jpg.webp 900w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"/></picture></a></p>
<p>Below is a complete listing of all newspaper titles on the initial ‘free to view’ list of one million pages, including changes of title. Start and end dates are for what is being made freely available, not necessarily the complete run of the newspaper. For a few titles, there are some missing issues for the dates given.</p>
<ul>
<li>The Age (1825-1843)</li>
<li>Alston Herald, and East Cumberland Advertiser (1875-1879)</li>
<li>The Argus, or, Broad-sheet of the Empire (1839-1843)</li>
<li>The Atherstone Times (1879-1879), The Atherstone, Nuneaton, and Warwickshire Times (1879-1879)</li>
<li>Baldwin’s London Weekly Journal (1803-1836)</li>
<li>The Barbadian (1822-1861)</li>
<li>Barbados Mercury (1783-1789), Barbados Mercury, and Bridge-town Gazette (1807-1848)</li>
<li>The Barrow Herald and Furness Advertiser (1863-1879)</li>
<li>The Beacon (Edinburgh) (1821-1821)</li>
<li>The Beacon (London) (1822-1822)</li>
<li>The Bee-Hive (1862-1870), The Penny Bee-Hive (1870-1870), The Bee-Hive (1870-1876), Industrial Review, Social and Political (1877-1878)</li>
<li>The Birkenhead News and Wirral General Advertiser (1878-1879)</li>
<li>The Blackpool Herald (1874-1879)</li>
<li>Blandford, Wimborne and Poole Telegram (1874-1879), The Blandford and Wimbourne Telegram (1879-1879)</li>
<li>Bridlington and Quay Gazette (1877-1877)</li>
<li>Bridport, Beaminster, and Lyme Regis Telegram and Dorset, Somerset, and Devon Advertiser (1865, 1877-1879)</li>
<li>Brighouse &amp; Rastrick Gazette (1879-1879)</li>
<li>The Brighton Patriot, and Lewes Free Press (1835-1836), Brighton Patriot and South of England Free Press (1836-1839)</li>
<li>The British Emancipator (1837-1840)</li>
<li>The British Liberator (1833-1833)</li>
<li>The British Luminary; or, Weekly News and General Advertiser (1818-1818), The British Luminary and National Intelligencer (1818-1818), The British Luminary and Weekly Intelligence (1818-1820), The British Luminary, or Weekly Intelligencer (1820-1820), The Weekly Intelligencer, and British Luminary (1820-1821), The British Luminary and Weekly Intelligencer (1821-1823)</li>
<li>British Miner and General Newsman (1862-1863), The Miner (1863-1863), The Miner and Workman’s Advocate (1863-1865), The Workman’s Advocate (1865-1866), The Commonwealth (1866-1867)</li>
<li>The British Press; or, Morning Literary Advertiser (1803-1826)</li>
<li>Caledonian Mercury (1720-1799), Caledonian Mercury (1800-1859), The Caledonian Mercury and Daily Express (1859-1860), The Caledonian Mercury (1860-1867)</li>
<li>The Cannock Chase Examiner (1874-1877)</li>
<li>The Central Glamorgan Gazette, and General, Commercial, and Agricultural Advertiser (1866-1879)</li>
<li>Champion (1836-1836), The Champion and Weekly Herald (1836-1840)</li>
<li>The Charter (1839-1840)</li>
<li>Chartist (1839-1839)</li>
<li>Chartist Circular (1839-1841)</li>
<li>Cleave’s Weekly Police Gazette (1835-1836), Cleave’s Weekly Police Gazette and Journal of News, Politics, and Literature (1836-1836)</li>
<li>Cobbett’s Annual Register (1802-1804), Cobbett’s Weekly Political Register (1804-1836)</li>
<li>Colored News (1855-1855)</li>
<li>Cradley Heath &amp; Stourbridge Observer (1864-1864), The Observer, Cradley Heath, Halesowen &amp; District Chronicle (1864-1866), The Stourbridge Observer, Cradley Heath, Halesowen &amp; District Chronicle (1866-1879)</li>
<li>The Darlington &amp; Stockton Telegraph, Richmond Herald, South Durham and North York Review (1870-1870), Darlington &amp; Richmond Herald (1873-1874)</li>
<li>Denton, Haughton, &amp; District Weekly News, The (1873-1874), Denton &amp; Haughton Weekly News, and Audenshaw, Hooley Hill, and Dukinfield Advertiser (1874-1875), Denton Examiner, Audenshaw, Hooley Hill and Dukinfield Advertiser (1875-1878), Denton and Haughton Examiner (1878-1879)</li>
<li>The Dewsbury Chronicle, and West Riding Advertiser (1872-1875)</li>
<li>The Dorset County Express and Agricultural Gazette (1858-1879)</li>
<li>The Examiner (1808-1880)</li>
<li>The Express (1846-1869)</li>
<li>The Forest of Dean Examiner (1875-1877)</li>
<li>The Glasgow Chronicle (1844-1857)</li>
<li>Glasgow Courier (1802, 1844-1866)</li>
<li>Illustrated Sporting News and Theatrical and Musical Review (Illustrated Sporting News, Theatrical Review (1862-1865), Illustrated Sporting and Theatrical News (1865-1870)</li>
<li>The Imperial Weekly Gazette (1808-1810), The Imperial Weekly Gazette and Westminster Journal (1818-1823), Imperial Gazette (1823-1825)</li>
<li>The Jamaica Mercury and Kingston Weekly Advertiser (1779-1780), The Royal Gazette (1780-1836), The Royal Gazette and Jamaica Times (1838-1840), The Royal Gazette of Jamaica (1840-1840)</li>
<li>Jewish Record (1868-1871)</li>
<li>The Kenilworth Advertiser (1872-1879)</li>
<li>The Lady’s Newspaper and Pictorial Times (1847-1863)</li>
<li>The Lady’s Own Paper (1866-1872)</li>
<li>The Lancaster Herald, and Town and County Advertiser (1831-1832)</li>
<li>The Liverpool Standard and General Commercial Advertiser (1832-1854), Liverpool Standard and General Advertiser (1854-1855), Liverpool Standard and General Commercial Advertiser (1855-1856)</li>
<li>Liverpool Weekly Courier (1867-1879)</li>
<li>Lloyd’s Companion to the “Penny Sunday Times and Peoples’ Police Gazette” (1841-1847)</li>
<li>London Dispatch and People’s Political and Social Reformer (1836-1839)</li>
<li>The Manchester Examiner (1848-1848)</li>
<li>Manchester Times (1828-1829), The Manchester Times and Gazette (1829-1848), Manchester Times and Manchester and Salford Advertiser and Chronicle (1848-1848)</li>
<li>The Midland Examiner and Times (1877-1877), The Midland Examiner and Wolverhampton Times (1877-1878)</li>
<li>Mirror of the Times (1800-1823)</li>
<li>The Morning Chronicle (1801-1865)</li>
<li>Morning Herald (1801-1869)</li>
<li>The National Register (1808-1823)</li>
<li>The New Weekly True Sun (1836-1836)</li>
<li>The News (1805-1835), The News and Sunday Herald (1835-1837), The News and Sunday Globe (1837-1839)</li>
<li>The Northern Daily Times (1853-1857), Northern Times (1857-1860), The Daily Times (1860-1861)</li>
<li>The Northern Liberator (1837-1840), The Northern Liberator and Champion (1840-1840)</li>
<li>Northern Star and Leeds General Advertiser (1838-1844), The Northern Star and National Trades’ Journal (1844-1852), The Star and National Trades’ Journal (1852-1852), The Star of Freedom (1852-1852)</li>
<li>The Nuneaton Times (1878-1879)</li>
<li>The Odd Fellow (1839-1842)</li>
<li>The Operative (1838-1839)</li>
<li>Pictorial Times (1843-1848)</li>
<li>Pierce Egan’s Life in London (1824-1827)</li>
<li>The Pontypridd District Herald and Rhondda Valley, Llantrisant, Caerphilly, and Mountain Ash News (1878-1879)</li>
<li>The Poole Telegram (1879-1879)</li>
<li>The Poor Man’s Guardian (1831-1835)</li>
<li>The Potteries Examiner (1871-1879)</li>
<li>The Press (1853-1866)</li>
<li>Runcorn and Widnes Examiner (1870-1876), Runcorn Examiner (1877-1879)</li>
<li>The St. Helens Examiner, and Prescot Weekly News (1879-1879)</li>
<li>The Saint James’s Chronicle (1801-1866)</li>
<li>Shropshire Examiner and all round the Wrekin Advertiser (1874-1877)</li>
<li>The South Staffordshire Examiner (1874-1874)</li>
<li>The Southern Star and London and Brighton Patriot (1840-1840)</li>
<li>Stalybridge Examiner, and Ashton, Dukinfield and Mossley Advertiser (1876-1876)</li>
<li>The Star (1801-1831)</li>
<li>The Statesman (1806-1824)</li>
<li>The Stockton Examiner, and South Durham and North Yorkshire Herald (1879-1879)</li>
<li>Stockton Gazette and Middlesbrough Times (1860-1865), Middlesbro’ &amp; Stockton Gazette and General Advertiser (1868-1868), The Middlesbrough Gazette and General Advertiser (1869-1869), Middlesbrough &amp; Stockton Gazette and General Advertiser (1869-1876), The Weekly Gazette for Middlesbrough, Stockton, Hartlepool and Cleveland District (1876-1879)</li>
<li>Stockton Herald, South Durham and Cleveland Advertiser (1858-1879)</li>
<li>Stretford and Urmston Examiner (1879-1879)</li>
<li>The Sun (1801-1871)</li>
<li>Swansea and Glamorgan Herald, and South Wales Free Press (1847-1879)</li>
<li>The Tamworth Miners’ Examiner and Working Men’s Journal (1873-1873), The Tamworth Examiner and Working Men’s Journal (1873-1876)</li>
<li>The Warrington Examiner (1885-1878), The Warrington &amp; Mid-Cheshire Examiner (1879-1879)</li>
<li>The Weekly Chronicle (1836-1851), The Weekly News and Chronicle (1851-1854), The Weekly Chronicle (1855-1855), The Weekly Chronicle and Register (1855-1864), The Weekly Chronicle and Register of Banking, Insurance, Railway and Mining Companies, Trade and Commerce (1864-1867)</li>
<li>Westminster Journal and Old British Spy (1805-1810)</li>
<li>The Weymouth, Portland and Dorchester Telegram (1862-1878)</li>
<li>Widnes Examiner (1876-1879)</li>
<li>Wolverhampton Times and Bilston, Willenhall, Wednesfield, and Sedgley Journal (1874-1875), The Wolverhampton and Midland Counties Advertiser (1875-1876)</li>
</ul>
</div>










<div>
<p>SUPPORT THIS WEBSITE</p>
<div>
<p>This website has been running now for just over a decade, and while advertising revenue contributes to funding the website, but doesn&#39;t cover the costs. That is why I have set up a facility with <a href="https://donorbox.org/support-ianvisits-reporting-and-events-guide" data-wpel-link="external" rel="nofollow noopener">DonorBox</a> where you can contribute to the costs of the website and time invested in writing and research for the news articles.</p>
<p>It&#39;s very similar to the way The Guardian and many smaller websites are now seeking to generate an income in the face of rising costs and declining advertising.</p>
<p>Whether its a one-off donation or a regular giver, every additional support goes a long way to covering the running costs of this website, and keeping you regularly topped up doses of Londony news and facts.</p>
<p>If you like what your read on here, then please support the website <a href="https://donorbox.org/support-ianvisits-reporting-and-events-guide" data-wpel-link="external" rel="nofollow noopener"><strong>here</strong></a>.</p>
<p>Thank you</p>
</div>
</div>
</div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 11:20:20 +0000</pubDate>
      <source>https://www.ianvisits.co.uk/blog/2021/08/11/british-library-puts-1-million-newspaper-pages-online-for-free/</source>
    </item>
    <item>
      <title>The Ancient Persian way to keep cool</title>
      <link>https://www.bbc.com/future/article/20210810-the-ancient-persian-way-to-keep-cool</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___www_bbc_com_future_article_20210810-the-ancient-persian-way-to-keep-cool/image.jpg" /> 
<div id="readability-page-1" class="page"><div><div><section id="futurearticle20210810-the-ancient-persian-way-to-keep-cool"><div id="headline-futurearticle20210810-the-ancient-persian-way-to-keep-cool"><div><p>The Ancient Persian way to keep cool</p></div><div><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p09rtttz.webp" type="image/webp"/><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p09rtttz.jpg" type="image/jpeg"/><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p09rtttz.webp" type="image/webp"/><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p09rtttz.jpg" type="image/jpeg"/><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p09rtttz.webp" type="image/webp"/><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p09rtttz.jpg" type="image/jpeg"/><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p09rtttz.webp" type="image/webp"/><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p09rtttz.jpg" type="image/jpeg"/></picture></div></div><div><div><div><article><div><p>From Ancient Egypt to the Persian Empire, an ingenious method of catching the breeze kept people cool for millennia. In the search for emissions-free cooling, the &#34;wind catcher&#34; could once again come to our aid.</p><div><div><p>T</p><div><div><p>The city of Yazd in the desert of central Iran has long been a focal point for creative ingenuity. Yazd is home to a system of ancient engineering marvels that include an underground refrigeration structure called <em><a href="https://theculturetrip.com/middle-east/articles/this-ancient-technique-to-make-ice-in-the-desert-is-mind-boggling/">y</a><a href="https://theculturetrip.com/middle-east/articles/this-ancient-technique-to-make-ice-in-the-desert-is-mind-boggling/">akhchāl</a></em>, an underground irrigation system called <em><a href="https://www.bbc.com/travel/article/20180619-irans-ancient-engineering-marvel">qanats</a></em>, and even a network of couriers called <em><a href="https://www.bbc.com/travel/article/20200624-iran-the-surprising-origins-of-the-postal-service">pirradaziš</a></em> that predate postal services in the US by more than 2,000 years.</p>
<p>Among Yazd&#39;s ancient technologies is the wind catcher, or <em>bâdgir</em> in Persian. These remarkable structures are a common sight soaring above the rooftops of Yazd. They are often rectangular towers, but they also appear in circular, square, octagonal and other ornate shapes.</p>
<p>Yazd is said to have the most wind catchers in the world, though <a href="https://www.bbc.com/travel/article/20180926-an-ancient-engineering-feat-that-harnessed-the-wind">they may have originated in Ancient Egypt</a>. In Yazd, the wind catcher soon proved indispensable, making this part of the hot and arid Iranian Plateau livable.</p>
<p>Though many of the city&#39;s wind catchers have fallen out of use, the structures are now drawing academics, architects and engineers back to the desert city to see what role they could play in keeping us cool in a <a href="https://www.bbc.co.uk/news/science-environment-57751918">rapidly heating world</a>.</p></div></div></div></div><div><div id="future/article/20210810-the-ancient-persian-way-to-keep-cool-p09rttxb"><div><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p09rttxb.webp" type="image/webp"/><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p09rttxb.jpg" type="image/jpeg"/><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p09rttxb.webp" type="image/webp"/><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p09rttxb.jpg" type="image/jpeg"/><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p09rttxb.webp" type="image/webp"/><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p09rttxb.jpg" type="image/jpeg"/><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p09rttxb.webp" type="image/webp"/><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p09rttxb.jpg" type="image/jpeg"/></picture><div><p>The openings of the towers face the prevailing wind, catching it and funneling it down to the interior below (Credit: Alamy)</p></div></div></div></div><div><div><div><div><p>As a wind catcher requires no electricity to power it, it is both a cost-efficient and green form of cooling. With conventional mechanical air conditioning already <a href="https://www.iea.org/reports/the-future-of-cooling">accounting for a fifth of total electricity consumption globally</a>, ancient alternatives like the wind catcher are becoming an <a href="https://www.bbc.com/future/article/20190822-are-there-alternatives-to-air-conditioning">increasingly appealing option</a>.</p>
<p>There are two main forces that drive the air through and down into the structures: the incoming wind and the change in buoyancy of air depending on temperature – with warmer air tending to rise above cooler, denser air. First, as air is caught by the opening of a wind catcher, it is funneled down to the dwelling below, depositing any sand or debris at the foot of the tower. Then the air flows throughout the interior of the building, <a href="https://www.sciencedirect.com/science/article/pii/S1364032114008351">sometimes over subterranean pools of water for further cooling</a>. Eventually, warmed air will rise and leave the building through another tower or opening, aided by the pressure within the building.</p>
<p>The shape of the tower, alongside factors like the layout of the house, the direction the tower is facing, how many openings it has, its configuration of fixed internal blades, canals and height are all finely tuned to improve the tower&#39;s ability to draw wind down into the dwellings below.</p></div></div></div></div><div><div><div><div><p>Using the wind to cool buildings has a history stretching back almost as long as people have lived in hot desert environments. Some of the <a href="https://ascelibrary.org/doi/10.1061/9780784413517.161">earliest wind-catching technology comes from Egypt 3,300 years ago</a>, according to researchers Chris Soelberg and Julie Rich of Weber State University in Utah. Here, buildings had thick walls, few windows facing the Sun, openings to take in air on the side of prevailing winds and an exit vent on the other side – known in Arabic as <em><a href="https://orbi.uliege.be/bitstream/2268/167581/1/Attia_Designing%20the%20Malqaf%20for%20summer%20cooling%20in%20low-rise%20housing,%20an%20experimental%20study.pdf">malq</a><a href="https://orbi.uliege.be/bitstream/2268/167581/1/Attia_Designing%20the%20Malqaf%20for%20summer%20cooling%20in%20low-rise%20housing,%20an%20experimental%20study.pdf">af architecture</a></em>. Though some argue that <a href="https://www.bbc.com/travel/article/20180926-an-ancient-engineering-feat-that-harnessed-the-wind">the birthplace of the wind catcher was Iran itself</a>.</p>
<p>Wherever it was first invented, wind catchers have since become widespread across the Middle East and North Africa. Variations of Iran&#39;s wind catchers can be found in the <em>barjeels</em> of Qatar and Bahrain, the <em>malqaf</em> of Egypt, the <em>mungh</em> of Pakistan, and <a href="https://eprints.whiterose.ac.uk/112557/3/A%252520review%252520on%252520windcatcher%252520for%252520passive%252520cooling%252520and%252520natural%252520ventilation%252520in%252520buildings%20%281%29.pdf">many other</a><a href="https://eprints.whiterose.ac.uk/112557/3/A%252520review%252520on%252520windcatcher%252520for%252520passive%252520cooling%252520and%252520natural%252520ventilation%252520in%252520buildings%20%281%29.pdf"> places</a><a href="https://eprints.whiterose.ac.uk/112557/3/A%252520review%252520on%252520windcatcher%252520for%252520passive%252520cooling%252520and%252520natural%252520ventilation%252520in%252520buildings%20%281%29.pdf">, notes </a><a href="https://eprints.whiterose.ac.uk/112557/3/A%252520review%252520on%252520windcatcher%252520for%252520passive%252520cooling%252520and%252520natural%252520ventilation%252520in%252520buildings%20%281%29.pdf">Fatemeh Jomehzadeh of the University of Technology Malaysia and colleagues</a>.</p></div></div></div></div><div><div id="future/article/20210810-the-ancient-persian-way-to-keep-cool-p09rtv6p"><div><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p09rtv6p.webp" type="image/webp"/><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p09rtv6p.jpg" type="image/jpeg"/><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p09rtv6p.webp" type="image/webp"/><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p09rtv6p.jpg" type="image/jpeg"/><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p09rtv6p.webp" type="image/webp"/><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p09rtv6p.jpg" type="image/jpeg"/><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p09rtv6p.webp" type="image/webp"/><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p09rtv6p.jpg" type="image/jpeg"/></picture><div><p>Due to long disuse, many of Iran&#39;s windcatchers are not in a good state of repair. But some researchers would like to see them restored to working order (Credit: Alamy)</p></div></div></div></div><div><div><div><div><p>The Persian civilisation is widely considered to have added structural variations to allow for better cooling – such as combining it with its existing irrigation system to help to cool the air down before releasing it throughout the home. In Yazd&#39;s hot, dry climate, these structures proved remarkably popular, until the city became a hotspot of soaring ornate towers seeking the desert wind. The historical city of Yazd was recognised as a Unesco World Heritage site in <a href="https://whc.unesco.org/en/list/1544/">2017</a>, in part for its proliferation of wind catchers.</p>
<p>As well as performing the functional purpose of cooling homes, the towers also had a strong cultural significance. In Yazd, the wind catchers are as much a part of the skyline as the Zoroastrian Fire Temple and Tower of Silence. Among them is the wind catcher at the Dowlatabad Abad Gardens, said to be the <a href="https://www.researchgate.net/figure/The-eight-sided-wind-catcher-of-Dowlat-Abad-Gardens-edifice-in-Yazd-is-the-tallest-in_fig2_308532117">tallest in the world at 33m</a> (108ft) and one of the few wind catchers still in operation. Housed in an octagonal building, it overlooks a fountain stretching past rows of pine trees.</p></div></div></div></div><div><div><div><div><p>The emissions-free cooling efficacy of such wind catchers make some researchers argue that they are due a revival.</p>
<p>Parham Kheirkhah Sangdeh has extensively studied the scientific application and surrounding culture of wind catchers in contemporary architecture at Ilam University in Tehran, Iran. He says inconveniences like pests entering the chutes and the gathering of dust and desert debris have meant many have turned away from traditional wind catchers. In their place are mechanical cooling systems, such as conventional air-conditioning units. Often, those options are powered by fossil fuels and use <a href="https://www.bbc.com/future/article/20201204-climate-change-how-chemicals-in-your-fridge-warm-the-planet">refrigerants that act as powerful greenhouse gases if released into the atmosphere</a>.</p></div></div></div></div><div><div id="future/article/20210810-the-ancient-persian-way-to-keep-cool-p09rtvlr"><div><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p09rtvlr.webp" type="image/webp"/><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p09rtvlr.jpg" type="image/jpeg"/><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p09rtvlr.webp" type="image/webp"/><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p09rtvlr.jpg" type="image/jpeg"/><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p09rtvlr.webp" type="image/webp"/><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p09rtvlr.jpg" type="image/jpeg"/></picture><div><p>The wind catchers of Iran have inspired modern designs in Europe, the US and elsewhere, as architects turn towards passive forms of cooling (Credit: Alamy)</p></div></div></div></div><div><div><div><div><p>The advent of modern cooling technologies has long been blamed for the deterioration of traditional methods in Iran, <a href="https://doi.org/10.2307/4300566">the historian of Iranian architecture Elizabeth Beazley wrote in 1977</a>. Without constant maintenance, the harsh climate of the Iranian Plateau has worn away many structures from wind catchers to ice houses. Kheirkhah Sangdeh also sees the shift away from wind catchers as in part down to a tendency among the public to engage with technologies from the West.</p>
<p>&#34;There needs to be some changes in cultural perspectives to use these technologies. People need to keep an eye on the past and understand why energy conservation is important,&#34; Kheirkhah Sangdeh says. &#34;It starts with recognising cultural history and the importance of energy conservation.&#34;</p>
<p>Kheirkhah Sangdeh hopes to see Iran&#39;s wind catchers updated to add energy-efficient cooling to existing buildings. But he has met many barriers to his work in the form of ongoing international tensions, the coronavirus pandemic and ongoing<a href="https://www.bbc.com/news/av/world-middle-east-57948717"> water shortage</a>. &#34;Things are so bad in Iran that [people] take it day by day,&#34; says Kheirkhah Sangdeh.</p></div></div></div></div><div><div id="future/article/20210810-the-ancient-persian-way-to-keep-cool-p09rtttz"><div><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p09rtttz.webp" type="image/webp"/><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p09rtttz.jpg" type="image/jpeg"/><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p09rtttz.webp" type="image/webp"/><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p09rtttz.jpg" type="image/jpeg"/><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p09rtttz.webp" type="image/webp"/><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p09rtttz.jpg" type="image/jpeg"/><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p09rtttz.webp" type="image/webp"/><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p09rtttz.jpg" type="image/jpeg"/></picture><div><p>Yazd is said to have the most wind catchers of any city in the world (Credit: Alamy)</p></div></div></div></div><div><div><div><div><p>Fossil-fuel-free methods of cooling like the wind catcher might well be due a revival, but to a surprising extent they are already present – albeit in a less magnificent form than those in Iran – in many Western countries.</p>
<p>In the UK, <a href="https://www.semanticscholar.org/paper/Evaluation-of-pressure-coefficients-and-estimation-Karakatsanis-Bahadori/6cfc2eb6ccf3b79c2197db02df62c31ee14e0b99">some 7,000 variations of wind catchers were installed in public buildings between 1979 and 1994</a>. They can be seen from buildings such as <a href="https://www.monodraught.com/projects/royal-chelsea-hospital">the Royal Chelsea Hospital in London</a>, to <a href="https://www.sciencedirect.com/science/article/abs/pii/S1364032116310358?via%3Dihub">supermarkets in Manchester</a>.</p>
<p>These modernised wind catchers bear little resemblance to Iran&#39;s towering structures. On one three-storey building on a busy road in north London, small hot pink ventilation towers allow passive ventilation. Atop a shopping centre in Dartford, <a href="https://www.visionventilation.co.uk/architectural-inspired-designs/">conical ventilation towers rotate to catch the breeze with the help of a rear wing that keeps the tower facing the prevailing wind</a>.</p>
<p>The US too has adopted wind-catcher-inspired designs with enthusiasm. One such example is the visitor center at <a href="http://sipb.sggw.pl/CRC2014/data/papers/9780784413517.161.pdf">Zion National Park</a> in southern Utah. The park sits in a high desert plateau, comparable to Yazd in climate and topography, and the use of passive cooling technologies including the wind catcher nearly eliminated the need for mechanical air-conditioning. Scientists have recorded a temperature difference of 16C (29F) between the outside and inside of the visitor centre, despite the many bodies regularly passing through.</p>
<p>There is further scope for the spread of the wind catcher, as <a href="https://www.theccc.org.uk/2017/08/08/hidden-problem-overheating/">the search for sustainable solutions to overheating continues</a>. In Palermo, Sicily, researchers have found that <a href="https://www.tandfonline.com/doi/abs/10.1080/14733315.2016.1214397">the climate and prevailing wind conditions make it a ripe location for a version of the Iranian wind catcher</a>. This October, meanwhile, the wind catcher is set to have a high-profile position at <a href="https://www.bbc.co.uk/news/business-56682427">the World Expo</a> fair in Dubai, as part of <a href="https://www.querkraft.at/en/projects/expo-pavilion">a network of conical buildings in the Austrian pavilion</a>, where the Austrian architecture firm Querkraft has taken inspiration from the Arabic <em>barjeel</em> version of the wind tower.</p>
<p>While researchers such as Kheirkhah Sangdeh argue that the wind catcher has much more to give in cooling homes without fossil fuels, this ingenious technology has already migrated further around the world than you might think. Next time you see a tall vented tower on top of a supermarket, high-rise or <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjH7rK7kKTyAhWiwOYKHStlCpgQFnoECA4QAw&amp;url=https%3A%2F%2Fs3-eu-west-1.amazonaws.com%2Fmonodraught%2Fdownloads%2Fprojects%2Faddey-and-stanhope-shool-windcatcher-case-study.pdf%3Fv%3D2&amp;usg=AOvVaw2b0GfphSTbK_2wygJxCHG9">school</a>, look carefully – you might just be looking at the legacy of the magnificent wind catchers of Iran.</p>
<p>--</p>
<p><em>The emissions from travel it took to report this story were 0kg CO2. The digital emissions from this story are an estimated 1.2g to 3.6g CO2 per page view. </em><a href="https://www.bbc.com/future/article/20200131-why-and-how-does-future-planet-count-carbon"><em>Find out more about how we calculated this figure here</em></a><em>.</em></p>
<p>--</p>
<p><em>Join one million Future fans by liking us on </em><a href="https://www.facebook.com/BBCFuture/"><strong><em>Facebook</em></strong></a><em>, or follow us on </em><a href="https://twitter.com/BBC_Future"><strong><em>Twitter</em></strong></a><em> or<strong> </strong></em><a href="https://www.instagram.com/bbcfuture_official/"><strong><em>Instagram</em></strong></a><em>.</em></p>
<p><em>If you liked this story, </em><a href="http://pages.emails.bbc.com/subscribe/?ocid=fut.bbc.email.we.email-signup"><strong><em>sign up for the weekly bbc.com features newsletter</em></strong></a><em>, called &#34;The Essential List&#34;. A handpicked selection of stories from BBC Future, Culture, Worklife, and Travel, delivered to your inbox every Friday.</em></p></div></div></div></div></div></article></div></div></div></section></div></div></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 06:46:09 +0000</pubDate>
      <source>https://www.bbc.com/future/article/20210810-the-ancient-persian-way-to-keep-cool</source>
    </item>
    <item>
      <title>What if we could transpile COBOL into Elixir</title>
      <link>https://dockyard.com/blog/2021/08/10/what-if-coboltoelixir</link>
      <description></description>
      <content:encoded><![CDATA[<img src="https://rss.markdessain.com/feeds/hackernews/https___dockyard_com_blog_2021_08_10_what-if-coboltoelixir/image.jpg" /> 
<div id="readability-page-1" class="page"><section>
    <div>
      <div id="ember23582451"><!---->
<p>Marvel Television has been on fire this past year with WandaVision, Falcon and the Winter Soldier, and most recently, Loki. Tomorrow, the next series called “What If?” premieres, and I am really looking forward to it. The show will explore what the Marvel Cinematic Universe (MCU) would look like if pivotal moments from throughout the MCU were different. </p>
<p>For example, the first episode explores what would happen if Peggy Carter had taken the Super Soldier Serum instead of Steve Rogers to become Captain America. These “what if” ideas are retrospective in nature, exploring “what would have happened if”. While interesting to consider, this type of thinking is less useful in the real world. In reality, future-oriented “what if” questions–that explore what possibilities might lay ahead–are far more impactful.</p>
<p>I’m happy to say the exploration of future-oriented “what if” questions is core to the Elixir programming community. We exist in large part due to one big “what if”: what if there was a new programming language that could use modern syntax, but still deliver the reliability of a 30-year old, rock-solid virtual machine? </p>
<p>Other “what ifs” followed: </p>
<ul>
<li>What if we built a blazing fast web framework in Elixir?;
</li>
<li>What if we put Elixir on embedded devices?;
</li>
<li>What if you could do Machine Learning on the GPU with Elixir?; and many more.
</li>
</ul>
<p>Along the way, there have also been plenty of “what ifs” that didn’t work out, such as “What if Elixir was Object Oriented?” or “What if Elixir had an explicit type system?”. </p>
<p>Today, I am sharing something I’ve been working on related to COBOL—all stemming from a “what if?” discussion that came up here at DockYard.</p>
<p>Like me just a few months ago, you may have heard about COBOL in your computer history class but have never seen or written a line of it. The <a href="​​https://americanhistory.si.edu/cobol/introduction">Smithsonian Institute COBOL History page</a> states:</p>
<blockquote><p>Fifty years ago, each computer maker used its own programming languages to tell a computer what to do. In 1959, a group of programmers devised COBOL, a COmmon, Business-Oriented Language. Programs written in COBOL could run on more than one manufacturer’s computer. In a 1960 test, the same COBOL programs ran successfully on two computers built by different manufacturers.</p>
</blockquote>
<p>But a language created in 1959 can’t possibly still be in use today, right? Wrong. According to <a href="http://fingfx.thomsonreuters.com/gfx/rngs/USA-BANKS-COBOL/010040KH18J/index.html">Reuters</a>:</p>
<ul>
<li>43% of banking systems are built on COBOL;
</li>
<li>80% of in-person transactions use COBOL;
</li>
<li>95% of ATM swipes rely on COBOL code; and 
</li>
<li>220 billion lines of COBOL are in use today.
</li>
</ul>
<p>That same article notes:</p>
<blockquote><p>An aging programming language known as COBOL underpins much of the U.S. financial industry, but it has fallen out of favor among coders. This sets up a problem when systems run into glitches or need updates, and companies no longer have COBOL experts on hand.</p>
</blockquote>
<p>With a renewed focus on “infrastructure” in the recent political landscape, and the very real potential that these COBOL systems could be revisited, the question arose in a DockYard discussion: “What if we could transpile COBOL into Elixir?”</p>
<p>I have been looking for a larger Elixir project to start or support, so I figured, why not see what it would take? While I knew it was a long shot–and there are any number of ways it might fail–I thought it was worth a look. A few hours, an “all you need to know about COBOL” YouTube video, and a code spike later, and I hadn’t run into any complete roadblocks. Furthermore, what I remembered about the properties of the COBOL language that might lend themselves to this idea all seemed to be accurate.</p>
<p>Fast forward a few months to today. CobolToElixir has come a long way. It has support for many of the basic building blocks of COBOL and plans for tackling some of the larger language constructs. Most importantly, it has a framework for testing that will run COBOL code, convert the code to Elixir and run it, and then compare the two outputs to ensure they are the same. I have not yet run into any major roadblocks that I’ve been unable to work around, but there is still plenty to do that could uncover them.</p>
<p>The source code is available at <a href="https://github.com/TheFirstAvenger/cobol_to_elixir">https://github.com/TheFirstAvenger/cobol_to_elixir</a>, and there is a Livebook that you can run to try it out for yourself. Below is an example basic COBOL program and what the transpiled Elixir code looks like:</p>
<p>COBOL:</p>
<pre><code>       &gt;&gt;SOURCE FORMAT FREE
IDENTIFICATION DIVISION.
PROGRAM-ID. Test1.
AUTHOR. Mike Binns.
DATE-WRITTEN. July 25th 2021
DATA DIVISION.
WORKING-STORAGE SECTION.
01 Name     PIC X(4) VALUE &#34;Mike&#34;.
PROCEDURE DIVISION.

DISPLAY &#34;Hello &#34; Name

STOP RUN.</code></pre>
<p>Elixir:</p>
<pre><code>defmodule ElixirFromCobol.Test1 do
  @moduledoc &#34;&#34;&#34;
  author: Mike Binns
  date written: July 25th 2021
  &#34;&#34;&#34;

  def main do
    try do
      do_main()
    catch
      :stop_run -&gt; :stop_run
    end
  end 

  def do_main do
    # pic: XXXX
    var_Name = &#34;Mike&#34;
    pics = %{&#34;Name&#34; =&gt; {:str, &#34;XXXX&#34;, 4}}
    IO.puts &#34;Hello &#34; &lt;&gt; var_Name
    throw :stop_run
  end
end</code></pre>
<p>COBOL is different from Elixir in a number of ways. For example, above you can see that COBOL has a <code>STOP RUN</code> command which can happen anywhere in the code, completely ending the program. In the transpiled Elixir, we handle this with throw and catch. Additionally, all variables have a <code>PIC</code> (“Picture”) clause that defines the data type and shape. For example, the <code>name</code> variable above has a <code>PIC</code> of <code>X(4)</code>, or <code>XXXX</code>, which means it is a string exactly four characters long.</p>
<p>Another difference, which has been addressed (though not shown in the above example), is the difference between COBOL group items and their near-counterpart in Elixir, Maps. One difference that hasn’t yet been addressed, but has a planned solution, is the immutability in Elixir vs. global variables in COBOL.</p>
<p>The goal of CobolToElixir is not to provide a one button accurate transpiling of a large codebase perfectly into Elixir. The idea would be that we transpile the code, test the accuracy with the built in tools, and possibly iterate on CobolToElixir to improve the transpiling of the specific codebase. Once CobolToElixir has done what it can, focus would move to iterating on the transpiled code to resolve differences in output and bring the code more in line with idiomatic Elixir. The CobolToElixir framework would be able to assist via the testing framework through this entire process.</p>
<p>If you are interested in following this project, star the repo and follow me on Twitter (<a href="https://twitter.com/1stAvenger">@1stavenger</a>). I would love input from anyone else who is interested in helping out on this project. Please reach out to me on the <a href="https://elixir-slackin.herokuapp.com/">Elixir Slack channel</a> (TheFirstAvenger).</p>
<p>And finally, what is your “What If?”. As mentioned above, the Elixir community is built on these “What If?” questions. If you don’t have one yet, keep thinking about it, and when one comes to you, pursue it.</p>
<p><em><a href="https://dockyard.com/">DockYard</a> is a digital product consultancy specializing in user-centered web application design and development. Our collaborative team of product strategists help clients to better understand the people they serve. We use future-forward technology and design thinking to transform those insights into impactful, inclusive, and reliable web experiences. DockYard provides professional services in strategy, user experience, design, and full-stack engineering using Ember.js, React.js, Ruby, and Elixir. From idea to impact, we empower ambitious product teams to build for the future.</em></p>

</div>
    </div>
  </section></div>]]></content:encoded>
      <pubDate>Wed, 11 Aug 2021 12:40:25 +0000</pubDate>
      <source>https://dockyard.com/blog/2021/08/10/what-if-coboltoelixir</source>
    </item>
  </channel>
</rss>